// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 6
// REQUIRES: amdgpu-registered-target
// RUN: %clang_cc1 -triple amdgcn-amd-amdhsa -target-cpu gfx1250 -emit-llvm -fcuda-is-device -o - %s | FileCheck %s

#define __device__ __attribute__((device))

typedef _Float16 v8h __attribute__((ext_vector_type(8)));
typedef _Float16 v16h __attribute__((ext_vector_type(16)));
typedef _Float16 v32h __attribute__((ext_vector_type(32)));
typedef int v2i __attribute__((ext_vector_type(2)));
typedef int v8i __attribute__((ext_vector_type(8)));
typedef int v16i __attribute__((ext_vector_type(16)));
typedef float v8f __attribute__((ext_vector_type(8)));

// CHECK-LABEL: define dso_local void @_Z30test_wmma_f16_16x16x64_fp8_fp8PDv8_DF16_Dv8_iS1_S_(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <8 x i32> noundef [[A:%.*]], <8 x i32> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0:[0-9]+]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i32>, align 32, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i32>, align 32, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x i32> [[A]], ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <8 x i32> [[B]], ptr [[B_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i32>, ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i32>, ptr [[B_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call contract <8 x half> @llvm.amdgcn.wmma.f16.16x16x64.fp8.fp8.v8f16.v8i32(<8 x i32> [[TMP0]], <8 x i32> [[TMP1]], i16 0, <8 x half> [[TMP2]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[TMP4]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_wmma_f16_16x16x64_fp8_fp8(v8h *out, v8i a, v8i b, v8h c) {
  *out = __builtin_amdgcn_wmma_f16_16x16x64_fp8_fp8(a, b, 0, c, false, true);
}

// CHECK-LABEL: define dso_local void @_Z30test_wmma_f16_16x16x64_fp8_bf8PDv8_DF16_Dv8_iS1_S_(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <8 x i32> noundef [[A:%.*]], <8 x i32> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i32>, align 32, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i32>, align 32, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x i32> [[A]], ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <8 x i32> [[B]], ptr [[B_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i32>, ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i32>, ptr [[B_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call contract <8 x half> @llvm.amdgcn.wmma.f16.16x16x64.fp8.bf8.v8f16.v8i32(<8 x i32> [[TMP0]], <8 x i32> [[TMP1]], i16 0, <8 x half> [[TMP2]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[TMP4]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_wmma_f16_16x16x64_fp8_bf8(v8h *out, v8i a, v8i b, v8h c) {
  *out = __builtin_amdgcn_wmma_f16_16x16x64_fp8_bf8(a, b, 0, c, false, true);
}

// CHECK-LABEL: define dso_local void @_Z30test_wmma_f16_16x16x64_bf8_fp8PDv8_DF16_Dv8_iS1_S_(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <8 x i32> noundef [[A:%.*]], <8 x i32> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i32>, align 32, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i32>, align 32, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x i32> [[A]], ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <8 x i32> [[B]], ptr [[B_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i32>, ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i32>, ptr [[B_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call contract <8 x half> @llvm.amdgcn.wmma.f16.16x16x64.bf8.fp8.v8f16.v8i32(<8 x i32> [[TMP0]], <8 x i32> [[TMP1]], i16 0, <8 x half> [[TMP2]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[TMP4]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_wmma_f16_16x16x64_bf8_fp8(v8h *out, v8i a, v8i b, v8h c) {
  *out = __builtin_amdgcn_wmma_f16_16x16x64_bf8_fp8(a, b, 0, c, false, true);
}

// CHECK-LABEL: define dso_local void @_Z30test_wmma_f16_16x16x64_bf8_bf8PDv8_DF16_Dv8_iS1_S_(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <8 x i32> noundef [[A:%.*]], <8 x i32> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i32>, align 32, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i32>, align 32, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x i32> [[A]], ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <8 x i32> [[B]], ptr [[B_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i32>, ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i32>, ptr [[B_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call contract <8 x half> @llvm.amdgcn.wmma.f16.16x16x64.bf8.bf8.v8f16.v8i32(<8 x i32> [[TMP0]], <8 x i32> [[TMP1]], i16 0, <8 x half> [[TMP2]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[TMP4]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_wmma_f16_16x16x64_bf8_bf8(v8h *out, v8i a, v8i b, v8h c) {
  *out = __builtin_amdgcn_wmma_f16_16x16x64_bf8_bf8(a, b, 0, c, false, true);
}

// CHECK-LABEL: define dso_local void @_Z31test_wmma_f16_16x16x128_fp8_fp8PDv8_DF16_Dv16_iS1_S_(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <16 x i32> noundef [[A:%.*]], <16 x i32> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x i32>, align 64, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x i32>, align 64, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <16 x i32> [[A]], ptr [[A_ADDR_ASCAST]], align 64
// CHECK-NEXT:    store <16 x i32> [[B]], ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i32>, ptr [[A_ADDR_ASCAST]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i32>, ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call contract <8 x half> @llvm.amdgcn.wmma.f16.16x16x128.fp8.fp8.v8f16.v16i32(<16 x i32> [[TMP0]], <16 x i32> [[TMP1]], i16 0, <8 x half> [[TMP2]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[TMP4]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_wmma_f16_16x16x128_fp8_fp8(v8h *out, v16i a, v16i b, v8h c) {
  *out = __builtin_amdgcn_wmma_f16_16x16x128_fp8_fp8(a, b, 0, c, false, true);
}

// CHECK-LABEL: define dso_local void @_Z31test_wmma_f16_16x16x128_fp8_bf8PDv8_DF16_Dv16_iS1_S_(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <16 x i32> noundef [[A:%.*]], <16 x i32> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x i32>, align 64, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x i32>, align 64, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <16 x i32> [[A]], ptr [[A_ADDR_ASCAST]], align 64
// CHECK-NEXT:    store <16 x i32> [[B]], ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i32>, ptr [[A_ADDR_ASCAST]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i32>, ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call contract <8 x half> @llvm.amdgcn.wmma.f16.16x16x128.fp8.bf8.v8f16.v16i32(<16 x i32> [[TMP0]], <16 x i32> [[TMP1]], i16 0, <8 x half> [[TMP2]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[TMP4]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_wmma_f16_16x16x128_fp8_bf8(v8h *out, v16i a, v16i b, v8h c) {
  *out = __builtin_amdgcn_wmma_f16_16x16x128_fp8_bf8(a, b, 0, c, false, true);
}

// CHECK-LABEL: define dso_local void @_Z31test_wmma_f16_16x16x128_bf8_fp8PDv8_DF16_Dv16_iS1_S_(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <16 x i32> noundef [[A:%.*]], <16 x i32> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x i32>, align 64, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x i32>, align 64, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <16 x i32> [[A]], ptr [[A_ADDR_ASCAST]], align 64
// CHECK-NEXT:    store <16 x i32> [[B]], ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i32>, ptr [[A_ADDR_ASCAST]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i32>, ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call contract <8 x half> @llvm.amdgcn.wmma.f16.16x16x128.bf8.fp8.v8f16.v16i32(<16 x i32> [[TMP0]], <16 x i32> [[TMP1]], i16 0, <8 x half> [[TMP2]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[TMP4]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_wmma_f16_16x16x128_bf8_fp8(v8h *out, v16i a, v16i b, v8h c) {
  *out = __builtin_amdgcn_wmma_f16_16x16x128_bf8_fp8(a, b, 0, c, false, true);
}

// CHECK-LABEL: define dso_local void @_Z31test_wmma_f16_16x16x128_bf8_bf8PDv8_DF16_Dv16_iS1_S_(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <16 x i32> noundef [[A:%.*]], <16 x i32> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x i32>, align 64, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x i32>, align 64, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <16 x i32> [[A]], ptr [[A_ADDR_ASCAST]], align 64
// CHECK-NEXT:    store <16 x i32> [[B]], ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i32>, ptr [[A_ADDR_ASCAST]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i32>, ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call contract <8 x half> @llvm.amdgcn.wmma.f16.16x16x128.bf8.bf8.v8f16.v16i32(<16 x i32> [[TMP0]], <16 x i32> [[TMP1]], i16 0, <8 x half> [[TMP2]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[TMP4]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_wmma_f16_16x16x128_bf8_bf8(v8h *out, v16i a, v16i b, v8h c) {
  *out = __builtin_amdgcn_wmma_f16_16x16x128_bf8_bf8(a, b, 0, c, false, true);
}

// CHECK-LABEL: define dso_local void @_Z26test_wmma_f32_16x16x32_f16PDv8_fDv16_DF16_S1_S_(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <16 x half> noundef [[A:%.*]], <16 x half> noundef [[B:%.*]], <8 x float> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x half>, align 32, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x float>, align 32, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <16 x half> [[B]], ptr [[B_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <8 x float> [[C]], ptr [[C_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[B_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[C_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = call contract <8 x float> @llvm.amdgcn.wmma.f32.16x16x32.f16.v8f32.v16f16(i1 false, <16 x half> [[TMP0]], i1 false, <16 x half> [[TMP1]], i16 0, <8 x float> [[TMP2]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x float> [[TMP3]], ptr [[TMP4]], align 32
// CHECK-NEXT:    ret void
//
__device__ void test_wmma_f32_16x16x32_f16(v8f *out, v16h a, v16h b, v8f c) {
  *out = __builtin_amdgcn_wmma_f32_16x16x32_f16(0, a, 0, b, 0, c, false, true);
}

// CHECK-LABEL: define dso_local void @_Z26test_wmma_f16_16x16x32_f16PDv8_DF16_Dv16_DF16_S1_S_(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <16 x half> noundef [[A:%.*]], <16 x half> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x half>, align 32, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <16 x half> [[B]], ptr [[B_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[B_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call contract <8 x half> @llvm.amdgcn.wmma.f16.16x16x32.f16.v8f16.v16f16(i1 false, <16 x half> [[TMP0]], i1 false, <16 x half> [[TMP1]], i16 0, <8 x half> [[TMP2]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[TMP4]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_wmma_f16_16x16x32_f16(v8h *out, v16h a, v16h b, v8h c) {
  *out = __builtin_amdgcn_wmma_f16_16x16x32_f16(0, a, 0, b, 0, c, false, true);
}

// CHECK-LABEL: define dso_local void @_Z33test_swmmac_f16_16x16x128_fp8_fp8PDv8_DF16_Dv8_iDv16_iS_Dv2_i(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <8 x i32> noundef [[A:%.*]], <16 x i32> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]], <2 x i32> noundef [[INDEX:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i32>, align 32, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x i32>, align 64, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[INDEX_ADDR:%.*]] = alloca <2 x i32>, align 8, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    [[INDEX_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[INDEX_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x i32> [[A]], ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <16 x i32> [[B]], ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    store <2 x i32> [[INDEX]], ptr [[INDEX_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i32>, ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i32>, ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i32>, ptr [[INDEX_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = call contract <8 x half> @llvm.amdgcn.swmmac.f16.16x16x128.fp8.fp8.v8f16.v8i32.v16i32.v2i32(<8 x i32> [[TMP0]], <16 x i32> [[TMP1]], <8 x half> [[TMP2]], <2 x i32> [[TMP3]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP4]], ptr [[TMP5]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_swmmac_f16_16x16x128_fp8_fp8(v8h *out, v8i a, v16i b, v8h c, v2i index) {
  *out = __builtin_amdgcn_swmmac_f16_16x16x128_fp8_fp8(a, b, c, index, false, true);
}

// CHECK-LABEL: define dso_local void @_Z33test_swmmac_f16_16x16x128_fp8_bf8PDv8_DF16_Dv8_iDv16_iS_Dv2_i(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <8 x i32> noundef [[A:%.*]], <16 x i32> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]], <2 x i32> noundef [[INDEX:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i32>, align 32, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x i32>, align 64, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[INDEX_ADDR:%.*]] = alloca <2 x i32>, align 8, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    [[INDEX_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[INDEX_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x i32> [[A]], ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <16 x i32> [[B]], ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    store <2 x i32> [[INDEX]], ptr [[INDEX_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i32>, ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i32>, ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i32>, ptr [[INDEX_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = call contract <8 x half> @llvm.amdgcn.swmmac.f16.16x16x128.fp8.bf8.v8f16.v8i32.v16i32.v2i32(<8 x i32> [[TMP0]], <16 x i32> [[TMP1]], <8 x half> [[TMP2]], <2 x i32> [[TMP3]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP4]], ptr [[TMP5]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_swmmac_f16_16x16x128_fp8_bf8(v8h *out, v8i a, v16i b, v8h c, v2i index) {
  *out = __builtin_amdgcn_swmmac_f16_16x16x128_fp8_bf8(a, b, c, index, false, true);
}

// CHECK-LABEL: define dso_local void @_Z33test_swmmac_f16_16x16x128_bf8_fp8PDv8_DF16_Dv8_iDv16_iS_Dv2_i(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <8 x i32> noundef [[A:%.*]], <16 x i32> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]], <2 x i32> noundef [[INDEX:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i32>, align 32, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x i32>, align 64, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[INDEX_ADDR:%.*]] = alloca <2 x i32>, align 8, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    [[INDEX_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[INDEX_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x i32> [[A]], ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <16 x i32> [[B]], ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    store <2 x i32> [[INDEX]], ptr [[INDEX_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i32>, ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i32>, ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i32>, ptr [[INDEX_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = call contract <8 x half> @llvm.amdgcn.swmmac.f16.16x16x128.bf8.fp8.v8f16.v8i32.v16i32.v2i32(<8 x i32> [[TMP0]], <16 x i32> [[TMP1]], <8 x half> [[TMP2]], <2 x i32> [[TMP3]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP4]], ptr [[TMP5]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_swmmac_f16_16x16x128_bf8_fp8(v8h *out, v8i a, v16i b, v8h c, v2i index) {
  *out = __builtin_amdgcn_swmmac_f16_16x16x128_bf8_fp8(a, b, c, index, false, true);
}

// CHECK-LABEL: define dso_local void @_Z33test_swmmac_f16_16x16x128_bf8_bf8PDv8_DF16_Dv8_iDv16_iS_Dv2_i(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <8 x i32> noundef [[A:%.*]], <16 x i32> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]], <2 x i32> noundef [[INDEX:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i32>, align 32, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x i32>, align 64, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[INDEX_ADDR:%.*]] = alloca <2 x i32>, align 8, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    [[INDEX_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[INDEX_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x i32> [[A]], ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <16 x i32> [[B]], ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    store <2 x i32> [[INDEX]], ptr [[INDEX_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i32>, ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i32>, ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i32>, ptr [[INDEX_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = call contract <8 x half> @llvm.amdgcn.swmmac.f16.16x16x128.bf8.bf8.v8f16.v8i32.v16i32.v2i32(<8 x i32> [[TMP0]], <16 x i32> [[TMP1]], <8 x half> [[TMP2]], <2 x i32> [[TMP3]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP4]], ptr [[TMP5]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_swmmac_f16_16x16x128_bf8_bf8(v8h *out, v8i a, v16i b, v8h c, v2i index) {
  *out = __builtin_amdgcn_swmmac_f16_16x16x128_bf8_bf8(a, b, c, index, false, true);
}

// CHECK-LABEL: define dso_local void @_Z28test_swmmac_f32_16x16x64_f16PDv8_fDv16_DF16_Dv32_DF16_S_i(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <16 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]], <8 x float> noundef [[C:%.*]], i32 noundef [[INDEX:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x float>, align 32, addrspace(5)
// CHECK-NEXT:    [[INDEX_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    [[INDEX_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[INDEX_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    store <8 x float> [[C]], ptr [[C_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store i32 [[INDEX]], ptr [[INDEX_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[C_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[INDEX_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call contract <8 x float> @llvm.amdgcn.swmmac.f32.16x16x64.f16.v8f32.v16f16.v32f16.i32(i1 false, <16 x half> [[TMP0]], i1 false, <32 x half> [[TMP1]], <8 x float> [[TMP2]], i32 [[TMP3]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x float> [[TMP4]], ptr [[TMP5]], align 32
// CHECK-NEXT:    ret void
//
__device__ void test_swmmac_f32_16x16x64_f16(v8f *out, v16h a, v32h b, v8f c, int index) {
  *out = __builtin_amdgcn_swmmac_f32_16x16x64_f16(0, a, 0, b, c, index, false, true);
}

// CHECK-LABEL: define dso_local void @_Z28test_swmmac_f16_16x16x64_f16PDv8_DF16_Dv16_DF16_Dv32_DF16_S_i(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <16 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]], i32 noundef [[INDEX:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[INDEX_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[C_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[C_ADDR]] to ptr
// CHECK-NEXT:    [[INDEX_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[INDEX_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    store i32 [[INDEX]], ptr [[INDEX_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR_ASCAST]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[INDEX_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call contract <8 x half> @llvm.amdgcn.swmmac.f16.16x16x64.f16.v8f16.v16f16.v32f16.i32(i1 false, <16 x half> [[TMP0]], i1 false, <32 x half> [[TMP1]], <8 x half> [[TMP2]], i32 [[TMP3]], i1 false, i1 true)
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP4]], ptr [[TMP5]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_swmmac_f16_16x16x64_f16(v8h *out, v16h a, v32h b, v8h c, int index) {
  *out = __builtin_amdgcn_swmmac_f16_16x16x64_f16(0, a, 0, b, c, index, false, true);
}
