; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-- -mcpu=x86-64    | FileCheck %s -check-prefixes=SSE,SSE2
; RUN: llc < %s -mtriple=x86_64-- -mcpu=x86-64-v2 | FileCheck %s -check-prefixes=SSE,SSE42
; RUN: llc < %s -mtriple=x86_64-- -mcpu=x86-64-v3 | FileCheck %s -check-prefixes=AVX2
; RUN: llc < %s -mtriple=x86_64-- -mcpu=knl | FileCheck %s -check-prefixes=AVX512,AVX512F
; RUN: llc < %s -mtriple=x86_64-- -mcpu=x86-64-v4 | FileCheck %s -check-prefixes=AVX512,AVX512VL
; RUN: llc < %s -mtriple=x86_64-- -mcpu=x86-64-v4 -mattr=+avx512vbmi2 | FileCheck %s -check-prefixes=AVX512,AVX512VBMI

define i512 @fshl_i512(i512 %a0, i512 %a1, i512 %a2) nounwind {
; SSE2-LABEL: fshl_i512:
; SSE2:       # %bb.0:
; SSE2-NEXT:    pushq %r15
; SSE2-NEXT:    pushq %r14
; SSE2-NEXT:    pushq %r13
; SSE2-NEXT:    pushq %r12
; SSE2-NEXT:    pushq %rbx
; SSE2-NEXT:    movq %rcx, %rax
; SSE2-NEXT:    movq {{[0-9]+}}(%rsp), %rbx
; SSE2-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; SSE2-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; SSE2-NEXT:    testl $256, %ecx # imm = 0x100
; SSE2-NEXT:    movq %r8, %r10
; SSE2-NEXT:    cmovneq %r11, %r10
; SSE2-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; SSE2-NEXT:    movq %rdx, %r13
; SSE2-NEXT:    cmovneq %r15, %r13
; SSE2-NEXT:    movq {{[0-9]+}}(%rsp), %r12
; SSE2-NEXT:    movq %rax, %r14
; SSE2-NEXT:    cmovneq %rbx, %r14
; SSE2-NEXT:    cmovneq %rsi, %r9
; SSE2-NEXT:    cmovneq %r12, %rsi
; SSE2-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rax
; SSE2-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r8
; SSE2-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rdx
; SSE2-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r11
; SSE2-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r15
; SSE2-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rbx
; SSE2-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r12
; SSE2-NEXT:    testb $-128, %cl
; SSE2-NEXT:    cmovneq %r9, %rax
; SSE2-NEXT:    cmovneq %r14, %r9
; SSE2-NEXT:    cmovneq %rsi, %r14
; SSE2-NEXT:    cmoveq %rbx, %r12
; SSE2-NEXT:    cmoveq %rsi, %rbx
; SSE2-NEXT:    movq %r10, %rsi
; SSE2-NEXT:    cmovneq %r13, %rsi
; SSE2-NEXT:    cmoveq %r11, %r15
; SSE2-NEXT:    cmoveq %r13, %r11
; SSE2-NEXT:    cmoveq %rdx, %r10
; SSE2-NEXT:    cmoveq %r8, %rdx
; SSE2-NEXT:    testb $64, %cl
; SSE2-NEXT:    cmovneq %rax, %rdx
; SSE2-NEXT:    cmovneq %r10, %rax
; SSE2-NEXT:    cmovneq %r9, %r10
; SSE2-NEXT:    movq %r9, %r8
; SSE2-NEXT:    cmovneq %rsi, %r8
; SSE2-NEXT:    cmovneq %r14, %rsi
; SSE2-NEXT:    cmovneq %r11, %r14
; SSE2-NEXT:    cmovneq %rbx, %r11
; SSE2-NEXT:    cmovneq %r15, %rbx
; SSE2-NEXT:    cmovneq %r12, %r15
; SSE2-NEXT:    movq %r8, %r9
; SSE2-NEXT:    shldq %cl, %rsi, %r9
; SSE2-NEXT:    movq %rbx, %r12
; SSE2-NEXT:    shldq %cl, %r15, %r12
; SSE2-NEXT:    movq %r11, %r15
; SSE2-NEXT:    shldq %cl, %rbx, %r15
; SSE2-NEXT:    movq %r14, %rbx
; SSE2-NEXT:    shldq %cl, %r11, %rbx
; SSE2-NEXT:    shldq %cl, %r14, %rsi
; SSE2-NEXT:    movq %r10, %r11
; SSE2-NEXT:    shldq %cl, %r8, %r11
; SSE2-NEXT:    movq %rax, %r8
; SSE2-NEXT:    shldq %cl, %r10, %r8
; SSE2-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE2-NEXT:    shldq %cl, %rax, %rdx
; SSE2-NEXT:    movq %rdi, %rax
; SSE2-NEXT:    movq %rdx, 56(%rdi)
; SSE2-NEXT:    movq %r8, 48(%rdi)
; SSE2-NEXT:    movq %r11, 40(%rdi)
; SSE2-NEXT:    movq %r9, 32(%rdi)
; SSE2-NEXT:    movq %rsi, 24(%rdi)
; SSE2-NEXT:    movq %rbx, 16(%rdi)
; SSE2-NEXT:    movq %r15, 8(%rdi)
; SSE2-NEXT:    movq %r12, (%rdi)
; SSE2-NEXT:    popq %rbx
; SSE2-NEXT:    popq %r12
; SSE2-NEXT:    popq %r13
; SSE2-NEXT:    popq %r14
; SSE2-NEXT:    popq %r15
; SSE2-NEXT:    retq
;
; SSE42-LABEL: fshl_i512:
; SSE42:       # %bb.0:
; SSE42-NEXT:    pushq %r15
; SSE42-NEXT:    pushq %r14
; SSE42-NEXT:    pushq %r13
; SSE42-NEXT:    pushq %r12
; SSE42-NEXT:    pushq %rbx
; SSE42-NEXT:    movq %rcx, %rax
; SSE42-NEXT:    movq {{[0-9]+}}(%rsp), %r14
; SSE42-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; SSE42-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; SSE42-NEXT:    testl $256, %ecx # imm = 0x100
; SSE42-NEXT:    movq %r8, %r10
; SSE42-NEXT:    cmovneq %r11, %r10
; SSE42-NEXT:    movq {{[0-9]+}}(%rsp), %r12
; SSE42-NEXT:    movq %rdx, %r13
; SSE42-NEXT:    cmovneq %r12, %r13
; SSE42-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; SSE42-NEXT:    movq %rax, %rbx
; SSE42-NEXT:    cmovneq %r15, %rbx
; SSE42-NEXT:    cmovneq %rsi, %r9
; SSE42-NEXT:    cmovneq %r14, %rsi
; SSE42-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rax
; SSE42-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r8
; SSE42-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rdx
; SSE42-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r11
; SSE42-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r12
; SSE42-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r15
; SSE42-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r14
; SSE42-NEXT:    testb $-128, %cl
; SSE42-NEXT:    cmovneq %r9, %rax
; SSE42-NEXT:    cmovneq %rbx, %r9
; SSE42-NEXT:    cmovneq %rsi, %rbx
; SSE42-NEXT:    cmoveq %r15, %r14
; SSE42-NEXT:    cmoveq %rsi, %r15
; SSE42-NEXT:    movq %r10, %rsi
; SSE42-NEXT:    cmovneq %r13, %rsi
; SSE42-NEXT:    cmoveq %r11, %r12
; SSE42-NEXT:    cmoveq %r13, %r11
; SSE42-NEXT:    cmoveq %rdx, %r10
; SSE42-NEXT:    cmoveq %r8, %rdx
; SSE42-NEXT:    testb $64, %cl
; SSE42-NEXT:    cmovneq %rax, %rdx
; SSE42-NEXT:    cmovneq %r10, %rax
; SSE42-NEXT:    cmovneq %r9, %r10
; SSE42-NEXT:    movq %r9, %r8
; SSE42-NEXT:    cmovneq %rsi, %r8
; SSE42-NEXT:    cmovneq %rbx, %rsi
; SSE42-NEXT:    cmovneq %r11, %rbx
; SSE42-NEXT:    cmovneq %r15, %r11
; SSE42-NEXT:    cmovneq %r12, %r15
; SSE42-NEXT:    cmovneq %r14, %r12
; SSE42-NEXT:    movq %r8, %r9
; SSE42-NEXT:    shldq %cl, %rsi, %r9
; SSE42-NEXT:    movq %r15, %r14
; SSE42-NEXT:    shldq %cl, %r12, %r14
; SSE42-NEXT:    movq %r11, %r12
; SSE42-NEXT:    shldq %cl, %r15, %r12
; SSE42-NEXT:    movq %rbx, %r15
; SSE42-NEXT:    shldq %cl, %r11, %r15
; SSE42-NEXT:    shldq %cl, %rbx, %rsi
; SSE42-NEXT:    movq %r10, %r11
; SSE42-NEXT:    shldq %cl, %r8, %r11
; SSE42-NEXT:    movq %rax, %r8
; SSE42-NEXT:    shldq %cl, %r10, %r8
; SSE42-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE42-NEXT:    shldq %cl, %rax, %rdx
; SSE42-NEXT:    movq %rdi, %rax
; SSE42-NEXT:    movq %rdx, 56(%rdi)
; SSE42-NEXT:    movq %r8, 48(%rdi)
; SSE42-NEXT:    movq %r11, 40(%rdi)
; SSE42-NEXT:    movq %r9, 32(%rdi)
; SSE42-NEXT:    movq %rsi, 24(%rdi)
; SSE42-NEXT:    movq %r15, 16(%rdi)
; SSE42-NEXT:    movq %r12, 8(%rdi)
; SSE42-NEXT:    movq %r14, (%rdi)
; SSE42-NEXT:    popq %rbx
; SSE42-NEXT:    popq %r12
; SSE42-NEXT:    popq %r13
; SSE42-NEXT:    popq %r14
; SSE42-NEXT:    popq %r15
; SSE42-NEXT:    retq
;
; AVX2-LABEL: fshl_i512:
; AVX2:       # %bb.0:
; AVX2-NEXT:    pushq %r15
; AVX2-NEXT:    pushq %r14
; AVX2-NEXT:    pushq %r13
; AVX2-NEXT:    pushq %r12
; AVX2-NEXT:    pushq %rbx
; AVX2-NEXT:    movq %rcx, %rax
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %rbx
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %r12
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX2-NEXT:    testl $256, %ecx # imm = 0x100
; AVX2-NEXT:    movq %r8, %r10
; AVX2-NEXT:    cmovneq %r11, %r10
; AVX2-NEXT:    movq %rdx, %r13
; AVX2-NEXT:    cmovneq %r15, %r13
; AVX2-NEXT:    movq %rax, %r14
; AVX2-NEXT:    cmovneq %rbx, %r14
; AVX2-NEXT:    cmovneq %rsi, %r9
; AVX2-NEXT:    cmovneq %r12, %rsi
; AVX2-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rax
; AVX2-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r8
; AVX2-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rdx
; AVX2-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r11
; AVX2-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r15
; AVX2-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rbx
; AVX2-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r12
; AVX2-NEXT:    testb $-128, %cl
; AVX2-NEXT:    cmovneq %r9, %rax
; AVX2-NEXT:    cmovneq %r14, %r9
; AVX2-NEXT:    cmovneq %rsi, %r14
; AVX2-NEXT:    cmoveq %rbx, %r12
; AVX2-NEXT:    cmoveq %rsi, %rbx
; AVX2-NEXT:    movq %r10, %rsi
; AVX2-NEXT:    cmovneq %r13, %rsi
; AVX2-NEXT:    cmoveq %r11, %r15
; AVX2-NEXT:    cmoveq %r13, %r11
; AVX2-NEXT:    cmoveq %rdx, %r10
; AVX2-NEXT:    cmoveq %r8, %rdx
; AVX2-NEXT:    testb $64, %cl
; AVX2-NEXT:    cmovneq %rax, %rdx
; AVX2-NEXT:    cmovneq %r10, %rax
; AVX2-NEXT:    cmovneq %r9, %r10
; AVX2-NEXT:    movq %r9, %r8
; AVX2-NEXT:    cmovneq %rsi, %r8
; AVX2-NEXT:    cmovneq %r14, %rsi
; AVX2-NEXT:    cmovneq %r11, %r14
; AVX2-NEXT:    cmovneq %rbx, %r11
; AVX2-NEXT:    cmovneq %r15, %rbx
; AVX2-NEXT:    cmovneq %r12, %r15
; AVX2-NEXT:    movq %r8, %r9
; AVX2-NEXT:    shldq %cl, %rsi, %r9
; AVX2-NEXT:    movq %rbx, %r12
; AVX2-NEXT:    shldq %cl, %r15, %r12
; AVX2-NEXT:    movq %r11, %r15
; AVX2-NEXT:    shldq %cl, %rbx, %r15
; AVX2-NEXT:    movq %r14, %rbx
; AVX2-NEXT:    shldq %cl, %r11, %rbx
; AVX2-NEXT:    shldq %cl, %r14, %rsi
; AVX2-NEXT:    movq %r10, %r11
; AVX2-NEXT:    shldq %cl, %r8, %r11
; AVX2-NEXT:    movq %rax, %r8
; AVX2-NEXT:    shldq %cl, %r10, %r8
; AVX2-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX2-NEXT:    shldq %cl, %rax, %rdx
; AVX2-NEXT:    movq %rdx, 56(%rdi)
; AVX2-NEXT:    movq %r8, 48(%rdi)
; AVX2-NEXT:    movq %r11, 40(%rdi)
; AVX2-NEXT:    movq %r9, 32(%rdi)
; AVX2-NEXT:    movq %rsi, 24(%rdi)
; AVX2-NEXT:    movq %rbx, 16(%rdi)
; AVX2-NEXT:    movq %r15, 8(%rdi)
; AVX2-NEXT:    movq %r12, (%rdi)
; AVX2-NEXT:    movq %rdi, %rax
; AVX2-NEXT:    popq %rbx
; AVX2-NEXT:    popq %r12
; AVX2-NEXT:    popq %r13
; AVX2-NEXT:    popq %r14
; AVX2-NEXT:    popq %r15
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: fshl_i512:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    pushq %r15
; AVX512F-NEXT:    pushq %r14
; AVX512F-NEXT:    pushq %r13
; AVX512F-NEXT:    pushq %r12
; AVX512F-NEXT:    pushq %rbx
; AVX512F-NEXT:    movq %rcx, %rax
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %rbx
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %r12
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX512F-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512F-NEXT:    movq %r8, %r10
; AVX512F-NEXT:    cmovneq %r11, %r10
; AVX512F-NEXT:    movq %rdx, %r13
; AVX512F-NEXT:    cmovneq %r15, %r13
; AVX512F-NEXT:    movq %rax, %r14
; AVX512F-NEXT:    cmovneq %rbx, %r14
; AVX512F-NEXT:    cmovneq %rsi, %r9
; AVX512F-NEXT:    cmovneq %r12, %rsi
; AVX512F-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rax
; AVX512F-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r8
; AVX512F-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rdx
; AVX512F-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r11
; AVX512F-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r15
; AVX512F-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rbx
; AVX512F-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r12
; AVX512F-NEXT:    testb $-128, %cl
; AVX512F-NEXT:    cmovneq %r9, %rax
; AVX512F-NEXT:    cmovneq %r14, %r9
; AVX512F-NEXT:    cmovneq %rsi, %r14
; AVX512F-NEXT:    cmoveq %rbx, %r12
; AVX512F-NEXT:    cmoveq %rsi, %rbx
; AVX512F-NEXT:    movq %r10, %rsi
; AVX512F-NEXT:    cmovneq %r13, %rsi
; AVX512F-NEXT:    cmoveq %r11, %r15
; AVX512F-NEXT:    cmoveq %r13, %r11
; AVX512F-NEXT:    cmoveq %rdx, %r10
; AVX512F-NEXT:    cmoveq %r8, %rdx
; AVX512F-NEXT:    testb $64, %cl
; AVX512F-NEXT:    cmovneq %rax, %rdx
; AVX512F-NEXT:    cmovneq %r10, %rax
; AVX512F-NEXT:    cmovneq %r9, %r10
; AVX512F-NEXT:    movq %r9, %r8
; AVX512F-NEXT:    cmovneq %rsi, %r8
; AVX512F-NEXT:    cmovneq %r14, %rsi
; AVX512F-NEXT:    cmovneq %r11, %r14
; AVX512F-NEXT:    cmovneq %rbx, %r11
; AVX512F-NEXT:    cmovneq %r15, %rbx
; AVX512F-NEXT:    cmovneq %r12, %r15
; AVX512F-NEXT:    movq %r8, %r9
; AVX512F-NEXT:    shldq %cl, %rsi, %r9
; AVX512F-NEXT:    movq %rbx, %r12
; AVX512F-NEXT:    shldq %cl, %r15, %r12
; AVX512F-NEXT:    movq %r11, %r15
; AVX512F-NEXT:    shldq %cl, %rbx, %r15
; AVX512F-NEXT:    movq %r14, %rbx
; AVX512F-NEXT:    shldq %cl, %r11, %rbx
; AVX512F-NEXT:    shldq %cl, %r14, %rsi
; AVX512F-NEXT:    movq %r10, %r11
; AVX512F-NEXT:    shldq %cl, %r8, %r11
; AVX512F-NEXT:    movq %rax, %r8
; AVX512F-NEXT:    shldq %cl, %r10, %r8
; AVX512F-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512F-NEXT:    shldq %cl, %rax, %rdx
; AVX512F-NEXT:    movq %rdx, 56(%rdi)
; AVX512F-NEXT:    movq %r8, 48(%rdi)
; AVX512F-NEXT:    movq %r11, 40(%rdi)
; AVX512F-NEXT:    movq %r9, 32(%rdi)
; AVX512F-NEXT:    movq %rsi, 24(%rdi)
; AVX512F-NEXT:    movq %rbx, 16(%rdi)
; AVX512F-NEXT:    movq %r15, 8(%rdi)
; AVX512F-NEXT:    movq %r12, (%rdi)
; AVX512F-NEXT:    movq %rdi, %rax
; AVX512F-NEXT:    popq %rbx
; AVX512F-NEXT:    popq %r12
; AVX512F-NEXT:    popq %r13
; AVX512F-NEXT:    popq %r14
; AVX512F-NEXT:    popq %r15
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: fshl_i512:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    pushq %r15
; AVX512VL-NEXT:    pushq %r14
; AVX512VL-NEXT:    pushq %r13
; AVX512VL-NEXT:    pushq %r12
; AVX512VL-NEXT:    pushq %rbx
; AVX512VL-NEXT:    movq %rcx, %rax
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %rbx
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %r12
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX512VL-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VL-NEXT:    movq %r8, %r10
; AVX512VL-NEXT:    cmovneq %r11, %r10
; AVX512VL-NEXT:    movq %rdx, %r13
; AVX512VL-NEXT:    cmovneq %r15, %r13
; AVX512VL-NEXT:    movq %rax, %r14
; AVX512VL-NEXT:    cmovneq %rbx, %r14
; AVX512VL-NEXT:    cmovneq %rsi, %r9
; AVX512VL-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rax
; AVX512VL-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r8
; AVX512VL-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rdx
; AVX512VL-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r11
; AVX512VL-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r15
; AVX512VL-NEXT:    cmovneq %r12, %rsi
; AVX512VL-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rbx
; AVX512VL-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r12
; AVX512VL-NEXT:    testb $-128, %cl
; AVX512VL-NEXT:    cmovneq %r9, %rax
; AVX512VL-NEXT:    cmovneq %r14, %r9
; AVX512VL-NEXT:    cmovneq %rsi, %r14
; AVX512VL-NEXT:    cmoveq %rbx, %r12
; AVX512VL-NEXT:    cmoveq %rsi, %rbx
; AVX512VL-NEXT:    movq %r10, %rsi
; AVX512VL-NEXT:    cmovneq %r13, %rsi
; AVX512VL-NEXT:    cmoveq %r11, %r15
; AVX512VL-NEXT:    cmoveq %r13, %r11
; AVX512VL-NEXT:    cmoveq %rdx, %r10
; AVX512VL-NEXT:    cmoveq %r8, %rdx
; AVX512VL-NEXT:    testb $64, %cl
; AVX512VL-NEXT:    cmovneq %rax, %rdx
; AVX512VL-NEXT:    cmovneq %r10, %rax
; AVX512VL-NEXT:    cmovneq %r9, %r10
; AVX512VL-NEXT:    movq %r9, %r8
; AVX512VL-NEXT:    cmovneq %rsi, %r8
; AVX512VL-NEXT:    cmovneq %r14, %rsi
; AVX512VL-NEXT:    cmovneq %r11, %r14
; AVX512VL-NEXT:    cmovneq %rbx, %r11
; AVX512VL-NEXT:    cmovneq %r15, %rbx
; AVX512VL-NEXT:    cmovneq %r12, %r15
; AVX512VL-NEXT:    movq %r8, %r9
; AVX512VL-NEXT:    shldq %cl, %rsi, %r9
; AVX512VL-NEXT:    movq %rbx, %r12
; AVX512VL-NEXT:    shldq %cl, %r15, %r12
; AVX512VL-NEXT:    movq %r11, %r15
; AVX512VL-NEXT:    shldq %cl, %rbx, %r15
; AVX512VL-NEXT:    movq %r14, %rbx
; AVX512VL-NEXT:    shldq %cl, %r11, %rbx
; AVX512VL-NEXT:    shldq %cl, %r14, %rsi
; AVX512VL-NEXT:    movq %r10, %r11
; AVX512VL-NEXT:    shldq %cl, %r8, %r11
; AVX512VL-NEXT:    movq %rax, %r8
; AVX512VL-NEXT:    shldq %cl, %r10, %r8
; AVX512VL-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VL-NEXT:    shldq %cl, %rax, %rdx
; AVX512VL-NEXT:    movq %rdx, 56(%rdi)
; AVX512VL-NEXT:    movq %r8, 48(%rdi)
; AVX512VL-NEXT:    movq %r11, 40(%rdi)
; AVX512VL-NEXT:    movq %r9, 32(%rdi)
; AVX512VL-NEXT:    movq %rsi, 24(%rdi)
; AVX512VL-NEXT:    movq %rbx, 16(%rdi)
; AVX512VL-NEXT:    movq %r15, 8(%rdi)
; AVX512VL-NEXT:    movq %r12, (%rdi)
; AVX512VL-NEXT:    movq %rdi, %rax
; AVX512VL-NEXT:    popq %rbx
; AVX512VL-NEXT:    popq %r12
; AVX512VL-NEXT:    popq %r13
; AVX512VL-NEXT:    popq %r14
; AVX512VL-NEXT:    popq %r15
; AVX512VL-NEXT:    retq
;
; AVX512VBMI-LABEL: fshl_i512:
; AVX512VBMI:       # %bb.0:
; AVX512VBMI-NEXT:    pushq %r15
; AVX512VBMI-NEXT:    pushq %r14
; AVX512VBMI-NEXT:    pushq %r13
; AVX512VBMI-NEXT:    pushq %r12
; AVX512VBMI-NEXT:    pushq %rbx
; AVX512VBMI-NEXT:    movq %rcx, %rax
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %rbx
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %r12
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX512VBMI-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VBMI-NEXT:    movq %r8, %r10
; AVX512VBMI-NEXT:    cmovneq %r11, %r10
; AVX512VBMI-NEXT:    movq %rdx, %r13
; AVX512VBMI-NEXT:    cmovneq %r15, %r13
; AVX512VBMI-NEXT:    movq %rax, %r14
; AVX512VBMI-NEXT:    cmovneq %rbx, %r14
; AVX512VBMI-NEXT:    cmovneq %rsi, %r9
; AVX512VBMI-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rax
; AVX512VBMI-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r8
; AVX512VBMI-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rdx
; AVX512VBMI-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r11
; AVX512VBMI-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r15
; AVX512VBMI-NEXT:    cmovneq %r12, %rsi
; AVX512VBMI-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rbx
; AVX512VBMI-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r12
; AVX512VBMI-NEXT:    testb $-128, %cl
; AVX512VBMI-NEXT:    cmovneq %r9, %rax
; AVX512VBMI-NEXT:    cmovneq %r14, %r9
; AVX512VBMI-NEXT:    cmovneq %rsi, %r14
; AVX512VBMI-NEXT:    cmoveq %rbx, %r12
; AVX512VBMI-NEXT:    cmoveq %rsi, %rbx
; AVX512VBMI-NEXT:    movq %r10, %rsi
; AVX512VBMI-NEXT:    cmovneq %r13, %rsi
; AVX512VBMI-NEXT:    cmoveq %r11, %r15
; AVX512VBMI-NEXT:    cmoveq %r13, %r11
; AVX512VBMI-NEXT:    cmoveq %rdx, %r10
; AVX512VBMI-NEXT:    cmoveq %r8, %rdx
; AVX512VBMI-NEXT:    testb $64, %cl
; AVX512VBMI-NEXT:    cmovneq %rax, %rdx
; AVX512VBMI-NEXT:    cmovneq %r10, %rax
; AVX512VBMI-NEXT:    cmovneq %r9, %r10
; AVX512VBMI-NEXT:    movq %r9, %r8
; AVX512VBMI-NEXT:    cmovneq %rsi, %r8
; AVX512VBMI-NEXT:    cmovneq %r14, %rsi
; AVX512VBMI-NEXT:    cmovneq %r11, %r14
; AVX512VBMI-NEXT:    cmovneq %rbx, %r11
; AVX512VBMI-NEXT:    cmovneq %r15, %rbx
; AVX512VBMI-NEXT:    cmovneq %r12, %r15
; AVX512VBMI-NEXT:    movq %r8, %r9
; AVX512VBMI-NEXT:    shldq %cl, %rsi, %r9
; AVX512VBMI-NEXT:    movq %rbx, %r12
; AVX512VBMI-NEXT:    shldq %cl, %r15, %r12
; AVX512VBMI-NEXT:    movq %r11, %r15
; AVX512VBMI-NEXT:    shldq %cl, %rbx, %r15
; AVX512VBMI-NEXT:    movq %r14, %rbx
; AVX512VBMI-NEXT:    shldq %cl, %r11, %rbx
; AVX512VBMI-NEXT:    shldq %cl, %r14, %rsi
; AVX512VBMI-NEXT:    movq %r10, %r11
; AVX512VBMI-NEXT:    shldq %cl, %r8, %r11
; AVX512VBMI-NEXT:    movq %rax, %r8
; AVX512VBMI-NEXT:    shldq %cl, %r10, %r8
; AVX512VBMI-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VBMI-NEXT:    shldq %cl, %rax, %rdx
; AVX512VBMI-NEXT:    movq %rdx, 56(%rdi)
; AVX512VBMI-NEXT:    movq %r8, 48(%rdi)
; AVX512VBMI-NEXT:    movq %r11, 40(%rdi)
; AVX512VBMI-NEXT:    movq %r9, 32(%rdi)
; AVX512VBMI-NEXT:    movq %rsi, 24(%rdi)
; AVX512VBMI-NEXT:    movq %rbx, 16(%rdi)
; AVX512VBMI-NEXT:    movq %r15, 8(%rdi)
; AVX512VBMI-NEXT:    movq %r12, (%rdi)
; AVX512VBMI-NEXT:    movq %rdi, %rax
; AVX512VBMI-NEXT:    popq %rbx
; AVX512VBMI-NEXT:    popq %r12
; AVX512VBMI-NEXT:    popq %r13
; AVX512VBMI-NEXT:    popq %r14
; AVX512VBMI-NEXT:    popq %r15
; AVX512VBMI-NEXT:    retq
  %r = call i512 @llvm.fshl.i512(i512 %a0, i512 %a1, i512 %a2)
  ret i512 %r
}

define i512 @fshr_i512(i512 %a0, i512 %a1, i512 %a2) nounwind {
; SSE-LABEL: fshr_i512:
; SSE:       # %bb.0:
; SSE-NEXT:    pushq %r15
; SSE-NEXT:    pushq %r14
; SSE-NEXT:    pushq %r13
; SSE-NEXT:    pushq %r12
; SSE-NEXT:    pushq %rbx
; SSE-NEXT:    movq %rcx, %rax
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %r12
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; SSE-NEXT:    testl $256, %ecx # imm = 0x100
; SSE-NEXT:    cmoveq %rsi, %r9
; SSE-NEXT:    movq %r8, %rbx
; SSE-NEXT:    cmoveq %r11, %rbx
; SSE-NEXT:    movq %rdx, %r13
; SSE-NEXT:    cmoveq %r12, %r13
; SSE-NEXT:    cmoveq %r10, %rsi
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %r14
; SSE-NEXT:    movq %rax, %r15
; SSE-NEXT:    cmoveq %r14, %r15
; SSE-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rax
; SSE-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r8
; SSE-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rdx
; SSE-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r14
; SSE-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r10
; SSE-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r11
; SSE-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r12
; SSE-NEXT:    testb $-128, %cl
; SSE-NEXT:    cmoveq %r9, %rax
; SSE-NEXT:    cmoveq %r15, %r9
; SSE-NEXT:    cmoveq %rsi, %r15
; SSE-NEXT:    cmovneq %r14, %r10
; SSE-NEXT:    cmovneq %rsi, %r14
; SSE-NEXT:    movq %rbx, %rsi
; SSE-NEXT:    cmoveq %r13, %rsi
; SSE-NEXT:    cmovneq %r11, %r12
; SSE-NEXT:    cmovneq %r13, %r11
; SSE-NEXT:    cmovneq %rdx, %rbx
; SSE-NEXT:    cmovneq %r8, %rdx
; SSE-NEXT:    testb $64, %cl
; SSE-NEXT:    cmoveq %rax, %rdx
; SSE-NEXT:    cmoveq %rbx, %rax
; SSE-NEXT:    cmoveq %r9, %rbx
; SSE-NEXT:    movq %r9, %r8
; SSE-NEXT:    cmoveq %rsi, %r8
; SSE-NEXT:    cmoveq %r15, %rsi
; SSE-NEXT:    cmoveq %r11, %r15
; SSE-NEXT:    cmoveq %r14, %r11
; SSE-NEXT:    cmovneq %r12, %r10
; SSE-NEXT:    cmoveq %r12, %r14
; SSE-NEXT:    movq %rsi, %r9
; SSE-NEXT:    shrdq %cl, %r8, %r9
; SSE-NEXT:    shrdq %cl, %r14, %r10
; SSE-NEXT:    shrdq %cl, %r11, %r14
; SSE-NEXT:    shrdq %cl, %r15, %r11
; SSE-NEXT:    shrdq %cl, %rsi, %r15
; SSE-NEXT:    shrdq %cl, %rbx, %r8
; SSE-NEXT:    shrdq %cl, %rax, %rbx
; SSE-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE-NEXT:    shrdq %cl, %rdx, %rax
; SSE-NEXT:    movq %rax, 56(%rdi)
; SSE-NEXT:    movq %rbx, 48(%rdi)
; SSE-NEXT:    movq %r8, 40(%rdi)
; SSE-NEXT:    movq %r9, 32(%rdi)
; SSE-NEXT:    movq %r15, 24(%rdi)
; SSE-NEXT:    movq %r11, 16(%rdi)
; SSE-NEXT:    movq %r14, 8(%rdi)
; SSE-NEXT:    movq %r10, (%rdi)
; SSE-NEXT:    movq %rdi, %rax
; SSE-NEXT:    popq %rbx
; SSE-NEXT:    popq %r12
; SSE-NEXT:    popq %r13
; SSE-NEXT:    popq %r14
; SSE-NEXT:    popq %r15
; SSE-NEXT:    retq
;
; AVX2-LABEL: fshr_i512:
; AVX2:       # %bb.0:
; AVX2-NEXT:    pushq %r15
; AVX2-NEXT:    pushq %r14
; AVX2-NEXT:    pushq %r13
; AVX2-NEXT:    pushq %r12
; AVX2-NEXT:    pushq %rbx
; AVX2-NEXT:    movq %rcx, %rax
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %r12
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %rbx
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX2-NEXT:    testl $256, %ecx # imm = 0x100
; AVX2-NEXT:    cmoveq %rsi, %r9
; AVX2-NEXT:    movq %r8, %r14
; AVX2-NEXT:    cmoveq %rbx, %r14
; AVX2-NEXT:    movq %rdx, %r13
; AVX2-NEXT:    cmoveq %r12, %r13
; AVX2-NEXT:    cmoveq %r10, %rsi
; AVX2-NEXT:    movq %rax, %r15
; AVX2-NEXT:    cmoveq %r11, %r15
; AVX2-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rax
; AVX2-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r8
; AVX2-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rdx
; AVX2-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r11
; AVX2-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r10
; AVX2-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rbx
; AVX2-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r12
; AVX2-NEXT:    testb $-128, %cl
; AVX2-NEXT:    cmoveq %r9, %rax
; AVX2-NEXT:    cmoveq %r15, %r9
; AVX2-NEXT:    cmoveq %rsi, %r15
; AVX2-NEXT:    cmovneq %r11, %r10
; AVX2-NEXT:    cmovneq %rsi, %r11
; AVX2-NEXT:    movq %r14, %rsi
; AVX2-NEXT:    cmoveq %r13, %rsi
; AVX2-NEXT:    cmovneq %rbx, %r12
; AVX2-NEXT:    cmovneq %r13, %rbx
; AVX2-NEXT:    cmovneq %rdx, %r14
; AVX2-NEXT:    cmovneq %r8, %rdx
; AVX2-NEXT:    testb $64, %cl
; AVX2-NEXT:    cmoveq %rax, %rdx
; AVX2-NEXT:    cmoveq %r14, %rax
; AVX2-NEXT:    cmoveq %r9, %r14
; AVX2-NEXT:    movq %r9, %r8
; AVX2-NEXT:    cmoveq %rsi, %r8
; AVX2-NEXT:    cmoveq %r15, %rsi
; AVX2-NEXT:    cmoveq %rbx, %r15
; AVX2-NEXT:    cmoveq %r11, %rbx
; AVX2-NEXT:    cmovneq %r12, %r10
; AVX2-NEXT:    cmoveq %r12, %r11
; AVX2-NEXT:    movq %rsi, %r9
; AVX2-NEXT:    shrdq %cl, %r8, %r9
; AVX2-NEXT:    shrdq %cl, %r11, %r10
; AVX2-NEXT:    shrdq %cl, %rbx, %r11
; AVX2-NEXT:    shrdq %cl, %r15, %rbx
; AVX2-NEXT:    shrdq %cl, %rsi, %r15
; AVX2-NEXT:    shrdq %cl, %r14, %r8
; AVX2-NEXT:    shrdq %cl, %rax, %r14
; AVX2-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX2-NEXT:    shrdq %cl, %rdx, %rax
; AVX2-NEXT:    movq %rax, 56(%rdi)
; AVX2-NEXT:    movq %r14, 48(%rdi)
; AVX2-NEXT:    movq %r8, 40(%rdi)
; AVX2-NEXT:    movq %r9, 32(%rdi)
; AVX2-NEXT:    movq %r15, 24(%rdi)
; AVX2-NEXT:    movq %rbx, 16(%rdi)
; AVX2-NEXT:    movq %r11, 8(%rdi)
; AVX2-NEXT:    movq %r10, (%rdi)
; AVX2-NEXT:    movq %rdi, %rax
; AVX2-NEXT:    popq %rbx
; AVX2-NEXT:    popq %r12
; AVX2-NEXT:    popq %r13
; AVX2-NEXT:    popq %r14
; AVX2-NEXT:    popq %r15
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: fshr_i512:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    pushq %r15
; AVX512F-NEXT:    pushq %r14
; AVX512F-NEXT:    pushq %r13
; AVX512F-NEXT:    pushq %r12
; AVX512F-NEXT:    pushq %rbx
; AVX512F-NEXT:    movq %rcx, %rax
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %r12
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %rbx
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX512F-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512F-NEXT:    cmoveq %rsi, %r9
; AVX512F-NEXT:    movq %r8, %r14
; AVX512F-NEXT:    cmoveq %rbx, %r14
; AVX512F-NEXT:    movq %rdx, %r13
; AVX512F-NEXT:    cmoveq %r12, %r13
; AVX512F-NEXT:    cmoveq %r10, %rsi
; AVX512F-NEXT:    movq %rax, %r15
; AVX512F-NEXT:    cmoveq %r11, %r15
; AVX512F-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rax
; AVX512F-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r8
; AVX512F-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rdx
; AVX512F-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r11
; AVX512F-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r10
; AVX512F-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rbx
; AVX512F-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r12
; AVX512F-NEXT:    testb $-128, %cl
; AVX512F-NEXT:    cmoveq %r9, %rax
; AVX512F-NEXT:    cmoveq %r15, %r9
; AVX512F-NEXT:    cmoveq %rsi, %r15
; AVX512F-NEXT:    cmovneq %r11, %r10
; AVX512F-NEXT:    cmovneq %rsi, %r11
; AVX512F-NEXT:    movq %r14, %rsi
; AVX512F-NEXT:    cmoveq %r13, %rsi
; AVX512F-NEXT:    cmovneq %rbx, %r12
; AVX512F-NEXT:    cmovneq %r13, %rbx
; AVX512F-NEXT:    cmovneq %rdx, %r14
; AVX512F-NEXT:    cmovneq %r8, %rdx
; AVX512F-NEXT:    testb $64, %cl
; AVX512F-NEXT:    cmoveq %rax, %rdx
; AVX512F-NEXT:    cmoveq %r14, %rax
; AVX512F-NEXT:    cmoveq %r9, %r14
; AVX512F-NEXT:    movq %r9, %r8
; AVX512F-NEXT:    cmoveq %rsi, %r8
; AVX512F-NEXT:    cmoveq %r15, %rsi
; AVX512F-NEXT:    cmoveq %rbx, %r15
; AVX512F-NEXT:    cmoveq %r11, %rbx
; AVX512F-NEXT:    cmovneq %r12, %r10
; AVX512F-NEXT:    cmoveq %r12, %r11
; AVX512F-NEXT:    movq %rsi, %r9
; AVX512F-NEXT:    shrdq %cl, %r8, %r9
; AVX512F-NEXT:    shrdq %cl, %r11, %r10
; AVX512F-NEXT:    shrdq %cl, %rbx, %r11
; AVX512F-NEXT:    shrdq %cl, %r15, %rbx
; AVX512F-NEXT:    shrdq %cl, %rsi, %r15
; AVX512F-NEXT:    shrdq %cl, %r14, %r8
; AVX512F-NEXT:    shrdq %cl, %rax, %r14
; AVX512F-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512F-NEXT:    shrdq %cl, %rdx, %rax
; AVX512F-NEXT:    movq %rax, 56(%rdi)
; AVX512F-NEXT:    movq %r14, 48(%rdi)
; AVX512F-NEXT:    movq %r8, 40(%rdi)
; AVX512F-NEXT:    movq %r9, 32(%rdi)
; AVX512F-NEXT:    movq %r15, 24(%rdi)
; AVX512F-NEXT:    movq %rbx, 16(%rdi)
; AVX512F-NEXT:    movq %r11, 8(%rdi)
; AVX512F-NEXT:    movq %r10, (%rdi)
; AVX512F-NEXT:    movq %rdi, %rax
; AVX512F-NEXT:    popq %rbx
; AVX512F-NEXT:    popq %r12
; AVX512F-NEXT:    popq %r13
; AVX512F-NEXT:    popq %r14
; AVX512F-NEXT:    popq %r15
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: fshr_i512:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    pushq %r15
; AVX512VL-NEXT:    pushq %r14
; AVX512VL-NEXT:    pushq %r13
; AVX512VL-NEXT:    pushq %r12
; AVX512VL-NEXT:    pushq %rbx
; AVX512VL-NEXT:    movq %rcx, %rax
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %r12
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %rbx
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX512VL-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VL-NEXT:    cmoveq %rsi, %r9
; AVX512VL-NEXT:    movq %r8, %r14
; AVX512VL-NEXT:    cmoveq %rbx, %r14
; AVX512VL-NEXT:    movq %rdx, %r13
; AVX512VL-NEXT:    cmoveq %r10, %rsi
; AVX512VL-NEXT:    movq %rax, %r15
; AVX512VL-NEXT:    cmoveq %r11, %r15
; AVX512VL-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rax
; AVX512VL-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r8
; AVX512VL-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rdx
; AVX512VL-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r11
; AVX512VL-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r10
; AVX512VL-NEXT:    cmoveq %r12, %r13
; AVX512VL-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rbx
; AVX512VL-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r12
; AVX512VL-NEXT:    testb $-128, %cl
; AVX512VL-NEXT:    cmoveq %r9, %rax
; AVX512VL-NEXT:    cmoveq %r15, %r9
; AVX512VL-NEXT:    cmoveq %rsi, %r15
; AVX512VL-NEXT:    cmovneq %r11, %r10
; AVX512VL-NEXT:    cmovneq %rsi, %r11
; AVX512VL-NEXT:    movq %r14, %rsi
; AVX512VL-NEXT:    cmoveq %r13, %rsi
; AVX512VL-NEXT:    cmovneq %rbx, %r12
; AVX512VL-NEXT:    cmovneq %r13, %rbx
; AVX512VL-NEXT:    cmovneq %rdx, %r14
; AVX512VL-NEXT:    cmovneq %r8, %rdx
; AVX512VL-NEXT:    testb $64, %cl
; AVX512VL-NEXT:    cmoveq %rax, %rdx
; AVX512VL-NEXT:    cmoveq %r14, %rax
; AVX512VL-NEXT:    cmoveq %r9, %r14
; AVX512VL-NEXT:    movq %r9, %r8
; AVX512VL-NEXT:    cmoveq %rsi, %r8
; AVX512VL-NEXT:    cmoveq %r15, %rsi
; AVX512VL-NEXT:    cmoveq %rbx, %r15
; AVX512VL-NEXT:    cmoveq %r11, %rbx
; AVX512VL-NEXT:    cmovneq %r12, %r10
; AVX512VL-NEXT:    cmoveq %r12, %r11
; AVX512VL-NEXT:    movq %rsi, %r9
; AVX512VL-NEXT:    shrdq %cl, %r8, %r9
; AVX512VL-NEXT:    shrdq %cl, %r11, %r10
; AVX512VL-NEXT:    shrdq %cl, %rbx, %r11
; AVX512VL-NEXT:    shrdq %cl, %r15, %rbx
; AVX512VL-NEXT:    shrdq %cl, %rsi, %r15
; AVX512VL-NEXT:    shrdq %cl, %r14, %r8
; AVX512VL-NEXT:    shrdq %cl, %rax, %r14
; AVX512VL-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VL-NEXT:    shrdq %cl, %rdx, %rax
; AVX512VL-NEXT:    movq %rax, 56(%rdi)
; AVX512VL-NEXT:    movq %r14, 48(%rdi)
; AVX512VL-NEXT:    movq %r8, 40(%rdi)
; AVX512VL-NEXT:    movq %r9, 32(%rdi)
; AVX512VL-NEXT:    movq %r15, 24(%rdi)
; AVX512VL-NEXT:    movq %rbx, 16(%rdi)
; AVX512VL-NEXT:    movq %r11, 8(%rdi)
; AVX512VL-NEXT:    movq %r10, (%rdi)
; AVX512VL-NEXT:    movq %rdi, %rax
; AVX512VL-NEXT:    popq %rbx
; AVX512VL-NEXT:    popq %r12
; AVX512VL-NEXT:    popq %r13
; AVX512VL-NEXT:    popq %r14
; AVX512VL-NEXT:    popq %r15
; AVX512VL-NEXT:    retq
;
; AVX512VBMI-LABEL: fshr_i512:
; AVX512VBMI:       # %bb.0:
; AVX512VBMI-NEXT:    pushq %r15
; AVX512VBMI-NEXT:    pushq %r14
; AVX512VBMI-NEXT:    pushq %r13
; AVX512VBMI-NEXT:    pushq %r12
; AVX512VBMI-NEXT:    pushq %rbx
; AVX512VBMI-NEXT:    movq %rcx, %rax
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %r12
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %rbx
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX512VBMI-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VBMI-NEXT:    cmoveq %rsi, %r9
; AVX512VBMI-NEXT:    movq %r8, %r14
; AVX512VBMI-NEXT:    cmoveq %rbx, %r14
; AVX512VBMI-NEXT:    movq %rdx, %r13
; AVX512VBMI-NEXT:    cmoveq %r10, %rsi
; AVX512VBMI-NEXT:    movq %rax, %r15
; AVX512VBMI-NEXT:    cmoveq %r11, %r15
; AVX512VBMI-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rax
; AVX512VBMI-NEXT:    cmovneq {{[0-9]+}}(%rsp), %r8
; AVX512VBMI-NEXT:    cmovneq {{[0-9]+}}(%rsp), %rdx
; AVX512VBMI-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r11
; AVX512VBMI-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r10
; AVX512VBMI-NEXT:    cmoveq %r12, %r13
; AVX512VBMI-NEXT:    cmoveq {{[0-9]+}}(%rsp), %rbx
; AVX512VBMI-NEXT:    cmoveq {{[0-9]+}}(%rsp), %r12
; AVX512VBMI-NEXT:    testb $-128, %cl
; AVX512VBMI-NEXT:    cmoveq %r9, %rax
; AVX512VBMI-NEXT:    cmoveq %r15, %r9
; AVX512VBMI-NEXT:    cmoveq %rsi, %r15
; AVX512VBMI-NEXT:    cmovneq %r11, %r10
; AVX512VBMI-NEXT:    cmovneq %rsi, %r11
; AVX512VBMI-NEXT:    movq %r14, %rsi
; AVX512VBMI-NEXT:    cmoveq %r13, %rsi
; AVX512VBMI-NEXT:    cmovneq %rbx, %r12
; AVX512VBMI-NEXT:    cmovneq %r13, %rbx
; AVX512VBMI-NEXT:    cmovneq %rdx, %r14
; AVX512VBMI-NEXT:    cmovneq %r8, %rdx
; AVX512VBMI-NEXT:    testb $64, %cl
; AVX512VBMI-NEXT:    cmoveq %rax, %rdx
; AVX512VBMI-NEXT:    cmoveq %r14, %rax
; AVX512VBMI-NEXT:    cmoveq %r9, %r14
; AVX512VBMI-NEXT:    movq %r9, %r8
; AVX512VBMI-NEXT:    cmoveq %rsi, %r8
; AVX512VBMI-NEXT:    cmoveq %r15, %rsi
; AVX512VBMI-NEXT:    cmoveq %rbx, %r15
; AVX512VBMI-NEXT:    cmoveq %r11, %rbx
; AVX512VBMI-NEXT:    cmovneq %r12, %r10
; AVX512VBMI-NEXT:    cmoveq %r12, %r11
; AVX512VBMI-NEXT:    movq %rsi, %r9
; AVX512VBMI-NEXT:    shrdq %cl, %r8, %r9
; AVX512VBMI-NEXT:    shrdq %cl, %r11, %r10
; AVX512VBMI-NEXT:    shrdq %cl, %rbx, %r11
; AVX512VBMI-NEXT:    shrdq %cl, %r15, %rbx
; AVX512VBMI-NEXT:    shrdq %cl, %rsi, %r15
; AVX512VBMI-NEXT:    shrdq %cl, %r14, %r8
; AVX512VBMI-NEXT:    shrdq %cl, %rax, %r14
; AVX512VBMI-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VBMI-NEXT:    shrdq %cl, %rdx, %rax
; AVX512VBMI-NEXT:    movq %rax, 56(%rdi)
; AVX512VBMI-NEXT:    movq %r14, 48(%rdi)
; AVX512VBMI-NEXT:    movq %r8, 40(%rdi)
; AVX512VBMI-NEXT:    movq %r9, 32(%rdi)
; AVX512VBMI-NEXT:    movq %r15, 24(%rdi)
; AVX512VBMI-NEXT:    movq %rbx, 16(%rdi)
; AVX512VBMI-NEXT:    movq %r11, 8(%rdi)
; AVX512VBMI-NEXT:    movq %r10, (%rdi)
; AVX512VBMI-NEXT:    movq %rdi, %rax
; AVX512VBMI-NEXT:    popq %rbx
; AVX512VBMI-NEXT:    popq %r12
; AVX512VBMI-NEXT:    popq %r13
; AVX512VBMI-NEXT:    popq %r14
; AVX512VBMI-NEXT:    popq %r15
; AVX512VBMI-NEXT:    retq
  %r = call i512 @llvm.fshr.i512(i512 %a0, i512 %a1, i512 %a2)
  ret i512 %r
}

define i512 @fshl_rot_i512(i512 %a0, i512 %a1) nounwind {
; SSE-LABEL: fshl_rot_i512:
; SSE:       # %bb.0:
; SSE-NEXT:    pushq %r15
; SSE-NEXT:    pushq %r14
; SSE-NEXT:    pushq %r13
; SSE-NEXT:    pushq %r12
; SSE-NEXT:    pushq %rbx
; SSE-NEXT:    movq %rcx, %rax
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; SSE-NEXT:    testl $256, %ecx # imm = 0x100
; SSE-NEXT:    movq %r9, %r12
; SSE-NEXT:    cmovneq %rsi, %r12
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; SSE-NEXT:    movq %r8, %r13
; SSE-NEXT:    cmovneq %r15, %r13
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; SSE-NEXT:    movq %rax, %rbx
; SSE-NEXT:    cmovneq %r10, %rbx
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; SSE-NEXT:    movq %rdx, %r14
; SSE-NEXT:    cmovneq %r11, %r14
; SSE-NEXT:    cmovneq %r9, %rsi
; SSE-NEXT:    cmovneq %rax, %r10
; SSE-NEXT:    cmovneq %rdx, %r11
; SSE-NEXT:    cmovneq %r8, %r15
; SSE-NEXT:    testb $-128, %cl
; SSE-NEXT:    movq %r14, %r8
; SSE-NEXT:    cmovneq %r15, %r8
; SSE-NEXT:    cmovneq %r11, %r15
; SSE-NEXT:    movq %rbx, %rdx
; SSE-NEXT:    cmovneq %rsi, %rdx
; SSE-NEXT:    cmovneq %r10, %rsi
; SSE-NEXT:    cmoveq %r13, %r14
; SSE-NEXT:    cmoveq %r12, %rbx
; SSE-NEXT:    cmovneq %r13, %r11
; SSE-NEXT:    cmovneq %r12, %r10
; SSE-NEXT:    testb $64, %cl
; SSE-NEXT:    movq %r10, %rax
; SSE-NEXT:    cmovneq %r11, %rax
; SSE-NEXT:    cmovneq %rbx, %r11
; SSE-NEXT:    cmovneq %r14, %rbx
; SSE-NEXT:    cmovneq %rdx, %r14
; SSE-NEXT:    cmovneq %r8, %rdx
; SSE-NEXT:    cmovneq %rsi, %r8
; SSE-NEXT:    cmoveq %r15, %r10
; SSE-NEXT:    cmovneq %r15, %rsi
; SSE-NEXT:    movq %rsi, %r9
; SSE-NEXT:    shldq %cl, %r10, %r9
; SSE-NEXT:    movq %r8, %r15
; SSE-NEXT:    shldq %cl, %rsi, %r15
; SSE-NEXT:    movq %rdx, %rsi
; SSE-NEXT:    shldq %cl, %r8, %rsi
; SSE-NEXT:    movq %r14, %r8
; SSE-NEXT:    shldq %cl, %rdx, %r8
; SSE-NEXT:    movq %rbx, %rdx
; SSE-NEXT:    shldq %cl, %r14, %rdx
; SSE-NEXT:    movq %r11, %r14
; SSE-NEXT:    shldq %cl, %rbx, %r14
; SSE-NEXT:    movq %rax, %rbx
; SSE-NEXT:    shldq %cl, %r11, %rbx
; SSE-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE-NEXT:    shldq %cl, %rax, %r10
; SSE-NEXT:    movq %rdi, %rax
; SSE-NEXT:    movq %r10, 56(%rdi)
; SSE-NEXT:    movq %rbx, 48(%rdi)
; SSE-NEXT:    movq %r14, 40(%rdi)
; SSE-NEXT:    movq %rdx, 32(%rdi)
; SSE-NEXT:    movq %r8, 24(%rdi)
; SSE-NEXT:    movq %rsi, 16(%rdi)
; SSE-NEXT:    movq %r15, 8(%rdi)
; SSE-NEXT:    movq %r9, (%rdi)
; SSE-NEXT:    popq %rbx
; SSE-NEXT:    popq %r12
; SSE-NEXT:    popq %r13
; SSE-NEXT:    popq %r14
; SSE-NEXT:    popq %r15
; SSE-NEXT:    retq
;
; AVX2-LABEL: fshl_rot_i512:
; AVX2:       # %bb.0:
; AVX2-NEXT:    pushq %r15
; AVX2-NEXT:    pushq %r14
; AVX2-NEXT:    pushq %r13
; AVX2-NEXT:    pushq %r12
; AVX2-NEXT:    pushq %rbx
; AVX2-NEXT:    movq %rcx, %rax
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX2-NEXT:    testl $256, %ecx # imm = 0x100
; AVX2-NEXT:    movq %r9, %r12
; AVX2-NEXT:    cmovneq %rsi, %r12
; AVX2-NEXT:    movq %r8, %r13
; AVX2-NEXT:    cmovneq %r15, %r13
; AVX2-NEXT:    movq %rax, %rbx
; AVX2-NEXT:    cmovneq %r10, %rbx
; AVX2-NEXT:    movq %rdx, %r14
; AVX2-NEXT:    cmovneq %r11, %r14
; AVX2-NEXT:    cmovneq %r9, %rsi
; AVX2-NEXT:    cmovneq %rax, %r10
; AVX2-NEXT:    cmovneq %rdx, %r11
; AVX2-NEXT:    cmovneq %r8, %r15
; AVX2-NEXT:    testb $-128, %cl
; AVX2-NEXT:    movq %r14, %r9
; AVX2-NEXT:    cmovneq %r15, %r9
; AVX2-NEXT:    cmovneq %r11, %r15
; AVX2-NEXT:    movq %rbx, %r8
; AVX2-NEXT:    cmovneq %rsi, %r8
; AVX2-NEXT:    cmovneq %r10, %rsi
; AVX2-NEXT:    cmoveq %r13, %r14
; AVX2-NEXT:    cmoveq %r12, %rbx
; AVX2-NEXT:    cmovneq %r13, %r11
; AVX2-NEXT:    movq %rdi, %rax
; AVX2-NEXT:    cmovneq %r12, %r10
; AVX2-NEXT:    testb $64, %cl
; AVX2-NEXT:    movq %r10, %rdx
; AVX2-NEXT:    cmovneq %r11, %rdx
; AVX2-NEXT:    cmovneq %rbx, %r11
; AVX2-NEXT:    cmovneq %r14, %rbx
; AVX2-NEXT:    cmovneq %r8, %r14
; AVX2-NEXT:    cmovneq %r9, %r8
; AVX2-NEXT:    cmovneq %rsi, %r9
; AVX2-NEXT:    cmoveq %r15, %r10
; AVX2-NEXT:    cmovneq %r15, %rsi
; AVX2-NEXT:    movq %rsi, %rdi
; AVX2-NEXT:    shldq %cl, %r10, %rdi
; AVX2-NEXT:    movq %r9, %r15
; AVX2-NEXT:    shldq %cl, %rsi, %r15
; AVX2-NEXT:    movq %r8, %rsi
; AVX2-NEXT:    shldq %cl, %r9, %rsi
; AVX2-NEXT:    movq %r14, %r9
; AVX2-NEXT:    shldq %cl, %r8, %r9
; AVX2-NEXT:    movq %rbx, %r8
; AVX2-NEXT:    shldq %cl, %r14, %r8
; AVX2-NEXT:    movq %r11, %r14
; AVX2-NEXT:    shldq %cl, %rbx, %r14
; AVX2-NEXT:    movq %rdx, %rbx
; AVX2-NEXT:    shldq %cl, %r11, %rbx
; AVX2-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX2-NEXT:    shldq %cl, %rdx, %r10
; AVX2-NEXT:    movq %r10, 56(%rax)
; AVX2-NEXT:    movq %rbx, 48(%rax)
; AVX2-NEXT:    movq %r14, 40(%rax)
; AVX2-NEXT:    movq %r8, 32(%rax)
; AVX2-NEXT:    movq %r9, 24(%rax)
; AVX2-NEXT:    movq %rsi, 16(%rax)
; AVX2-NEXT:    movq %r15, 8(%rax)
; AVX2-NEXT:    movq %rdi, (%rax)
; AVX2-NEXT:    popq %rbx
; AVX2-NEXT:    popq %r12
; AVX2-NEXT:    popq %r13
; AVX2-NEXT:    popq %r14
; AVX2-NEXT:    popq %r15
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: fshl_rot_i512:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    pushq %r15
; AVX512F-NEXT:    pushq %r14
; AVX512F-NEXT:    pushq %r13
; AVX512F-NEXT:    pushq %r12
; AVX512F-NEXT:    pushq %rbx
; AVX512F-NEXT:    movq %rcx, %rax
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX512F-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512F-NEXT:    movq %r9, %r12
; AVX512F-NEXT:    cmovneq %rsi, %r12
; AVX512F-NEXT:    movq %r8, %r13
; AVX512F-NEXT:    cmovneq %r15, %r13
; AVX512F-NEXT:    movq %rax, %rbx
; AVX512F-NEXT:    cmovneq %r10, %rbx
; AVX512F-NEXT:    movq %rdx, %r14
; AVX512F-NEXT:    cmovneq %r11, %r14
; AVX512F-NEXT:    cmovneq %r9, %rsi
; AVX512F-NEXT:    cmovneq %rax, %r10
; AVX512F-NEXT:    cmovneq %rdx, %r11
; AVX512F-NEXT:    cmovneq %r8, %r15
; AVX512F-NEXT:    testb $-128, %cl
; AVX512F-NEXT:    movq %r14, %r9
; AVX512F-NEXT:    cmovneq %r15, %r9
; AVX512F-NEXT:    cmovneq %r11, %r15
; AVX512F-NEXT:    movq %rbx, %r8
; AVX512F-NEXT:    cmovneq %rsi, %r8
; AVX512F-NEXT:    cmovneq %r10, %rsi
; AVX512F-NEXT:    cmoveq %r13, %r14
; AVX512F-NEXT:    cmoveq %r12, %rbx
; AVX512F-NEXT:    cmovneq %r13, %r11
; AVX512F-NEXT:    movq %rdi, %rax
; AVX512F-NEXT:    cmovneq %r12, %r10
; AVX512F-NEXT:    testb $64, %cl
; AVX512F-NEXT:    movq %r10, %rdx
; AVX512F-NEXT:    cmovneq %r11, %rdx
; AVX512F-NEXT:    cmovneq %rbx, %r11
; AVX512F-NEXT:    cmovneq %r14, %rbx
; AVX512F-NEXT:    cmovneq %r8, %r14
; AVX512F-NEXT:    cmovneq %r9, %r8
; AVX512F-NEXT:    cmovneq %rsi, %r9
; AVX512F-NEXT:    cmoveq %r15, %r10
; AVX512F-NEXT:    cmovneq %r15, %rsi
; AVX512F-NEXT:    movq %rsi, %rdi
; AVX512F-NEXT:    shldq %cl, %r10, %rdi
; AVX512F-NEXT:    movq %r9, %r15
; AVX512F-NEXT:    shldq %cl, %rsi, %r15
; AVX512F-NEXT:    movq %r8, %rsi
; AVX512F-NEXT:    shldq %cl, %r9, %rsi
; AVX512F-NEXT:    movq %r14, %r9
; AVX512F-NEXT:    shldq %cl, %r8, %r9
; AVX512F-NEXT:    movq %rbx, %r8
; AVX512F-NEXT:    shldq %cl, %r14, %r8
; AVX512F-NEXT:    movq %r11, %r14
; AVX512F-NEXT:    shldq %cl, %rbx, %r14
; AVX512F-NEXT:    movq %rdx, %rbx
; AVX512F-NEXT:    shldq %cl, %r11, %rbx
; AVX512F-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512F-NEXT:    shldq %cl, %rdx, %r10
; AVX512F-NEXT:    movq %r10, 56(%rax)
; AVX512F-NEXT:    movq %rbx, 48(%rax)
; AVX512F-NEXT:    movq %r14, 40(%rax)
; AVX512F-NEXT:    movq %r8, 32(%rax)
; AVX512F-NEXT:    movq %r9, 24(%rax)
; AVX512F-NEXT:    movq %rsi, 16(%rax)
; AVX512F-NEXT:    movq %r15, 8(%rax)
; AVX512F-NEXT:    movq %rdi, (%rax)
; AVX512F-NEXT:    popq %rbx
; AVX512F-NEXT:    popq %r12
; AVX512F-NEXT:    popq %r13
; AVX512F-NEXT:    popq %r14
; AVX512F-NEXT:    popq %r15
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: fshl_rot_i512:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    pushq %r15
; AVX512VL-NEXT:    pushq %r14
; AVX512VL-NEXT:    pushq %r13
; AVX512VL-NEXT:    pushq %r12
; AVX512VL-NEXT:    pushq %rbx
; AVX512VL-NEXT:    movq %rcx, %rax
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX512VL-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VL-NEXT:    movq %r9, %r12
; AVX512VL-NEXT:    cmovneq %rsi, %r12
; AVX512VL-NEXT:    movq %r8, %r13
; AVX512VL-NEXT:    cmovneq %r15, %r13
; AVX512VL-NEXT:    movq %rax, %rbx
; AVX512VL-NEXT:    cmovneq %r10, %rbx
; AVX512VL-NEXT:    movq %rdx, %r14
; AVX512VL-NEXT:    cmovneq %r11, %r14
; AVX512VL-NEXT:    cmovneq %r9, %rsi
; AVX512VL-NEXT:    cmovneq %rax, %r10
; AVX512VL-NEXT:    cmovneq %rdx, %r11
; AVX512VL-NEXT:    cmovneq %r8, %r15
; AVX512VL-NEXT:    testb $-128, %cl
; AVX512VL-NEXT:    movq %r14, %r8
; AVX512VL-NEXT:    cmovneq %r15, %r8
; AVX512VL-NEXT:    cmovneq %r11, %r15
; AVX512VL-NEXT:    movq %rbx, %rax
; AVX512VL-NEXT:    cmovneq %rsi, %rax
; AVX512VL-NEXT:    cmovneq %r10, %rsi
; AVX512VL-NEXT:    cmoveq %r13, %r14
; AVX512VL-NEXT:    cmoveq %r12, %rbx
; AVX512VL-NEXT:    cmovneq %r13, %r11
; AVX512VL-NEXT:    cmovneq %r12, %r10
; AVX512VL-NEXT:    testb $64, %cl
; AVX512VL-NEXT:    movq %r10, %rdx
; AVX512VL-NEXT:    cmovneq %r11, %rdx
; AVX512VL-NEXT:    cmovneq %rbx, %r11
; AVX512VL-NEXT:    cmovneq %r14, %rbx
; AVX512VL-NEXT:    cmovneq %rax, %r14
; AVX512VL-NEXT:    cmovneq %r8, %rax
; AVX512VL-NEXT:    cmovneq %rsi, %r8
; AVX512VL-NEXT:    cmoveq %r15, %r10
; AVX512VL-NEXT:    cmovneq %r15, %rsi
; AVX512VL-NEXT:    movq %rsi, %r9
; AVX512VL-NEXT:    shldq %cl, %r10, %r9
; AVX512VL-NEXT:    movq %r8, %r15
; AVX512VL-NEXT:    shldq %cl, %rsi, %r15
; AVX512VL-NEXT:    movq %rax, %rsi
; AVX512VL-NEXT:    shldq %cl, %r8, %rsi
; AVX512VL-NEXT:    movq %r14, %r8
; AVX512VL-NEXT:    shldq %cl, %rax, %r8
; AVX512VL-NEXT:    movq %rbx, %r12
; AVX512VL-NEXT:    shldq %cl, %r14, %r12
; AVX512VL-NEXT:    movq %r11, %r14
; AVX512VL-NEXT:    shldq %cl, %rbx, %r14
; AVX512VL-NEXT:    movq %rdi, %rax
; AVX512VL-NEXT:    movq %rdx, %rdi
; AVX512VL-NEXT:    shldq %cl, %r11, %rdi
; AVX512VL-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VL-NEXT:    shldq %cl, %rdx, %r10
; AVX512VL-NEXT:    movq %r10, 56(%rax)
; AVX512VL-NEXT:    movq %rdi, 48(%rax)
; AVX512VL-NEXT:    movq %r14, 40(%rax)
; AVX512VL-NEXT:    movq %r12, 32(%rax)
; AVX512VL-NEXT:    movq %r8, 24(%rax)
; AVX512VL-NEXT:    movq %rsi, 16(%rax)
; AVX512VL-NEXT:    movq %r15, 8(%rax)
; AVX512VL-NEXT:    movq %r9, (%rax)
; AVX512VL-NEXT:    popq %rbx
; AVX512VL-NEXT:    popq %r12
; AVX512VL-NEXT:    popq %r13
; AVX512VL-NEXT:    popq %r14
; AVX512VL-NEXT:    popq %r15
; AVX512VL-NEXT:    retq
;
; AVX512VBMI-LABEL: fshl_rot_i512:
; AVX512VBMI:       # %bb.0:
; AVX512VBMI-NEXT:    pushq %r15
; AVX512VBMI-NEXT:    pushq %r14
; AVX512VBMI-NEXT:    pushq %r13
; AVX512VBMI-NEXT:    pushq %r12
; AVX512VBMI-NEXT:    pushq %rbx
; AVX512VBMI-NEXT:    movq %rcx, %rax
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX512VBMI-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VBMI-NEXT:    movq %r9, %r12
; AVX512VBMI-NEXT:    cmovneq %rsi, %r12
; AVX512VBMI-NEXT:    movq %r8, %r13
; AVX512VBMI-NEXT:    cmovneq %r15, %r13
; AVX512VBMI-NEXT:    movq %rax, %rbx
; AVX512VBMI-NEXT:    cmovneq %r10, %rbx
; AVX512VBMI-NEXT:    movq %rdx, %r14
; AVX512VBMI-NEXT:    cmovneq %r11, %r14
; AVX512VBMI-NEXT:    cmovneq %r9, %rsi
; AVX512VBMI-NEXT:    cmovneq %rax, %r10
; AVX512VBMI-NEXT:    cmovneq %rdx, %r11
; AVX512VBMI-NEXT:    cmovneq %r8, %r15
; AVX512VBMI-NEXT:    testb $-128, %cl
; AVX512VBMI-NEXT:    movq %r14, %r8
; AVX512VBMI-NEXT:    cmovneq %r15, %r8
; AVX512VBMI-NEXT:    cmovneq %r11, %r15
; AVX512VBMI-NEXT:    movq %rbx, %rax
; AVX512VBMI-NEXT:    cmovneq %rsi, %rax
; AVX512VBMI-NEXT:    cmovneq %r10, %rsi
; AVX512VBMI-NEXT:    cmoveq %r13, %r14
; AVX512VBMI-NEXT:    cmoveq %r12, %rbx
; AVX512VBMI-NEXT:    cmovneq %r13, %r11
; AVX512VBMI-NEXT:    cmovneq %r12, %r10
; AVX512VBMI-NEXT:    testb $64, %cl
; AVX512VBMI-NEXT:    movq %r10, %rdx
; AVX512VBMI-NEXT:    cmovneq %r11, %rdx
; AVX512VBMI-NEXT:    cmovneq %rbx, %r11
; AVX512VBMI-NEXT:    cmovneq %r14, %rbx
; AVX512VBMI-NEXT:    cmovneq %rax, %r14
; AVX512VBMI-NEXT:    cmovneq %r8, %rax
; AVX512VBMI-NEXT:    cmovneq %rsi, %r8
; AVX512VBMI-NEXT:    cmoveq %r15, %r10
; AVX512VBMI-NEXT:    cmovneq %r15, %rsi
; AVX512VBMI-NEXT:    movq %rsi, %r9
; AVX512VBMI-NEXT:    shldq %cl, %r10, %r9
; AVX512VBMI-NEXT:    movq %r8, %r15
; AVX512VBMI-NEXT:    shldq %cl, %rsi, %r15
; AVX512VBMI-NEXT:    movq %rax, %rsi
; AVX512VBMI-NEXT:    shldq %cl, %r8, %rsi
; AVX512VBMI-NEXT:    movq %r14, %r8
; AVX512VBMI-NEXT:    shldq %cl, %rax, %r8
; AVX512VBMI-NEXT:    movq %rbx, %r12
; AVX512VBMI-NEXT:    shldq %cl, %r14, %r12
; AVX512VBMI-NEXT:    movq %r11, %r14
; AVX512VBMI-NEXT:    shldq %cl, %rbx, %r14
; AVX512VBMI-NEXT:    movq %rdi, %rax
; AVX512VBMI-NEXT:    movq %rdx, %rdi
; AVX512VBMI-NEXT:    shldq %cl, %r11, %rdi
; AVX512VBMI-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VBMI-NEXT:    shldq %cl, %rdx, %r10
; AVX512VBMI-NEXT:    movq %r10, 56(%rax)
; AVX512VBMI-NEXT:    movq %rdi, 48(%rax)
; AVX512VBMI-NEXT:    movq %r14, 40(%rax)
; AVX512VBMI-NEXT:    movq %r12, 32(%rax)
; AVX512VBMI-NEXT:    movq %r8, 24(%rax)
; AVX512VBMI-NEXT:    movq %rsi, 16(%rax)
; AVX512VBMI-NEXT:    movq %r15, 8(%rax)
; AVX512VBMI-NEXT:    movq %r9, (%rax)
; AVX512VBMI-NEXT:    popq %rbx
; AVX512VBMI-NEXT:    popq %r12
; AVX512VBMI-NEXT:    popq %r13
; AVX512VBMI-NEXT:    popq %r14
; AVX512VBMI-NEXT:    popq %r15
; AVX512VBMI-NEXT:    retq
  %r = call i512 @llvm.fshl.i512(i512 %a0, i512 %a0, i512 %a1)
  ret i512 %r
}

define i512 @fshr_rot_i512(i512 %a0, i512 %a1) nounwind {
; SSE-LABEL: fshr_rot_i512:
; SSE:       # %bb.0:
; SSE-NEXT:    pushq %r15
; SSE-NEXT:    pushq %r14
; SSE-NEXT:    pushq %r13
; SSE-NEXT:    pushq %r12
; SSE-NEXT:    pushq %rbx
; SSE-NEXT:    movq %rcx, %rax
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; SSE-NEXT:    testl $256, %ecx # imm = 0x100
; SSE-NEXT:    movq %r9, %r12
; SSE-NEXT:    cmoveq %rsi, %r12
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; SSE-NEXT:    movq %r8, %r13
; SSE-NEXT:    cmoveq %r15, %r13
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %r14
; SSE-NEXT:    movq %rax, %r10
; SSE-NEXT:    cmoveq %r14, %r10
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %rbx
; SSE-NEXT:    movq %rdx, %r11
; SSE-NEXT:    cmoveq %rbx, %r11
; SSE-NEXT:    cmoveq %r9, %rsi
; SSE-NEXT:    cmoveq %rax, %r14
; SSE-NEXT:    cmoveq %rdx, %rbx
; SSE-NEXT:    cmoveq %r8, %r15
; SSE-NEXT:    testb $-128, %cl
; SSE-NEXT:    movq %r11, %rdx
; SSE-NEXT:    cmoveq %r15, %rdx
; SSE-NEXT:    cmoveq %rbx, %r15
; SSE-NEXT:    movq %r10, %r8
; SSE-NEXT:    cmoveq %rsi, %r8
; SSE-NEXT:    cmoveq %r14, %rsi
; SSE-NEXT:    cmovneq %r13, %r11
; SSE-NEXT:    cmovneq %r12, %r10
; SSE-NEXT:    cmoveq %r13, %rbx
; SSE-NEXT:    cmoveq %r12, %r14
; SSE-NEXT:    testb $64, %cl
; SSE-NEXT:    movq %r14, %r9
; SSE-NEXT:    cmoveq %rbx, %r9
; SSE-NEXT:    cmoveq %r10, %rbx
; SSE-NEXT:    cmoveq %r11, %r10
; SSE-NEXT:    cmoveq %r8, %r11
; SSE-NEXT:    cmoveq %rdx, %r8
; SSE-NEXT:    cmoveq %rsi, %rdx
; SSE-NEXT:    cmoveq %r15, %rsi
; SSE-NEXT:    cmovneq %r15, %r14
; SSE-NEXT:    movq %r14, %r15
; SSE-NEXT:    shrdq %cl, %rsi, %r15
; SSE-NEXT:    shrdq %cl, %rdx, %rsi
; SSE-NEXT:    shrdq %cl, %r8, %rdx
; SSE-NEXT:    shrdq %cl, %r11, %r8
; SSE-NEXT:    shrdq %cl, %r10, %r11
; SSE-NEXT:    shrdq %cl, %rbx, %r10
; SSE-NEXT:    shrdq %cl, %r9, %rbx
; SSE-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE-NEXT:    shrdq %cl, %r14, %r9
; SSE-NEXT:    movq %rdi, %rax
; SSE-NEXT:    movq %r9, 56(%rdi)
; SSE-NEXT:    movq %rbx, 48(%rdi)
; SSE-NEXT:    movq %r10, 40(%rdi)
; SSE-NEXT:    movq %r11, 32(%rdi)
; SSE-NEXT:    movq %r8, 24(%rdi)
; SSE-NEXT:    movq %rdx, 16(%rdi)
; SSE-NEXT:    movq %rsi, 8(%rdi)
; SSE-NEXT:    movq %r15, (%rdi)
; SSE-NEXT:    popq %rbx
; SSE-NEXT:    popq %r12
; SSE-NEXT:    popq %r13
; SSE-NEXT:    popq %r14
; SSE-NEXT:    popq %r15
; SSE-NEXT:    retq
;
; AVX2-LABEL: fshr_rot_i512:
; AVX2:       # %bb.0:
; AVX2-NEXT:    pushq %r15
; AVX2-NEXT:    pushq %r14
; AVX2-NEXT:    pushq %r13
; AVX2-NEXT:    pushq %r12
; AVX2-NEXT:    pushq %rbx
; AVX2-NEXT:    movq %rcx, %rax
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX2-NEXT:    testl $256, %ecx # imm = 0x100
; AVX2-NEXT:    movq %r9, %r12
; AVX2-NEXT:    cmoveq %rsi, %r12
; AVX2-NEXT:    movq %r8, %r13
; AVX2-NEXT:    cmoveq %r15, %r13
; AVX2-NEXT:    movq %rax, %rbx
; AVX2-NEXT:    cmoveq %r11, %rbx
; AVX2-NEXT:    movq %rdx, %r14
; AVX2-NEXT:    cmoveq %r10, %r14
; AVX2-NEXT:    cmoveq %r9, %rsi
; AVX2-NEXT:    cmoveq %rax, %r11
; AVX2-NEXT:    cmoveq %rdx, %r10
; AVX2-NEXT:    cmoveq %r8, %r15
; AVX2-NEXT:    testb $-128, %cl
; AVX2-NEXT:    movq %r14, %rdx
; AVX2-NEXT:    cmoveq %r15, %rdx
; AVX2-NEXT:    cmoveq %r10, %r15
; AVX2-NEXT:    movq %rbx, %r8
; AVX2-NEXT:    cmoveq %rsi, %r8
; AVX2-NEXT:    cmoveq %r11, %rsi
; AVX2-NEXT:    cmovneq %r13, %r14
; AVX2-NEXT:    cmovneq %r12, %rbx
; AVX2-NEXT:    cmoveq %r13, %r10
; AVX2-NEXT:    movq %rdi, %rax
; AVX2-NEXT:    cmoveq %r12, %r11
; AVX2-NEXT:    testb $64, %cl
; AVX2-NEXT:    movq %r11, %rdi
; AVX2-NEXT:    cmoveq %r10, %rdi
; AVX2-NEXT:    cmoveq %rbx, %r10
; AVX2-NEXT:    cmoveq %r14, %rbx
; AVX2-NEXT:    cmoveq %r8, %r14
; AVX2-NEXT:    cmoveq %rdx, %r8
; AVX2-NEXT:    cmoveq %rsi, %rdx
; AVX2-NEXT:    cmoveq %r15, %rsi
; AVX2-NEXT:    cmovneq %r15, %r11
; AVX2-NEXT:    movq %r11, %r9
; AVX2-NEXT:    shrdq %cl, %rsi, %r9
; AVX2-NEXT:    shrdq %cl, %rdx, %rsi
; AVX2-NEXT:    shrdq %cl, %r8, %rdx
; AVX2-NEXT:    shrdq %cl, %r14, %r8
; AVX2-NEXT:    shrdq %cl, %rbx, %r14
; AVX2-NEXT:    shrdq %cl, %r10, %rbx
; AVX2-NEXT:    shrdq %cl, %rdi, %r10
; AVX2-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX2-NEXT:    shrdq %cl, %r11, %rdi
; AVX2-NEXT:    movq %rdi, 56(%rax)
; AVX2-NEXT:    movq %r10, 48(%rax)
; AVX2-NEXT:    movq %rbx, 40(%rax)
; AVX2-NEXT:    movq %r14, 32(%rax)
; AVX2-NEXT:    movq %r8, 24(%rax)
; AVX2-NEXT:    movq %rdx, 16(%rax)
; AVX2-NEXT:    movq %rsi, 8(%rax)
; AVX2-NEXT:    movq %r9, (%rax)
; AVX2-NEXT:    popq %rbx
; AVX2-NEXT:    popq %r12
; AVX2-NEXT:    popq %r13
; AVX2-NEXT:    popq %r14
; AVX2-NEXT:    popq %r15
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: fshr_rot_i512:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    pushq %r15
; AVX512F-NEXT:    pushq %r14
; AVX512F-NEXT:    pushq %r13
; AVX512F-NEXT:    pushq %r12
; AVX512F-NEXT:    pushq %rbx
; AVX512F-NEXT:    movq %rcx, %rax
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; AVX512F-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX512F-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512F-NEXT:    movq %r9, %r12
; AVX512F-NEXT:    cmoveq %rsi, %r12
; AVX512F-NEXT:    movq %r8, %r13
; AVX512F-NEXT:    cmoveq %r15, %r13
; AVX512F-NEXT:    movq %rax, %rbx
; AVX512F-NEXT:    cmoveq %r11, %rbx
; AVX512F-NEXT:    movq %rdx, %r14
; AVX512F-NEXT:    cmoveq %r10, %r14
; AVX512F-NEXT:    cmoveq %r9, %rsi
; AVX512F-NEXT:    cmoveq %rax, %r11
; AVX512F-NEXT:    cmoveq %rdx, %r10
; AVX512F-NEXT:    cmoveq %r8, %r15
; AVX512F-NEXT:    testb $-128, %cl
; AVX512F-NEXT:    movq %r14, %rdx
; AVX512F-NEXT:    cmoveq %r15, %rdx
; AVX512F-NEXT:    cmoveq %r10, %r15
; AVX512F-NEXT:    movq %rbx, %r8
; AVX512F-NEXT:    cmoveq %rsi, %r8
; AVX512F-NEXT:    cmoveq %r11, %rsi
; AVX512F-NEXT:    cmovneq %r13, %r14
; AVX512F-NEXT:    cmovneq %r12, %rbx
; AVX512F-NEXT:    cmoveq %r13, %r10
; AVX512F-NEXT:    movq %rdi, %rax
; AVX512F-NEXT:    cmoveq %r12, %r11
; AVX512F-NEXT:    testb $64, %cl
; AVX512F-NEXT:    movq %r11, %rdi
; AVX512F-NEXT:    cmoveq %r10, %rdi
; AVX512F-NEXT:    cmoveq %rbx, %r10
; AVX512F-NEXT:    cmoveq %r14, %rbx
; AVX512F-NEXT:    cmoveq %r8, %r14
; AVX512F-NEXT:    cmoveq %rdx, %r8
; AVX512F-NEXT:    cmoveq %rsi, %rdx
; AVX512F-NEXT:    cmoveq %r15, %rsi
; AVX512F-NEXT:    cmovneq %r15, %r11
; AVX512F-NEXT:    movq %r11, %r9
; AVX512F-NEXT:    shrdq %cl, %rsi, %r9
; AVX512F-NEXT:    shrdq %cl, %rdx, %rsi
; AVX512F-NEXT:    shrdq %cl, %r8, %rdx
; AVX512F-NEXT:    shrdq %cl, %r14, %r8
; AVX512F-NEXT:    shrdq %cl, %rbx, %r14
; AVX512F-NEXT:    shrdq %cl, %r10, %rbx
; AVX512F-NEXT:    shrdq %cl, %rdi, %r10
; AVX512F-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512F-NEXT:    shrdq %cl, %r11, %rdi
; AVX512F-NEXT:    movq %rdi, 56(%rax)
; AVX512F-NEXT:    movq %r10, 48(%rax)
; AVX512F-NEXT:    movq %rbx, 40(%rax)
; AVX512F-NEXT:    movq %r14, 32(%rax)
; AVX512F-NEXT:    movq %r8, 24(%rax)
; AVX512F-NEXT:    movq %rdx, 16(%rax)
; AVX512F-NEXT:    movq %rsi, 8(%rax)
; AVX512F-NEXT:    movq %r9, (%rax)
; AVX512F-NEXT:    popq %rbx
; AVX512F-NEXT:    popq %r12
; AVX512F-NEXT:    popq %r13
; AVX512F-NEXT:    popq %r14
; AVX512F-NEXT:    popq %r15
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: fshr_rot_i512:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    pushq %r15
; AVX512VL-NEXT:    pushq %r14
; AVX512VL-NEXT:    pushq %r13
; AVX512VL-NEXT:    pushq %r12
; AVX512VL-NEXT:    pushq %rbx
; AVX512VL-NEXT:    movq %rcx, %rax
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; AVX512VL-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX512VL-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VL-NEXT:    movq %r9, %r12
; AVX512VL-NEXT:    cmoveq %rsi, %r12
; AVX512VL-NEXT:    movq %r8, %r13
; AVX512VL-NEXT:    cmoveq %r15, %r13
; AVX512VL-NEXT:    movq %rax, %rbx
; AVX512VL-NEXT:    cmoveq %r11, %rbx
; AVX512VL-NEXT:    movq %rdx, %r14
; AVX512VL-NEXT:    cmoveq %r10, %r14
; AVX512VL-NEXT:    cmoveq %r9, %rsi
; AVX512VL-NEXT:    cmoveq %rax, %r11
; AVX512VL-NEXT:    cmoveq %rdx, %r10
; AVX512VL-NEXT:    cmoveq %r8, %r15
; AVX512VL-NEXT:    testb $-128, %cl
; AVX512VL-NEXT:    movq %r14, %rdx
; AVX512VL-NEXT:    cmoveq %r15, %rdx
; AVX512VL-NEXT:    cmoveq %r10, %r15
; AVX512VL-NEXT:    movq %rbx, %r8
; AVX512VL-NEXT:    cmoveq %rsi, %r8
; AVX512VL-NEXT:    cmoveq %r11, %rsi
; AVX512VL-NEXT:    cmovneq %r13, %r14
; AVX512VL-NEXT:    cmovneq %r12, %rbx
; AVX512VL-NEXT:    cmoveq %r13, %r10
; AVX512VL-NEXT:    cmoveq %r12, %r11
; AVX512VL-NEXT:    testb $64, %cl
; AVX512VL-NEXT:    movq %r11, %r9
; AVX512VL-NEXT:    cmoveq %r10, %r9
; AVX512VL-NEXT:    cmoveq %rbx, %r10
; AVX512VL-NEXT:    cmoveq %r14, %rbx
; AVX512VL-NEXT:    cmoveq %r8, %r14
; AVX512VL-NEXT:    cmoveq %rdx, %r8
; AVX512VL-NEXT:    cmoveq %rsi, %rdx
; AVX512VL-NEXT:    cmoveq %r15, %rsi
; AVX512VL-NEXT:    cmovneq %r15, %r11
; AVX512VL-NEXT:    movq %r11, %r15
; AVX512VL-NEXT:    shrdq %cl, %rsi, %r15
; AVX512VL-NEXT:    shrdq %cl, %rdx, %rsi
; AVX512VL-NEXT:    shrdq %cl, %r8, %rdx
; AVX512VL-NEXT:    shrdq %cl, %r14, %r8
; AVX512VL-NEXT:    shrdq %cl, %rbx, %r14
; AVX512VL-NEXT:    shrdq %cl, %r10, %rbx
; AVX512VL-NEXT:    movq %rdi, %rax
; AVX512VL-NEXT:    shrdq %cl, %r9, %r10
; AVX512VL-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VL-NEXT:    shrdq %cl, %r11, %r9
; AVX512VL-NEXT:    movq %r9, 56(%rdi)
; AVX512VL-NEXT:    movq %r10, 48(%rdi)
; AVX512VL-NEXT:    movq %rbx, 40(%rdi)
; AVX512VL-NEXT:    movq %r14, 32(%rdi)
; AVX512VL-NEXT:    movq %r8, 24(%rdi)
; AVX512VL-NEXT:    movq %rdx, 16(%rdi)
; AVX512VL-NEXT:    movq %rsi, 8(%rdi)
; AVX512VL-NEXT:    movq %r15, (%rdi)
; AVX512VL-NEXT:    popq %rbx
; AVX512VL-NEXT:    popq %r12
; AVX512VL-NEXT:    popq %r13
; AVX512VL-NEXT:    popq %r14
; AVX512VL-NEXT:    popq %r15
; AVX512VL-NEXT:    retq
;
; AVX512VBMI-LABEL: fshr_rot_i512:
; AVX512VBMI:       # %bb.0:
; AVX512VBMI-NEXT:    pushq %r15
; AVX512VBMI-NEXT:    pushq %r14
; AVX512VBMI-NEXT:    pushq %r13
; AVX512VBMI-NEXT:    pushq %r12
; AVX512VBMI-NEXT:    pushq %rbx
; AVX512VBMI-NEXT:    movq %rcx, %rax
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; AVX512VBMI-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; AVX512VBMI-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VBMI-NEXT:    movq %r9, %r12
; AVX512VBMI-NEXT:    cmoveq %rsi, %r12
; AVX512VBMI-NEXT:    movq %r8, %r13
; AVX512VBMI-NEXT:    cmoveq %r15, %r13
; AVX512VBMI-NEXT:    movq %rax, %rbx
; AVX512VBMI-NEXT:    cmoveq %r11, %rbx
; AVX512VBMI-NEXT:    movq %rdx, %r14
; AVX512VBMI-NEXT:    cmoveq %r10, %r14
; AVX512VBMI-NEXT:    cmoveq %r9, %rsi
; AVX512VBMI-NEXT:    cmoveq %rax, %r11
; AVX512VBMI-NEXT:    cmoveq %rdx, %r10
; AVX512VBMI-NEXT:    cmoveq %r8, %r15
; AVX512VBMI-NEXT:    testb $-128, %cl
; AVX512VBMI-NEXT:    movq %r14, %rdx
; AVX512VBMI-NEXT:    cmoveq %r15, %rdx
; AVX512VBMI-NEXT:    cmoveq %r10, %r15
; AVX512VBMI-NEXT:    movq %rbx, %r8
; AVX512VBMI-NEXT:    cmoveq %rsi, %r8
; AVX512VBMI-NEXT:    cmoveq %r11, %rsi
; AVX512VBMI-NEXT:    cmovneq %r13, %r14
; AVX512VBMI-NEXT:    cmovneq %r12, %rbx
; AVX512VBMI-NEXT:    cmoveq %r13, %r10
; AVX512VBMI-NEXT:    cmoveq %r12, %r11
; AVX512VBMI-NEXT:    testb $64, %cl
; AVX512VBMI-NEXT:    movq %r11, %r9
; AVX512VBMI-NEXT:    cmoveq %r10, %r9
; AVX512VBMI-NEXT:    cmoveq %rbx, %r10
; AVX512VBMI-NEXT:    cmoveq %r14, %rbx
; AVX512VBMI-NEXT:    cmoveq %r8, %r14
; AVX512VBMI-NEXT:    cmoveq %rdx, %r8
; AVX512VBMI-NEXT:    cmoveq %rsi, %rdx
; AVX512VBMI-NEXT:    cmoveq %r15, %rsi
; AVX512VBMI-NEXT:    cmovneq %r15, %r11
; AVX512VBMI-NEXT:    movq %r11, %r15
; AVX512VBMI-NEXT:    shrdq %cl, %rsi, %r15
; AVX512VBMI-NEXT:    shrdq %cl, %rdx, %rsi
; AVX512VBMI-NEXT:    shrdq %cl, %r8, %rdx
; AVX512VBMI-NEXT:    shrdq %cl, %r14, %r8
; AVX512VBMI-NEXT:    shrdq %cl, %rbx, %r14
; AVX512VBMI-NEXT:    shrdq %cl, %r10, %rbx
; AVX512VBMI-NEXT:    movq %rdi, %rax
; AVX512VBMI-NEXT:    shrdq %cl, %r9, %r10
; AVX512VBMI-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VBMI-NEXT:    shrdq %cl, %r11, %r9
; AVX512VBMI-NEXT:    movq %r9, 56(%rdi)
; AVX512VBMI-NEXT:    movq %r10, 48(%rdi)
; AVX512VBMI-NEXT:    movq %rbx, 40(%rdi)
; AVX512VBMI-NEXT:    movq %r14, 32(%rdi)
; AVX512VBMI-NEXT:    movq %r8, 24(%rdi)
; AVX512VBMI-NEXT:    movq %rdx, 16(%rdi)
; AVX512VBMI-NEXT:    movq %rsi, 8(%rdi)
; AVX512VBMI-NEXT:    movq %r15, (%rdi)
; AVX512VBMI-NEXT:    popq %rbx
; AVX512VBMI-NEXT:    popq %r12
; AVX512VBMI-NEXT:    popq %r13
; AVX512VBMI-NEXT:    popq %r14
; AVX512VBMI-NEXT:    popq %r15
; AVX512VBMI-NEXT:    retq
  %r = call i512 @llvm.fshr.i512(i512 %a0, i512 %a0, i512 %a1)
  ret i512 %r
}

define i512 @fshl_i512_load(ptr %p0, ptr %p1, i512 %a2) nounwind {
; SSE-LABEL: fshl_i512_load:
; SSE:       # %bb.0:
; SSE-NEXT:    pushq %rbp
; SSE-NEXT:    pushq %r15
; SSE-NEXT:    pushq %r14
; SSE-NEXT:    pushq %r13
; SSE-NEXT:    pushq %r12
; SSE-NEXT:    pushq %rbx
; SSE-NEXT:    movq (%rsi), %r10
; SSE-NEXT:    movq 8(%rsi), %r9
; SSE-NEXT:    movq 16(%rsi), %r8
; SSE-NEXT:    movq 24(%rsi), %rax
; SSE-NEXT:    movq 32(%rdx), %rbx
; SSE-NEXT:    testl $256, %ecx # imm = 0x100
; SSE-NEXT:    movq 56(%rsi), %r13
; SSE-NEXT:    cmovneq %rax, %r13
; SSE-NEXT:    movq 40(%rdx), %r12
; SSE-NEXT:    movq 48(%rsi), %rbp
; SSE-NEXT:    cmovneq %r8, %rbp
; SSE-NEXT:    movq 48(%rdx), %r14
; SSE-NEXT:    movq 40(%rsi), %r11
; SSE-NEXT:    cmovneq %r9, %r11
; SSE-NEXT:    movq 56(%rdx), %r15
; SSE-NEXT:    movq 32(%rsi), %rsi
; SSE-NEXT:    cmovneq %r10, %rsi
; SSE-NEXT:    cmovneq %r15, %rax
; SSE-NEXT:    cmovneq %r14, %r8
; SSE-NEXT:    cmovneq %r12, %r9
; SSE-NEXT:    cmovneq %rbx, %r10
; SSE-NEXT:    cmovneq 16(%rdx), %r14
; SSE-NEXT:    cmovneq (%rdx), %rbx
; SSE-NEXT:    cmovneq 8(%rdx), %r12
; SSE-NEXT:    cmovneq 24(%rdx), %r15
; SSE-NEXT:    testb $-128, %cl
; SSE-NEXT:    cmoveq %r15, %r12
; SSE-NEXT:    cmoveq %r14, %rbx
; SSE-NEXT:    cmoveq %r10, %r14
; SSE-NEXT:    cmoveq %r9, %r15
; SSE-NEXT:    cmoveq %r8, %r10
; SSE-NEXT:    cmoveq %rax, %r9
; SSE-NEXT:    cmoveq %rsi, %r8
; SSE-NEXT:    cmoveq %r11, %rax
; SSE-NEXT:    cmoveq %rbp, %rsi
; SSE-NEXT:    cmoveq %r13, %r11
; SSE-NEXT:    testb $64, %cl
; SSE-NEXT:    cmovneq %rsi, %r11
; SSE-NEXT:    cmovneq %rax, %rsi
; SSE-NEXT:    cmovneq %r8, %rax
; SSE-NEXT:    cmovneq %r9, %r8
; SSE-NEXT:    cmovneq %r10, %r9
; SSE-NEXT:    cmovneq %r15, %r10
; SSE-NEXT:    cmovneq %r14, %r15
; SSE-NEXT:    cmoveq %r12, %rbx
; SSE-NEXT:    cmovneq %r12, %r14
; SSE-NEXT:    movq %r14, %rdx
; SSE-NEXT:    shldq %cl, %rbx, %rdx
; SSE-NEXT:    movq %r15, %rbx
; SSE-NEXT:    shldq %cl, %r14, %rbx
; SSE-NEXT:    movq %r10, %r14
; SSE-NEXT:    shldq %cl, %r15, %r14
; SSE-NEXT:    movq %r9, %r15
; SSE-NEXT:    shldq %cl, %r10, %r15
; SSE-NEXT:    movq %r8, %r10
; SSE-NEXT:    shldq %cl, %r9, %r10
; SSE-NEXT:    movq %rax, %r9
; SSE-NEXT:    shldq %cl, %r8, %r9
; SSE-NEXT:    movq %rsi, %r8
; SSE-NEXT:    shldq %cl, %rax, %r8
; SSE-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE-NEXT:    shldq %cl, %rsi, %r11
; SSE-NEXT:    movq %rdi, %rax
; SSE-NEXT:    movq %r11, 56(%rdi)
; SSE-NEXT:    movq %r8, 48(%rdi)
; SSE-NEXT:    movq %r9, 40(%rdi)
; SSE-NEXT:    movq %r10, 32(%rdi)
; SSE-NEXT:    movq %r15, 24(%rdi)
; SSE-NEXT:    movq %r14, 16(%rdi)
; SSE-NEXT:    movq %rbx, 8(%rdi)
; SSE-NEXT:    movq %rdx, (%rdi)
; SSE-NEXT:    popq %rbx
; SSE-NEXT:    popq %r12
; SSE-NEXT:    popq %r13
; SSE-NEXT:    popq %r14
; SSE-NEXT:    popq %r15
; SSE-NEXT:    popq %rbp
; SSE-NEXT:    retq
;
; AVX2-LABEL: fshl_i512_load:
; AVX2:       # %bb.0:
; AVX2-NEXT:    pushq %rbp
; AVX2-NEXT:    pushq %r15
; AVX2-NEXT:    pushq %r14
; AVX2-NEXT:    pushq %r13
; AVX2-NEXT:    pushq %r12
; AVX2-NEXT:    pushq %rbx
; AVX2-NEXT:    movq (%rsi), %r11
; AVX2-NEXT:    movq 8(%rsi), %r10
; AVX2-NEXT:    movq 16(%rsi), %r9
; AVX2-NEXT:    movq 24(%rsi), %r8
; AVX2-NEXT:    movq 32(%rdx), %r14
; AVX2-NEXT:    testl $256, %ecx # imm = 0x100
; AVX2-NEXT:    movq 56(%rsi), %rax
; AVX2-NEXT:    cmovneq %r8, %rax
; AVX2-NEXT:    movq 40(%rdx), %r13
; AVX2-NEXT:    movq 48(%rsi), %rbp
; AVX2-NEXT:    cmovneq %r9, %rbp
; AVX2-NEXT:    movq 48(%rdx), %r15
; AVX2-NEXT:    movq 40(%rsi), %rbx
; AVX2-NEXT:    cmovneq %r10, %rbx
; AVX2-NEXT:    movq 56(%rdx), %r12
; AVX2-NEXT:    movq 32(%rsi), %rsi
; AVX2-NEXT:    cmovneq %r11, %rsi
; AVX2-NEXT:    cmovneq %r12, %r8
; AVX2-NEXT:    cmovneq %r15, %r9
; AVX2-NEXT:    cmovneq %r13, %r10
; AVX2-NEXT:    cmovneq %r14, %r11
; AVX2-NEXT:    cmovneq 16(%rdx), %r15
; AVX2-NEXT:    cmovneq (%rdx), %r14
; AVX2-NEXT:    cmovneq 8(%rdx), %r13
; AVX2-NEXT:    cmovneq 24(%rdx), %r12
; AVX2-NEXT:    testb $-128, %cl
; AVX2-NEXT:    cmoveq %r12, %r13
; AVX2-NEXT:    cmoveq %r15, %r14
; AVX2-NEXT:    cmoveq %r11, %r15
; AVX2-NEXT:    cmoveq %r10, %r12
; AVX2-NEXT:    cmoveq %r9, %r11
; AVX2-NEXT:    cmoveq %r8, %r10
; AVX2-NEXT:    cmoveq %rsi, %r9
; AVX2-NEXT:    cmoveq %rbx, %r8
; AVX2-NEXT:    cmoveq %rbp, %rsi
; AVX2-NEXT:    cmoveq %rax, %rbx
; AVX2-NEXT:    movq %rdi, %rax
; AVX2-NEXT:    testb $64, %cl
; AVX2-NEXT:    cmovneq %rsi, %rbx
; AVX2-NEXT:    cmovneq %r8, %rsi
; AVX2-NEXT:    cmovneq %r9, %r8
; AVX2-NEXT:    cmovneq %r10, %r9
; AVX2-NEXT:    cmovneq %r11, %r10
; AVX2-NEXT:    cmovneq %r12, %r11
; AVX2-NEXT:    cmovneq %r15, %r12
; AVX2-NEXT:    cmoveq %r13, %r14
; AVX2-NEXT:    cmovneq %r13, %r15
; AVX2-NEXT:    movq %r15, %rdx
; AVX2-NEXT:    shldq %cl, %r14, %rdx
; AVX2-NEXT:    movq %r12, %rdi
; AVX2-NEXT:    shldq %cl, %r15, %rdi
; AVX2-NEXT:    movq %r11, %r14
; AVX2-NEXT:    shldq %cl, %r12, %r14
; AVX2-NEXT:    movq %r10, %r15
; AVX2-NEXT:    shldq %cl, %r11, %r15
; AVX2-NEXT:    movq %r9, %r11
; AVX2-NEXT:    shldq %cl, %r10, %r11
; AVX2-NEXT:    movq %r8, %r10
; AVX2-NEXT:    shldq %cl, %r9, %r10
; AVX2-NEXT:    movq %rsi, %r9
; AVX2-NEXT:    shldq %cl, %r8, %r9
; AVX2-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX2-NEXT:    shldq %cl, %rsi, %rbx
; AVX2-NEXT:    movq %rbx, 56(%rax)
; AVX2-NEXT:    movq %r9, 48(%rax)
; AVX2-NEXT:    movq %r10, 40(%rax)
; AVX2-NEXT:    movq %r11, 32(%rax)
; AVX2-NEXT:    movq %r15, 24(%rax)
; AVX2-NEXT:    movq %r14, 16(%rax)
; AVX2-NEXT:    movq %rdi, 8(%rax)
; AVX2-NEXT:    movq %rdx, (%rax)
; AVX2-NEXT:    popq %rbx
; AVX2-NEXT:    popq %r12
; AVX2-NEXT:    popq %r13
; AVX2-NEXT:    popq %r14
; AVX2-NEXT:    popq %r15
; AVX2-NEXT:    popq %rbp
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: fshl_i512_load:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    pushq %rbp
; AVX512F-NEXT:    pushq %r15
; AVX512F-NEXT:    pushq %r14
; AVX512F-NEXT:    pushq %r13
; AVX512F-NEXT:    pushq %r12
; AVX512F-NEXT:    pushq %rbx
; AVX512F-NEXT:    movq (%rsi), %r11
; AVX512F-NEXT:    movq 8(%rsi), %r10
; AVX512F-NEXT:    movq 16(%rsi), %r9
; AVX512F-NEXT:    movq 24(%rsi), %r8
; AVX512F-NEXT:    movq 32(%rdx), %r14
; AVX512F-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512F-NEXT:    movq 56(%rsi), %rax
; AVX512F-NEXT:    cmovneq %r8, %rax
; AVX512F-NEXT:    movq 40(%rdx), %r13
; AVX512F-NEXT:    movq 48(%rsi), %rbp
; AVX512F-NEXT:    cmovneq %r9, %rbp
; AVX512F-NEXT:    movq 48(%rdx), %r15
; AVX512F-NEXT:    movq 40(%rsi), %rbx
; AVX512F-NEXT:    cmovneq %r10, %rbx
; AVX512F-NEXT:    movq 56(%rdx), %r12
; AVX512F-NEXT:    movq 32(%rsi), %rsi
; AVX512F-NEXT:    cmovneq %r11, %rsi
; AVX512F-NEXT:    cmovneq %r12, %r8
; AVX512F-NEXT:    cmovneq %r15, %r9
; AVX512F-NEXT:    cmovneq %r13, %r10
; AVX512F-NEXT:    cmovneq %r14, %r11
; AVX512F-NEXT:    cmovneq 16(%rdx), %r15
; AVX512F-NEXT:    cmovneq (%rdx), %r14
; AVX512F-NEXT:    cmovneq 8(%rdx), %r13
; AVX512F-NEXT:    cmovneq 24(%rdx), %r12
; AVX512F-NEXT:    testb $-128, %cl
; AVX512F-NEXT:    cmoveq %r12, %r13
; AVX512F-NEXT:    cmoveq %r15, %r14
; AVX512F-NEXT:    cmoveq %r11, %r15
; AVX512F-NEXT:    cmoveq %r10, %r12
; AVX512F-NEXT:    cmoveq %r9, %r11
; AVX512F-NEXT:    cmoveq %r8, %r10
; AVX512F-NEXT:    cmoveq %rsi, %r9
; AVX512F-NEXT:    cmoveq %rbx, %r8
; AVX512F-NEXT:    cmoveq %rbp, %rsi
; AVX512F-NEXT:    cmoveq %rax, %rbx
; AVX512F-NEXT:    movq %rdi, %rax
; AVX512F-NEXT:    testb $64, %cl
; AVX512F-NEXT:    cmovneq %rsi, %rbx
; AVX512F-NEXT:    cmovneq %r8, %rsi
; AVX512F-NEXT:    cmovneq %r9, %r8
; AVX512F-NEXT:    cmovneq %r10, %r9
; AVX512F-NEXT:    cmovneq %r11, %r10
; AVX512F-NEXT:    cmovneq %r12, %r11
; AVX512F-NEXT:    cmovneq %r15, %r12
; AVX512F-NEXT:    cmoveq %r13, %r14
; AVX512F-NEXT:    cmovneq %r13, %r15
; AVX512F-NEXT:    movq %r15, %rdx
; AVX512F-NEXT:    shldq %cl, %r14, %rdx
; AVX512F-NEXT:    movq %r12, %rdi
; AVX512F-NEXT:    shldq %cl, %r15, %rdi
; AVX512F-NEXT:    movq %r11, %r14
; AVX512F-NEXT:    shldq %cl, %r12, %r14
; AVX512F-NEXT:    movq %r10, %r15
; AVX512F-NEXT:    shldq %cl, %r11, %r15
; AVX512F-NEXT:    movq %r9, %r11
; AVX512F-NEXT:    shldq %cl, %r10, %r11
; AVX512F-NEXT:    movq %r8, %r10
; AVX512F-NEXT:    shldq %cl, %r9, %r10
; AVX512F-NEXT:    movq %rsi, %r9
; AVX512F-NEXT:    shldq %cl, %r8, %r9
; AVX512F-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512F-NEXT:    shldq %cl, %rsi, %rbx
; AVX512F-NEXT:    movq %rbx, 56(%rax)
; AVX512F-NEXT:    movq %r9, 48(%rax)
; AVX512F-NEXT:    movq %r10, 40(%rax)
; AVX512F-NEXT:    movq %r11, 32(%rax)
; AVX512F-NEXT:    movq %r15, 24(%rax)
; AVX512F-NEXT:    movq %r14, 16(%rax)
; AVX512F-NEXT:    movq %rdi, 8(%rax)
; AVX512F-NEXT:    movq %rdx, (%rax)
; AVX512F-NEXT:    popq %rbx
; AVX512F-NEXT:    popq %r12
; AVX512F-NEXT:    popq %r13
; AVX512F-NEXT:    popq %r14
; AVX512F-NEXT:    popq %r15
; AVX512F-NEXT:    popq %rbp
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: fshl_i512_load:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    pushq %rbp
; AVX512VL-NEXT:    pushq %r15
; AVX512VL-NEXT:    pushq %r14
; AVX512VL-NEXT:    pushq %r13
; AVX512VL-NEXT:    pushq %r12
; AVX512VL-NEXT:    pushq %rbx
; AVX512VL-NEXT:    movq (%rsi), %r11
; AVX512VL-NEXT:    movq 8(%rsi), %r9
; AVX512VL-NEXT:    movq 16(%rsi), %rax
; AVX512VL-NEXT:    movq 24(%rsi), %r8
; AVX512VL-NEXT:    movq 32(%rdx), %r15
; AVX512VL-NEXT:    movq 40(%rdx), %r12
; AVX512VL-NEXT:    movq 48(%rdx), %r14
; AVX512VL-NEXT:    movq 56(%rdx), %rbx
; AVX512VL-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VL-NEXT:    movq 56(%rsi), %r13
; AVX512VL-NEXT:    cmovneq %r8, %r13
; AVX512VL-NEXT:    movq 48(%rsi), %rbp
; AVX512VL-NEXT:    cmovneq %rax, %rbp
; AVX512VL-NEXT:    movq 40(%rsi), %r10
; AVX512VL-NEXT:    cmovneq %r9, %r10
; AVX512VL-NEXT:    movq 32(%rsi), %rsi
; AVX512VL-NEXT:    cmovneq %r11, %rsi
; AVX512VL-NEXT:    cmovneq %rbx, %r8
; AVX512VL-NEXT:    cmovneq %r14, %rax
; AVX512VL-NEXT:    cmovneq %r15, %r11
; AVX512VL-NEXT:    cmovneq 16(%rdx), %r14
; AVX512VL-NEXT:    cmovneq (%rdx), %r15
; AVX512VL-NEXT:    cmovneq %r12, %r9
; AVX512VL-NEXT:    cmovneq 8(%rdx), %r12
; AVX512VL-NEXT:    cmovneq 24(%rdx), %rbx
; AVX512VL-NEXT:    testb $-128, %cl
; AVX512VL-NEXT:    cmoveq %rbx, %r12
; AVX512VL-NEXT:    cmoveq %r14, %r15
; AVX512VL-NEXT:    cmoveq %r11, %r14
; AVX512VL-NEXT:    cmoveq %r9, %rbx
; AVX512VL-NEXT:    cmoveq %rax, %r11
; AVX512VL-NEXT:    cmoveq %r8, %r9
; AVX512VL-NEXT:    cmoveq %rsi, %rax
; AVX512VL-NEXT:    cmoveq %r10, %r8
; AVX512VL-NEXT:    cmoveq %rbp, %rsi
; AVX512VL-NEXT:    cmoveq %r13, %r10
; AVX512VL-NEXT:    testb $64, %cl
; AVX512VL-NEXT:    cmovneq %rsi, %r10
; AVX512VL-NEXT:    cmovneq %r8, %rsi
; AVX512VL-NEXT:    cmovneq %rax, %r8
; AVX512VL-NEXT:    cmovneq %r9, %rax
; AVX512VL-NEXT:    cmovneq %r11, %r9
; AVX512VL-NEXT:    cmovneq %rbx, %r11
; AVX512VL-NEXT:    cmovneq %r14, %rbx
; AVX512VL-NEXT:    cmoveq %r12, %r15
; AVX512VL-NEXT:    cmovneq %r12, %r14
; AVX512VL-NEXT:    movq %r14, %rdx
; AVX512VL-NEXT:    shldq %cl, %r15, %rdx
; AVX512VL-NEXT:    movq %rbx, %r15
; AVX512VL-NEXT:    shldq %cl, %r14, %r15
; AVX512VL-NEXT:    movq %r11, %r14
; AVX512VL-NEXT:    shldq %cl, %rbx, %r14
; AVX512VL-NEXT:    movq %r9, %rbx
; AVX512VL-NEXT:    shldq %cl, %r11, %rbx
; AVX512VL-NEXT:    movq %rax, %r11
; AVX512VL-NEXT:    shldq %cl, %r9, %r11
; AVX512VL-NEXT:    movq %r8, %r9
; AVX512VL-NEXT:    shldq %cl, %rax, %r9
; AVX512VL-NEXT:    movq %rdi, %rax
; AVX512VL-NEXT:    movq %rsi, %rdi
; AVX512VL-NEXT:    shldq %cl, %r8, %rdi
; AVX512VL-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VL-NEXT:    shldq %cl, %rsi, %r10
; AVX512VL-NEXT:    movq %r10, 56(%rax)
; AVX512VL-NEXT:    movq %rdi, 48(%rax)
; AVX512VL-NEXT:    movq %r9, 40(%rax)
; AVX512VL-NEXT:    movq %r11, 32(%rax)
; AVX512VL-NEXT:    movq %rbx, 24(%rax)
; AVX512VL-NEXT:    movq %r14, 16(%rax)
; AVX512VL-NEXT:    movq %r15, 8(%rax)
; AVX512VL-NEXT:    movq %rdx, (%rax)
; AVX512VL-NEXT:    popq %rbx
; AVX512VL-NEXT:    popq %r12
; AVX512VL-NEXT:    popq %r13
; AVX512VL-NEXT:    popq %r14
; AVX512VL-NEXT:    popq %r15
; AVX512VL-NEXT:    popq %rbp
; AVX512VL-NEXT:    retq
;
; AVX512VBMI-LABEL: fshl_i512_load:
; AVX512VBMI:       # %bb.0:
; AVX512VBMI-NEXT:    pushq %rbp
; AVX512VBMI-NEXT:    pushq %r15
; AVX512VBMI-NEXT:    pushq %r14
; AVX512VBMI-NEXT:    pushq %r13
; AVX512VBMI-NEXT:    pushq %r12
; AVX512VBMI-NEXT:    pushq %rbx
; AVX512VBMI-NEXT:    movq (%rsi), %r11
; AVX512VBMI-NEXT:    movq 8(%rsi), %r9
; AVX512VBMI-NEXT:    movq 16(%rsi), %rax
; AVX512VBMI-NEXT:    movq 24(%rsi), %r8
; AVX512VBMI-NEXT:    movq 32(%rdx), %r15
; AVX512VBMI-NEXT:    movq 40(%rdx), %r12
; AVX512VBMI-NEXT:    movq 48(%rdx), %r14
; AVX512VBMI-NEXT:    movq 56(%rdx), %rbx
; AVX512VBMI-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VBMI-NEXT:    movq 56(%rsi), %r13
; AVX512VBMI-NEXT:    cmovneq %r8, %r13
; AVX512VBMI-NEXT:    movq 48(%rsi), %rbp
; AVX512VBMI-NEXT:    cmovneq %rax, %rbp
; AVX512VBMI-NEXT:    movq 40(%rsi), %r10
; AVX512VBMI-NEXT:    cmovneq %r9, %r10
; AVX512VBMI-NEXT:    movq 32(%rsi), %rsi
; AVX512VBMI-NEXT:    cmovneq %r11, %rsi
; AVX512VBMI-NEXT:    cmovneq %rbx, %r8
; AVX512VBMI-NEXT:    cmovneq %r14, %rax
; AVX512VBMI-NEXT:    cmovneq %r15, %r11
; AVX512VBMI-NEXT:    cmovneq 16(%rdx), %r14
; AVX512VBMI-NEXT:    cmovneq (%rdx), %r15
; AVX512VBMI-NEXT:    cmovneq %r12, %r9
; AVX512VBMI-NEXT:    cmovneq 8(%rdx), %r12
; AVX512VBMI-NEXT:    cmovneq 24(%rdx), %rbx
; AVX512VBMI-NEXT:    testb $-128, %cl
; AVX512VBMI-NEXT:    cmoveq %rbx, %r12
; AVX512VBMI-NEXT:    cmoveq %r14, %r15
; AVX512VBMI-NEXT:    cmoveq %r11, %r14
; AVX512VBMI-NEXT:    cmoveq %r9, %rbx
; AVX512VBMI-NEXT:    cmoveq %rax, %r11
; AVX512VBMI-NEXT:    cmoveq %r8, %r9
; AVX512VBMI-NEXT:    cmoveq %rsi, %rax
; AVX512VBMI-NEXT:    cmoveq %r10, %r8
; AVX512VBMI-NEXT:    cmoveq %rbp, %rsi
; AVX512VBMI-NEXT:    cmoveq %r13, %r10
; AVX512VBMI-NEXT:    testb $64, %cl
; AVX512VBMI-NEXT:    cmovneq %rsi, %r10
; AVX512VBMI-NEXT:    cmovneq %r8, %rsi
; AVX512VBMI-NEXT:    cmovneq %rax, %r8
; AVX512VBMI-NEXT:    cmovneq %r9, %rax
; AVX512VBMI-NEXT:    cmovneq %r11, %r9
; AVX512VBMI-NEXT:    cmovneq %rbx, %r11
; AVX512VBMI-NEXT:    cmovneq %r14, %rbx
; AVX512VBMI-NEXT:    cmoveq %r12, %r15
; AVX512VBMI-NEXT:    cmovneq %r12, %r14
; AVX512VBMI-NEXT:    movq %r14, %rdx
; AVX512VBMI-NEXT:    shldq %cl, %r15, %rdx
; AVX512VBMI-NEXT:    movq %rbx, %r15
; AVX512VBMI-NEXT:    shldq %cl, %r14, %r15
; AVX512VBMI-NEXT:    movq %r11, %r14
; AVX512VBMI-NEXT:    shldq %cl, %rbx, %r14
; AVX512VBMI-NEXT:    movq %r9, %rbx
; AVX512VBMI-NEXT:    shldq %cl, %r11, %rbx
; AVX512VBMI-NEXT:    movq %rax, %r11
; AVX512VBMI-NEXT:    shldq %cl, %r9, %r11
; AVX512VBMI-NEXT:    movq %r8, %r9
; AVX512VBMI-NEXT:    shldq %cl, %rax, %r9
; AVX512VBMI-NEXT:    movq %rdi, %rax
; AVX512VBMI-NEXT:    movq %rsi, %rdi
; AVX512VBMI-NEXT:    shldq %cl, %r8, %rdi
; AVX512VBMI-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VBMI-NEXT:    shldq %cl, %rsi, %r10
; AVX512VBMI-NEXT:    movq %r10, 56(%rax)
; AVX512VBMI-NEXT:    movq %rdi, 48(%rax)
; AVX512VBMI-NEXT:    movq %r9, 40(%rax)
; AVX512VBMI-NEXT:    movq %r11, 32(%rax)
; AVX512VBMI-NEXT:    movq %rbx, 24(%rax)
; AVX512VBMI-NEXT:    movq %r14, 16(%rax)
; AVX512VBMI-NEXT:    movq %r15, 8(%rax)
; AVX512VBMI-NEXT:    movq %rdx, (%rax)
; AVX512VBMI-NEXT:    popq %rbx
; AVX512VBMI-NEXT:    popq %r12
; AVX512VBMI-NEXT:    popq %r13
; AVX512VBMI-NEXT:    popq %r14
; AVX512VBMI-NEXT:    popq %r15
; AVX512VBMI-NEXT:    popq %rbp
; AVX512VBMI-NEXT:    retq
  %a0 = load i512, ptr %p0
  %a1 = load i512, ptr %p1
  %r = call i512 @llvm.fshl.i512(i512 %a0, i512 %a1, i512 %a2)
  ret i512 %r
}

define i512 @fshr_i512_load(ptr %p0, ptr %p1, i512 %a2) nounwind {
; SSE-LABEL: fshr_i512_load:
; SSE:       # %bb.0:
; SSE-NEXT:    pushq %rbp
; SSE-NEXT:    pushq %r15
; SSE-NEXT:    pushq %r14
; SSE-NEXT:    pushq %r13
; SSE-NEXT:    pushq %r12
; SSE-NEXT:    pushq %rbx
; SSE-NEXT:    movq (%rsi), %r8
; SSE-NEXT:    movq 8(%rsi), %r9
; SSE-NEXT:    movq 16(%rsi), %r10
; SSE-NEXT:    movq 24(%rsi), %r11
; SSE-NEXT:    movq 32(%rdx), %r12
; SSE-NEXT:    testl $256, %ecx # imm = 0x100
; SSE-NEXT:    movq 56(%rsi), %r13
; SSE-NEXT:    cmoveq %r11, %r13
; SSE-NEXT:    movq 40(%rdx), %rbx
; SSE-NEXT:    movq 48(%rsi), %rbp
; SSE-NEXT:    cmoveq %r10, %rbp
; SSE-NEXT:    movq 48(%rdx), %r14
; SSE-NEXT:    movq 40(%rsi), %rax
; SSE-NEXT:    cmoveq %r9, %rax
; SSE-NEXT:    movq 56(%rdx), %r15
; SSE-NEXT:    movq 32(%rsi), %rsi
; SSE-NEXT:    cmoveq %r8, %rsi
; SSE-NEXT:    cmoveq %r15, %r11
; SSE-NEXT:    cmoveq %r14, %r10
; SSE-NEXT:    cmoveq %rbx, %r9
; SSE-NEXT:    cmoveq %r12, %r8
; SSE-NEXT:    cmoveq 24(%rdx), %r15
; SSE-NEXT:    cmoveq 8(%rdx), %rbx
; SSE-NEXT:    cmoveq (%rdx), %r12
; SSE-NEXT:    cmoveq 16(%rdx), %r14
; SSE-NEXT:    testb $-128, %cl
; SSE-NEXT:    cmovneq %r14, %r12
; SSE-NEXT:    cmovneq %r15, %rbx
; SSE-NEXT:    cmovneq %r8, %r14
; SSE-NEXT:    cmovneq %r9, %r15
; SSE-NEXT:    cmovneq %r10, %r8
; SSE-NEXT:    cmovneq %r11, %r9
; SSE-NEXT:    cmovneq %rsi, %r10
; SSE-NEXT:    cmovneq %rax, %r11
; SSE-NEXT:    cmovneq %rbp, %rsi
; SSE-NEXT:    cmovneq %r13, %rax
; SSE-NEXT:    testb $64, %cl
; SSE-NEXT:    cmoveq %rsi, %rax
; SSE-NEXT:    cmoveq %r11, %rsi
; SSE-NEXT:    cmoveq %r10, %r11
; SSE-NEXT:    cmoveq %r9, %r10
; SSE-NEXT:    cmoveq %r8, %r9
; SSE-NEXT:    cmoveq %r15, %r8
; SSE-NEXT:    cmoveq %r14, %r15
; SSE-NEXT:    cmoveq %rbx, %r14
; SSE-NEXT:    cmoveq %r12, %rbx
; SSE-NEXT:    shrdq %cl, %r14, %rbx
; SSE-NEXT:    shrdq %cl, %r15, %r14
; SSE-NEXT:    shrdq %cl, %r8, %r15
; SSE-NEXT:    shrdq %cl, %r9, %r8
; SSE-NEXT:    shrdq %cl, %r10, %r9
; SSE-NEXT:    shrdq %cl, %r11, %r10
; SSE-NEXT:    shrdq %cl, %rsi, %r11
; SSE-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE-NEXT:    shrdq %cl, %rax, %rsi
; SSE-NEXT:    movq %rdi, %rax
; SSE-NEXT:    movq %rsi, 56(%rdi)
; SSE-NEXT:    movq %r11, 48(%rdi)
; SSE-NEXT:    movq %r10, 40(%rdi)
; SSE-NEXT:    movq %r9, 32(%rdi)
; SSE-NEXT:    movq %r8, 24(%rdi)
; SSE-NEXT:    movq %r15, 16(%rdi)
; SSE-NEXT:    movq %r14, 8(%rdi)
; SSE-NEXT:    movq %rbx, (%rdi)
; SSE-NEXT:    popq %rbx
; SSE-NEXT:    popq %r12
; SSE-NEXT:    popq %r13
; SSE-NEXT:    popq %r14
; SSE-NEXT:    popq %r15
; SSE-NEXT:    popq %rbp
; SSE-NEXT:    retq
;
; AVX2-LABEL: fshr_i512_load:
; AVX2:       # %bb.0:
; AVX2-NEXT:    pushq %rbp
; AVX2-NEXT:    pushq %r15
; AVX2-NEXT:    pushq %r14
; AVX2-NEXT:    pushq %r13
; AVX2-NEXT:    pushq %r12
; AVX2-NEXT:    pushq %rbx
; AVX2-NEXT:    movq (%rsi), %r8
; AVX2-NEXT:    movq 8(%rsi), %r9
; AVX2-NEXT:    movq 16(%rsi), %r10
; AVX2-NEXT:    movq 24(%rsi), %r11
; AVX2-NEXT:    movq 32(%rdx), %r13
; AVX2-NEXT:    testl $256, %ecx # imm = 0x100
; AVX2-NEXT:    movq 56(%rsi), %rax
; AVX2-NEXT:    cmoveq %r11, %rax
; AVX2-NEXT:    movq 40(%rdx), %rbx
; AVX2-NEXT:    movq 48(%rsi), %rbp
; AVX2-NEXT:    cmoveq %r10, %rbp
; AVX2-NEXT:    movq 48(%rdx), %r14
; AVX2-NEXT:    movq 40(%rsi), %r12
; AVX2-NEXT:    cmoveq %r9, %r12
; AVX2-NEXT:    movq 56(%rdx), %r15
; AVX2-NEXT:    movq 32(%rsi), %rsi
; AVX2-NEXT:    cmoveq %r8, %rsi
; AVX2-NEXT:    cmoveq %r15, %r11
; AVX2-NEXT:    cmoveq %r14, %r10
; AVX2-NEXT:    cmoveq %rbx, %r9
; AVX2-NEXT:    cmoveq %r13, %r8
; AVX2-NEXT:    cmoveq 24(%rdx), %r15
; AVX2-NEXT:    cmoveq 8(%rdx), %rbx
; AVX2-NEXT:    cmoveq (%rdx), %r13
; AVX2-NEXT:    cmoveq 16(%rdx), %r14
; AVX2-NEXT:    testb $-128, %cl
; AVX2-NEXT:    cmovneq %r14, %r13
; AVX2-NEXT:    cmovneq %r15, %rbx
; AVX2-NEXT:    cmovneq %r8, %r14
; AVX2-NEXT:    cmovneq %r9, %r15
; AVX2-NEXT:    cmovneq %r10, %r8
; AVX2-NEXT:    cmovneq %r11, %r9
; AVX2-NEXT:    cmovneq %rsi, %r10
; AVX2-NEXT:    cmovneq %r12, %r11
; AVX2-NEXT:    cmovneq %rbp, %rsi
; AVX2-NEXT:    cmovneq %rax, %r12
; AVX2-NEXT:    movq %rdi, %rax
; AVX2-NEXT:    testb $64, %cl
; AVX2-NEXT:    cmoveq %rsi, %r12
; AVX2-NEXT:    cmoveq %r11, %rsi
; AVX2-NEXT:    cmoveq %r10, %r11
; AVX2-NEXT:    cmoveq %r9, %r10
; AVX2-NEXT:    cmoveq %r8, %r9
; AVX2-NEXT:    cmoveq %r15, %r8
; AVX2-NEXT:    cmoveq %r14, %r15
; AVX2-NEXT:    cmoveq %rbx, %r14
; AVX2-NEXT:    cmoveq %r13, %rbx
; AVX2-NEXT:    shrdq %cl, %r14, %rbx
; AVX2-NEXT:    shrdq %cl, %r15, %r14
; AVX2-NEXT:    shrdq %cl, %r8, %r15
; AVX2-NEXT:    shrdq %cl, %r9, %r8
; AVX2-NEXT:    shrdq %cl, %r10, %r9
; AVX2-NEXT:    shrdq %cl, %r11, %r10
; AVX2-NEXT:    shrdq %cl, %rsi, %r11
; AVX2-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX2-NEXT:    shrdq %cl, %r12, %rsi
; AVX2-NEXT:    movq %rsi, 56(%rdi)
; AVX2-NEXT:    movq %r11, 48(%rdi)
; AVX2-NEXT:    movq %r10, 40(%rdi)
; AVX2-NEXT:    movq %r9, 32(%rdi)
; AVX2-NEXT:    movq %r8, 24(%rdi)
; AVX2-NEXT:    movq %r15, 16(%rdi)
; AVX2-NEXT:    movq %r14, 8(%rdi)
; AVX2-NEXT:    movq %rbx, (%rdi)
; AVX2-NEXT:    popq %rbx
; AVX2-NEXT:    popq %r12
; AVX2-NEXT:    popq %r13
; AVX2-NEXT:    popq %r14
; AVX2-NEXT:    popq %r15
; AVX2-NEXT:    popq %rbp
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: fshr_i512_load:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    pushq %rbp
; AVX512F-NEXT:    pushq %r15
; AVX512F-NEXT:    pushq %r14
; AVX512F-NEXT:    pushq %r13
; AVX512F-NEXT:    pushq %r12
; AVX512F-NEXT:    pushq %rbx
; AVX512F-NEXT:    movq (%rsi), %r8
; AVX512F-NEXT:    movq 8(%rsi), %r9
; AVX512F-NEXT:    movq 16(%rsi), %r10
; AVX512F-NEXT:    movq 24(%rsi), %r11
; AVX512F-NEXT:    movq 32(%rdx), %r13
; AVX512F-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512F-NEXT:    movq 56(%rsi), %rax
; AVX512F-NEXT:    cmoveq %r11, %rax
; AVX512F-NEXT:    movq 40(%rdx), %rbx
; AVX512F-NEXT:    movq 48(%rsi), %rbp
; AVX512F-NEXT:    cmoveq %r10, %rbp
; AVX512F-NEXT:    movq 48(%rdx), %r14
; AVX512F-NEXT:    movq 40(%rsi), %r12
; AVX512F-NEXT:    cmoveq %r9, %r12
; AVX512F-NEXT:    movq 56(%rdx), %r15
; AVX512F-NEXT:    movq 32(%rsi), %rsi
; AVX512F-NEXT:    cmoveq %r8, %rsi
; AVX512F-NEXT:    cmoveq %r15, %r11
; AVX512F-NEXT:    cmoveq %r14, %r10
; AVX512F-NEXT:    cmoveq %rbx, %r9
; AVX512F-NEXT:    cmoveq %r13, %r8
; AVX512F-NEXT:    cmoveq 24(%rdx), %r15
; AVX512F-NEXT:    cmoveq 8(%rdx), %rbx
; AVX512F-NEXT:    cmoveq (%rdx), %r13
; AVX512F-NEXT:    cmoveq 16(%rdx), %r14
; AVX512F-NEXT:    testb $-128, %cl
; AVX512F-NEXT:    cmovneq %r14, %r13
; AVX512F-NEXT:    cmovneq %r15, %rbx
; AVX512F-NEXT:    cmovneq %r8, %r14
; AVX512F-NEXT:    cmovneq %r9, %r15
; AVX512F-NEXT:    cmovneq %r10, %r8
; AVX512F-NEXT:    cmovneq %r11, %r9
; AVX512F-NEXT:    cmovneq %rsi, %r10
; AVX512F-NEXT:    cmovneq %r12, %r11
; AVX512F-NEXT:    cmovneq %rbp, %rsi
; AVX512F-NEXT:    cmovneq %rax, %r12
; AVX512F-NEXT:    movq %rdi, %rax
; AVX512F-NEXT:    testb $64, %cl
; AVX512F-NEXT:    cmoveq %rsi, %r12
; AVX512F-NEXT:    cmoveq %r11, %rsi
; AVX512F-NEXT:    cmoveq %r10, %r11
; AVX512F-NEXT:    cmoveq %r9, %r10
; AVX512F-NEXT:    cmoveq %r8, %r9
; AVX512F-NEXT:    cmoveq %r15, %r8
; AVX512F-NEXT:    cmoveq %r14, %r15
; AVX512F-NEXT:    cmoveq %rbx, %r14
; AVX512F-NEXT:    cmoveq %r13, %rbx
; AVX512F-NEXT:    shrdq %cl, %r14, %rbx
; AVX512F-NEXT:    shrdq %cl, %r15, %r14
; AVX512F-NEXT:    shrdq %cl, %r8, %r15
; AVX512F-NEXT:    shrdq %cl, %r9, %r8
; AVX512F-NEXT:    shrdq %cl, %r10, %r9
; AVX512F-NEXT:    shrdq %cl, %r11, %r10
; AVX512F-NEXT:    shrdq %cl, %rsi, %r11
; AVX512F-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512F-NEXT:    shrdq %cl, %r12, %rsi
; AVX512F-NEXT:    movq %rsi, 56(%rdi)
; AVX512F-NEXT:    movq %r11, 48(%rdi)
; AVX512F-NEXT:    movq %r10, 40(%rdi)
; AVX512F-NEXT:    movq %r9, 32(%rdi)
; AVX512F-NEXT:    movq %r8, 24(%rdi)
; AVX512F-NEXT:    movq %r15, 16(%rdi)
; AVX512F-NEXT:    movq %r14, 8(%rdi)
; AVX512F-NEXT:    movq %rbx, (%rdi)
; AVX512F-NEXT:    popq %rbx
; AVX512F-NEXT:    popq %r12
; AVX512F-NEXT:    popq %r13
; AVX512F-NEXT:    popq %r14
; AVX512F-NEXT:    popq %r15
; AVX512F-NEXT:    popq %rbp
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: fshr_i512_load:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    pushq %rbp
; AVX512VL-NEXT:    pushq %r15
; AVX512VL-NEXT:    pushq %r14
; AVX512VL-NEXT:    pushq %r13
; AVX512VL-NEXT:    pushq %r12
; AVX512VL-NEXT:    pushq %rbx
; AVX512VL-NEXT:    movq (%rsi), %r8
; AVX512VL-NEXT:    movq 8(%rsi), %r10
; AVX512VL-NEXT:    movq 16(%rsi), %rbx
; AVX512VL-NEXT:    movq 24(%rsi), %r15
; AVX512VL-NEXT:    movq 32(%rdx), %rax
; AVX512VL-NEXT:    movq 40(%rdx), %r9
; AVX512VL-NEXT:    movq 48(%rdx), %r11
; AVX512VL-NEXT:    movq 56(%rdx), %r14
; AVX512VL-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VL-NEXT:    movq 56(%rsi), %r13
; AVX512VL-NEXT:    cmoveq %r15, %r13
; AVX512VL-NEXT:    movq 48(%rsi), %rbp
; AVX512VL-NEXT:    cmoveq %rbx, %rbp
; AVX512VL-NEXT:    movq 40(%rsi), %r12
; AVX512VL-NEXT:    cmoveq %r10, %r12
; AVX512VL-NEXT:    movq 32(%rsi), %rsi
; AVX512VL-NEXT:    cmoveq %r8, %rsi
; AVX512VL-NEXT:    cmoveq %r14, %r15
; AVX512VL-NEXT:    cmoveq %r11, %rbx
; AVX512VL-NEXT:    cmoveq %r9, %r10
; AVX512VL-NEXT:    cmoveq 24(%rdx), %r14
; AVX512VL-NEXT:    cmoveq 8(%rdx), %r9
; AVX512VL-NEXT:    cmoveq %rax, %r8
; AVX512VL-NEXT:    cmoveq (%rdx), %rax
; AVX512VL-NEXT:    cmoveq 16(%rdx), %r11
; AVX512VL-NEXT:    testb $-128, %cl
; AVX512VL-NEXT:    cmovneq %r11, %rax
; AVX512VL-NEXT:    cmovneq %r14, %r9
; AVX512VL-NEXT:    cmovneq %r8, %r11
; AVX512VL-NEXT:    cmovneq %r10, %r14
; AVX512VL-NEXT:    cmovneq %rbx, %r8
; AVX512VL-NEXT:    cmovneq %r15, %r10
; AVX512VL-NEXT:    cmovneq %rsi, %rbx
; AVX512VL-NEXT:    cmovneq %r12, %r15
; AVX512VL-NEXT:    cmovneq %rbp, %rsi
; AVX512VL-NEXT:    cmovneq %r13, %r12
; AVX512VL-NEXT:    testb $64, %cl
; AVX512VL-NEXT:    cmoveq %rsi, %r12
; AVX512VL-NEXT:    cmoveq %r15, %rsi
; AVX512VL-NEXT:    cmoveq %rbx, %r15
; AVX512VL-NEXT:    cmoveq %r10, %rbx
; AVX512VL-NEXT:    cmoveq %r8, %r10
; AVX512VL-NEXT:    cmoveq %r14, %r8
; AVX512VL-NEXT:    cmoveq %r11, %r14
; AVX512VL-NEXT:    cmoveq %r9, %r11
; AVX512VL-NEXT:    cmoveq %rax, %r9
; AVX512VL-NEXT:    shrdq %cl, %r11, %r9
; AVX512VL-NEXT:    shrdq %cl, %r14, %r11
; AVX512VL-NEXT:    shrdq %cl, %r8, %r14
; AVX512VL-NEXT:    shrdq %cl, %r10, %r8
; AVX512VL-NEXT:    shrdq %cl, %rbx, %r10
; AVX512VL-NEXT:    shrdq %cl, %r15, %rbx
; AVX512VL-NEXT:    movq %rdi, %rax
; AVX512VL-NEXT:    shrdq %cl, %rsi, %r15
; AVX512VL-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VL-NEXT:    shrdq %cl, %r12, %rsi
; AVX512VL-NEXT:    movq %rsi, 56(%rdi)
; AVX512VL-NEXT:    movq %r15, 48(%rdi)
; AVX512VL-NEXT:    movq %rbx, 40(%rdi)
; AVX512VL-NEXT:    movq %r10, 32(%rdi)
; AVX512VL-NEXT:    movq %r8, 24(%rdi)
; AVX512VL-NEXT:    movq %r14, 16(%rdi)
; AVX512VL-NEXT:    movq %r11, 8(%rdi)
; AVX512VL-NEXT:    movq %r9, (%rdi)
; AVX512VL-NEXT:    popq %rbx
; AVX512VL-NEXT:    popq %r12
; AVX512VL-NEXT:    popq %r13
; AVX512VL-NEXT:    popq %r14
; AVX512VL-NEXT:    popq %r15
; AVX512VL-NEXT:    popq %rbp
; AVX512VL-NEXT:    retq
;
; AVX512VBMI-LABEL: fshr_i512_load:
; AVX512VBMI:       # %bb.0:
; AVX512VBMI-NEXT:    pushq %rbp
; AVX512VBMI-NEXT:    pushq %r15
; AVX512VBMI-NEXT:    pushq %r14
; AVX512VBMI-NEXT:    pushq %r13
; AVX512VBMI-NEXT:    pushq %r12
; AVX512VBMI-NEXT:    pushq %rbx
; AVX512VBMI-NEXT:    movq (%rsi), %r8
; AVX512VBMI-NEXT:    movq 8(%rsi), %r10
; AVX512VBMI-NEXT:    movq 16(%rsi), %rbx
; AVX512VBMI-NEXT:    movq 24(%rsi), %r15
; AVX512VBMI-NEXT:    movq 32(%rdx), %rax
; AVX512VBMI-NEXT:    movq 40(%rdx), %r9
; AVX512VBMI-NEXT:    movq 48(%rdx), %r11
; AVX512VBMI-NEXT:    movq 56(%rdx), %r14
; AVX512VBMI-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VBMI-NEXT:    movq 56(%rsi), %r13
; AVX512VBMI-NEXT:    cmoveq %r15, %r13
; AVX512VBMI-NEXT:    movq 48(%rsi), %rbp
; AVX512VBMI-NEXT:    cmoveq %rbx, %rbp
; AVX512VBMI-NEXT:    movq 40(%rsi), %r12
; AVX512VBMI-NEXT:    cmoveq %r10, %r12
; AVX512VBMI-NEXT:    movq 32(%rsi), %rsi
; AVX512VBMI-NEXT:    cmoveq %r8, %rsi
; AVX512VBMI-NEXT:    cmoveq %r14, %r15
; AVX512VBMI-NEXT:    cmoveq %r11, %rbx
; AVX512VBMI-NEXT:    cmoveq %r9, %r10
; AVX512VBMI-NEXT:    cmoveq 24(%rdx), %r14
; AVX512VBMI-NEXT:    cmoveq 8(%rdx), %r9
; AVX512VBMI-NEXT:    cmoveq %rax, %r8
; AVX512VBMI-NEXT:    cmoveq (%rdx), %rax
; AVX512VBMI-NEXT:    cmoveq 16(%rdx), %r11
; AVX512VBMI-NEXT:    testb $-128, %cl
; AVX512VBMI-NEXT:    cmovneq %r11, %rax
; AVX512VBMI-NEXT:    cmovneq %r14, %r9
; AVX512VBMI-NEXT:    cmovneq %r8, %r11
; AVX512VBMI-NEXT:    cmovneq %r10, %r14
; AVX512VBMI-NEXT:    cmovneq %rbx, %r8
; AVX512VBMI-NEXT:    cmovneq %r15, %r10
; AVX512VBMI-NEXT:    cmovneq %rsi, %rbx
; AVX512VBMI-NEXT:    cmovneq %r12, %r15
; AVX512VBMI-NEXT:    cmovneq %rbp, %rsi
; AVX512VBMI-NEXT:    cmovneq %r13, %r12
; AVX512VBMI-NEXT:    testb $64, %cl
; AVX512VBMI-NEXT:    cmoveq %rsi, %r12
; AVX512VBMI-NEXT:    cmoveq %r15, %rsi
; AVX512VBMI-NEXT:    cmoveq %rbx, %r15
; AVX512VBMI-NEXT:    cmoveq %r10, %rbx
; AVX512VBMI-NEXT:    cmoveq %r8, %r10
; AVX512VBMI-NEXT:    cmoveq %r14, %r8
; AVX512VBMI-NEXT:    cmoveq %r11, %r14
; AVX512VBMI-NEXT:    cmoveq %r9, %r11
; AVX512VBMI-NEXT:    cmoveq %rax, %r9
; AVX512VBMI-NEXT:    shrdq %cl, %r11, %r9
; AVX512VBMI-NEXT:    shrdq %cl, %r14, %r11
; AVX512VBMI-NEXT:    shrdq %cl, %r8, %r14
; AVX512VBMI-NEXT:    shrdq %cl, %r10, %r8
; AVX512VBMI-NEXT:    shrdq %cl, %rbx, %r10
; AVX512VBMI-NEXT:    shrdq %cl, %r15, %rbx
; AVX512VBMI-NEXT:    movq %rdi, %rax
; AVX512VBMI-NEXT:    shrdq %cl, %rsi, %r15
; AVX512VBMI-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VBMI-NEXT:    shrdq %cl, %r12, %rsi
; AVX512VBMI-NEXT:    movq %rsi, 56(%rdi)
; AVX512VBMI-NEXT:    movq %r15, 48(%rdi)
; AVX512VBMI-NEXT:    movq %rbx, 40(%rdi)
; AVX512VBMI-NEXT:    movq %r10, 32(%rdi)
; AVX512VBMI-NEXT:    movq %r8, 24(%rdi)
; AVX512VBMI-NEXT:    movq %r14, 16(%rdi)
; AVX512VBMI-NEXT:    movq %r11, 8(%rdi)
; AVX512VBMI-NEXT:    movq %r9, (%rdi)
; AVX512VBMI-NEXT:    popq %rbx
; AVX512VBMI-NEXT:    popq %r12
; AVX512VBMI-NEXT:    popq %r13
; AVX512VBMI-NEXT:    popq %r14
; AVX512VBMI-NEXT:    popq %r15
; AVX512VBMI-NEXT:    popq %rbp
; AVX512VBMI-NEXT:    retq
  %a0 = load i512, ptr %p0
  %a1 = load i512, ptr %p1
  %r = call i512 @llvm.fshr.i512(i512 %a0, i512 %a1, i512 %a2)
  ret i512 %r
}

define i512 @fshl_rot_i512_load(ptr %p0, i512 %a2) nounwind {
; SSE-LABEL: fshl_rot_i512_load:
; SSE:       # %bb.0:
; SSE-NEXT:    pushq %r15
; SSE-NEXT:    pushq %r14
; SSE-NEXT:    pushq %r13
; SSE-NEXT:    pushq %r12
; SSE-NEXT:    pushq %rbx
; SSE-NEXT:    movq %rdx, %rcx
; SSE-NEXT:    movq 40(%rsi), %rax
; SSE-NEXT:    movq 48(%rsi), %rdx
; SSE-NEXT:    movq 56(%rsi), %r10
; SSE-NEXT:    movq (%rsi), %r9
; SSE-NEXT:    movq 32(%rsi), %rbx
; SSE-NEXT:    testl $256, %ecx # imm = 0x100
; SSE-NEXT:    movq %rbx, %r11
; SSE-NEXT:    cmovneq %r9, %r11
; SSE-NEXT:    movq 24(%rsi), %r14
; SSE-NEXT:    movq %r14, %r15
; SSE-NEXT:    cmovneq %r10, %r15
; SSE-NEXT:    movq 16(%rsi), %r12
; SSE-NEXT:    movq %r12, %r8
; SSE-NEXT:    cmovneq %rdx, %r8
; SSE-NEXT:    movq 8(%rsi), %r13
; SSE-NEXT:    movq %r13, %rsi
; SSE-NEXT:    cmovneq %rax, %rsi
; SSE-NEXT:    cmovneq %rbx, %r9
; SSE-NEXT:    cmovneq %r12, %rdx
; SSE-NEXT:    cmovneq %r13, %rax
; SSE-NEXT:    cmovneq %r14, %r10
; SSE-NEXT:    testb $-128, %cl
; SSE-NEXT:    movq %rsi, %r14
; SSE-NEXT:    cmovneq %r10, %r14
; SSE-NEXT:    cmovneq %rax, %r10
; SSE-NEXT:    movq %r8, %rbx
; SSE-NEXT:    cmovneq %r9, %rbx
; SSE-NEXT:    cmovneq %rdx, %r9
; SSE-NEXT:    cmoveq %r15, %rsi
; SSE-NEXT:    cmoveq %r11, %r8
; SSE-NEXT:    cmovneq %r15, %rax
; SSE-NEXT:    cmovneq %r11, %rdx
; SSE-NEXT:    testb $64, %cl
; SSE-NEXT:    movq %rdx, %r11
; SSE-NEXT:    cmovneq %rax, %r11
; SSE-NEXT:    cmovneq %r8, %rax
; SSE-NEXT:    cmovneq %rsi, %r8
; SSE-NEXT:    cmovneq %rbx, %rsi
; SSE-NEXT:    cmovneq %r14, %rbx
; SSE-NEXT:    cmovneq %r9, %r14
; SSE-NEXT:    cmoveq %r10, %rdx
; SSE-NEXT:    cmovneq %r10, %r9
; SSE-NEXT:    movq %r9, %r10
; SSE-NEXT:    shldq %cl, %rdx, %r10
; SSE-NEXT:    movq %r14, %r15
; SSE-NEXT:    shldq %cl, %r9, %r15
; SSE-NEXT:    movq %rbx, %r9
; SSE-NEXT:    shldq %cl, %r14, %r9
; SSE-NEXT:    movq %rsi, %r14
; SSE-NEXT:    shldq %cl, %rbx, %r14
; SSE-NEXT:    movq %r8, %rbx
; SSE-NEXT:    shldq %cl, %rsi, %rbx
; SSE-NEXT:    movq %rax, %rsi
; SSE-NEXT:    shldq %cl, %r8, %rsi
; SSE-NEXT:    movq %r11, %r8
; SSE-NEXT:    shldq %cl, %rax, %r8
; SSE-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE-NEXT:    shldq %cl, %r11, %rdx
; SSE-NEXT:    movq %rdi, %rax
; SSE-NEXT:    movq %rdx, 56(%rdi)
; SSE-NEXT:    movq %r8, 48(%rdi)
; SSE-NEXT:    movq %rsi, 40(%rdi)
; SSE-NEXT:    movq %rbx, 32(%rdi)
; SSE-NEXT:    movq %r14, 24(%rdi)
; SSE-NEXT:    movq %r9, 16(%rdi)
; SSE-NEXT:    movq %r15, 8(%rdi)
; SSE-NEXT:    movq %r10, (%rdi)
; SSE-NEXT:    popq %rbx
; SSE-NEXT:    popq %r12
; SSE-NEXT:    popq %r13
; SSE-NEXT:    popq %r14
; SSE-NEXT:    popq %r15
; SSE-NEXT:    retq
;
; AVX2-LABEL: fshl_rot_i512_load:
; AVX2:       # %bb.0:
; AVX2-NEXT:    pushq %r15
; AVX2-NEXT:    pushq %r14
; AVX2-NEXT:    pushq %r13
; AVX2-NEXT:    pushq %r12
; AVX2-NEXT:    pushq %rbx
; AVX2-NEXT:    movq %rdx, %rcx
; AVX2-NEXT:    movq 40(%rsi), %r8
; AVX2-NEXT:    movq 48(%rsi), %rdx
; AVX2-NEXT:    movq 16(%rsi), %rax
; AVX2-NEXT:    movq 56(%rsi), %r11
; AVX2-NEXT:    movq 24(%rsi), %rbx
; AVX2-NEXT:    movq (%rsi), %r10
; AVX2-NEXT:    movq 8(%rsi), %r14
; AVX2-NEXT:    movq 32(%rsi), %r12
; AVX2-NEXT:    testl $256, %ecx # imm = 0x100
; AVX2-NEXT:    movq %r12, %r15
; AVX2-NEXT:    cmovneq %r10, %r15
; AVX2-NEXT:    movq %rbx, %r13
; AVX2-NEXT:    cmovneq %r11, %r13
; AVX2-NEXT:    movq %rax, %rsi
; AVX2-NEXT:    cmovneq %rdx, %rsi
; AVX2-NEXT:    movq %r14, %r9
; AVX2-NEXT:    cmovneq %r8, %r9
; AVX2-NEXT:    cmovneq %r12, %r10
; AVX2-NEXT:    cmovneq %rax, %rdx
; AVX2-NEXT:    cmovneq %r14, %r8
; AVX2-NEXT:    cmovneq %rbx, %r11
; AVX2-NEXT:    testb $-128, %cl
; AVX2-NEXT:    movq %r9, %r14
; AVX2-NEXT:    cmovneq %r11, %r14
; AVX2-NEXT:    cmovneq %r8, %r11
; AVX2-NEXT:    movq %rsi, %rbx
; AVX2-NEXT:    cmovneq %r10, %rbx
; AVX2-NEXT:    cmovneq %rdx, %r10
; AVX2-NEXT:    cmoveq %r13, %r9
; AVX2-NEXT:    cmoveq %r15, %rsi
; AVX2-NEXT:    cmovneq %r13, %r8
; AVX2-NEXT:    movq %rdi, %rax
; AVX2-NEXT:    cmovneq %r15, %rdx
; AVX2-NEXT:    testb $64, %cl
; AVX2-NEXT:    movq %rdx, %rdi
; AVX2-NEXT:    cmovneq %r8, %rdi
; AVX2-NEXT:    cmovneq %rsi, %r8
; AVX2-NEXT:    cmovneq %r9, %rsi
; AVX2-NEXT:    cmovneq %rbx, %r9
; AVX2-NEXT:    cmovneq %r14, %rbx
; AVX2-NEXT:    cmovneq %r10, %r14
; AVX2-NEXT:    cmoveq %r11, %rdx
; AVX2-NEXT:    cmovneq %r11, %r10
; AVX2-NEXT:    movq %r10, %r11
; AVX2-NEXT:    shldq %cl, %rdx, %r11
; AVX2-NEXT:    movq %r14, %r15
; AVX2-NEXT:    shldq %cl, %r10, %r15
; AVX2-NEXT:    movq %rbx, %r10
; AVX2-NEXT:    shldq %cl, %r14, %r10
; AVX2-NEXT:    movq %r9, %r14
; AVX2-NEXT:    shldq %cl, %rbx, %r14
; AVX2-NEXT:    movq %rsi, %rbx
; AVX2-NEXT:    shldq %cl, %r9, %rbx
; AVX2-NEXT:    movq %r8, %r9
; AVX2-NEXT:    shldq %cl, %rsi, %r9
; AVX2-NEXT:    movq %rdi, %rsi
; AVX2-NEXT:    shldq %cl, %r8, %rsi
; AVX2-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX2-NEXT:    shldq %cl, %rdi, %rdx
; AVX2-NEXT:    movq %rdx, 56(%rax)
; AVX2-NEXT:    movq %rsi, 48(%rax)
; AVX2-NEXT:    movq %r9, 40(%rax)
; AVX2-NEXT:    movq %rbx, 32(%rax)
; AVX2-NEXT:    movq %r14, 24(%rax)
; AVX2-NEXT:    movq %r10, 16(%rax)
; AVX2-NEXT:    movq %r15, 8(%rax)
; AVX2-NEXT:    movq %r11, (%rax)
; AVX2-NEXT:    popq %rbx
; AVX2-NEXT:    popq %r12
; AVX2-NEXT:    popq %r13
; AVX2-NEXT:    popq %r14
; AVX2-NEXT:    popq %r15
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: fshl_rot_i512_load:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    pushq %r15
; AVX512F-NEXT:    pushq %r14
; AVX512F-NEXT:    pushq %r13
; AVX512F-NEXT:    pushq %r12
; AVX512F-NEXT:    pushq %rbx
; AVX512F-NEXT:    movq %rdx, %rcx
; AVX512F-NEXT:    movq 40(%rsi), %r8
; AVX512F-NEXT:    movq 48(%rsi), %rdx
; AVX512F-NEXT:    movq 16(%rsi), %rax
; AVX512F-NEXT:    movq 56(%rsi), %r11
; AVX512F-NEXT:    movq 24(%rsi), %rbx
; AVX512F-NEXT:    movq (%rsi), %r10
; AVX512F-NEXT:    movq 8(%rsi), %r14
; AVX512F-NEXT:    movq 32(%rsi), %r12
; AVX512F-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512F-NEXT:    movq %r12, %r15
; AVX512F-NEXT:    cmovneq %r10, %r15
; AVX512F-NEXT:    movq %rbx, %r13
; AVX512F-NEXT:    cmovneq %r11, %r13
; AVX512F-NEXT:    movq %rax, %rsi
; AVX512F-NEXT:    cmovneq %rdx, %rsi
; AVX512F-NEXT:    movq %r14, %r9
; AVX512F-NEXT:    cmovneq %r8, %r9
; AVX512F-NEXT:    cmovneq %r12, %r10
; AVX512F-NEXT:    cmovneq %rax, %rdx
; AVX512F-NEXT:    cmovneq %r14, %r8
; AVX512F-NEXT:    cmovneq %rbx, %r11
; AVX512F-NEXT:    testb $-128, %cl
; AVX512F-NEXT:    movq %r9, %r14
; AVX512F-NEXT:    cmovneq %r11, %r14
; AVX512F-NEXT:    cmovneq %r8, %r11
; AVX512F-NEXT:    movq %rsi, %rbx
; AVX512F-NEXT:    cmovneq %r10, %rbx
; AVX512F-NEXT:    cmovneq %rdx, %r10
; AVX512F-NEXT:    cmoveq %r13, %r9
; AVX512F-NEXT:    cmoveq %r15, %rsi
; AVX512F-NEXT:    cmovneq %r13, %r8
; AVX512F-NEXT:    movq %rdi, %rax
; AVX512F-NEXT:    cmovneq %r15, %rdx
; AVX512F-NEXT:    testb $64, %cl
; AVX512F-NEXT:    movq %rdx, %rdi
; AVX512F-NEXT:    cmovneq %r8, %rdi
; AVX512F-NEXT:    cmovneq %rsi, %r8
; AVX512F-NEXT:    cmovneq %r9, %rsi
; AVX512F-NEXT:    cmovneq %rbx, %r9
; AVX512F-NEXT:    cmovneq %r14, %rbx
; AVX512F-NEXT:    cmovneq %r10, %r14
; AVX512F-NEXT:    cmoveq %r11, %rdx
; AVX512F-NEXT:    cmovneq %r11, %r10
; AVX512F-NEXT:    movq %r10, %r11
; AVX512F-NEXT:    shldq %cl, %rdx, %r11
; AVX512F-NEXT:    movq %r14, %r15
; AVX512F-NEXT:    shldq %cl, %r10, %r15
; AVX512F-NEXT:    movq %rbx, %r10
; AVX512F-NEXT:    shldq %cl, %r14, %r10
; AVX512F-NEXT:    movq %r9, %r14
; AVX512F-NEXT:    shldq %cl, %rbx, %r14
; AVX512F-NEXT:    movq %rsi, %rbx
; AVX512F-NEXT:    shldq %cl, %r9, %rbx
; AVX512F-NEXT:    movq %r8, %r9
; AVX512F-NEXT:    shldq %cl, %rsi, %r9
; AVX512F-NEXT:    movq %rdi, %rsi
; AVX512F-NEXT:    shldq %cl, %r8, %rsi
; AVX512F-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512F-NEXT:    shldq %cl, %rdi, %rdx
; AVX512F-NEXT:    movq %rdx, 56(%rax)
; AVX512F-NEXT:    movq %rsi, 48(%rax)
; AVX512F-NEXT:    movq %r9, 40(%rax)
; AVX512F-NEXT:    movq %rbx, 32(%rax)
; AVX512F-NEXT:    movq %r14, 24(%rax)
; AVX512F-NEXT:    movq %r10, 16(%rax)
; AVX512F-NEXT:    movq %r15, 8(%rax)
; AVX512F-NEXT:    movq %r11, (%rax)
; AVX512F-NEXT:    popq %rbx
; AVX512F-NEXT:    popq %r12
; AVX512F-NEXT:    popq %r13
; AVX512F-NEXT:    popq %r14
; AVX512F-NEXT:    popq %r15
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: fshl_rot_i512_load:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    pushq %r15
; AVX512VL-NEXT:    pushq %r14
; AVX512VL-NEXT:    pushq %r13
; AVX512VL-NEXT:    pushq %r12
; AVX512VL-NEXT:    pushq %rbx
; AVX512VL-NEXT:    movq %rdx, %rcx
; AVX512VL-NEXT:    movq 40(%rsi), %r8
; AVX512VL-NEXT:    movq 48(%rsi), %rdx
; AVX512VL-NEXT:    movq 16(%rsi), %rbx
; AVX512VL-NEXT:    movq 56(%rsi), %r10
; AVX512VL-NEXT:    movq 24(%rsi), %r14
; AVX512VL-NEXT:    movq (%rsi), %r9
; AVX512VL-NEXT:    movq 8(%rsi), %r15
; AVX512VL-NEXT:    movq 32(%rsi), %r12
; AVX512VL-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VL-NEXT:    movq %r12, %r11
; AVX512VL-NEXT:    cmovneq %r9, %r11
; AVX512VL-NEXT:    movq %r14, %r13
; AVX512VL-NEXT:    cmovneq %r10, %r13
; AVX512VL-NEXT:    movq %rbx, %rax
; AVX512VL-NEXT:    cmovneq %rdx, %rax
; AVX512VL-NEXT:    movq %r15, %rsi
; AVX512VL-NEXT:    cmovneq %r8, %rsi
; AVX512VL-NEXT:    cmovneq %r12, %r9
; AVX512VL-NEXT:    cmovneq %rbx, %rdx
; AVX512VL-NEXT:    cmovneq %r15, %r8
; AVX512VL-NEXT:    cmovneq %r14, %r10
; AVX512VL-NEXT:    testb $-128, %cl
; AVX512VL-NEXT:    movq %rsi, %r14
; AVX512VL-NEXT:    cmovneq %r10, %r14
; AVX512VL-NEXT:    cmovneq %r8, %r10
; AVX512VL-NEXT:    movq %rax, %rbx
; AVX512VL-NEXT:    cmovneq %r9, %rbx
; AVX512VL-NEXT:    cmovneq %rdx, %r9
; AVX512VL-NEXT:    cmoveq %r13, %rsi
; AVX512VL-NEXT:    cmoveq %r11, %rax
; AVX512VL-NEXT:    cmovneq %r13, %r8
; AVX512VL-NEXT:    cmovneq %r11, %rdx
; AVX512VL-NEXT:    testb $64, %cl
; AVX512VL-NEXT:    movq %rdx, %r11
; AVX512VL-NEXT:    cmovneq %r8, %r11
; AVX512VL-NEXT:    cmovneq %rax, %r8
; AVX512VL-NEXT:    cmovneq %rsi, %rax
; AVX512VL-NEXT:    cmovneq %rbx, %rsi
; AVX512VL-NEXT:    cmovneq %r14, %rbx
; AVX512VL-NEXT:    cmovneq %r9, %r14
; AVX512VL-NEXT:    cmoveq %r10, %rdx
; AVX512VL-NEXT:    cmovneq %r10, %r9
; AVX512VL-NEXT:    movq %r9, %r10
; AVX512VL-NEXT:    shldq %cl, %rdx, %r10
; AVX512VL-NEXT:    movq %r14, %r15
; AVX512VL-NEXT:    shldq %cl, %r9, %r15
; AVX512VL-NEXT:    movq %rbx, %r9
; AVX512VL-NEXT:    shldq %cl, %r14, %r9
; AVX512VL-NEXT:    movq %rsi, %r14
; AVX512VL-NEXT:    shldq %cl, %rbx, %r14
; AVX512VL-NEXT:    movq %rax, %rbx
; AVX512VL-NEXT:    shldq %cl, %rsi, %rbx
; AVX512VL-NEXT:    movq %r8, %rsi
; AVX512VL-NEXT:    shldq %cl, %rax, %rsi
; AVX512VL-NEXT:    movq %rdi, %rax
; AVX512VL-NEXT:    movq %r11, %rdi
; AVX512VL-NEXT:    shldq %cl, %r8, %rdi
; AVX512VL-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VL-NEXT:    shldq %cl, %r11, %rdx
; AVX512VL-NEXT:    movq %rdx, 56(%rax)
; AVX512VL-NEXT:    movq %rdi, 48(%rax)
; AVX512VL-NEXT:    movq %rsi, 40(%rax)
; AVX512VL-NEXT:    movq %rbx, 32(%rax)
; AVX512VL-NEXT:    movq %r14, 24(%rax)
; AVX512VL-NEXT:    movq %r9, 16(%rax)
; AVX512VL-NEXT:    movq %r15, 8(%rax)
; AVX512VL-NEXT:    movq %r10, (%rax)
; AVX512VL-NEXT:    popq %rbx
; AVX512VL-NEXT:    popq %r12
; AVX512VL-NEXT:    popq %r13
; AVX512VL-NEXT:    popq %r14
; AVX512VL-NEXT:    popq %r15
; AVX512VL-NEXT:    retq
;
; AVX512VBMI-LABEL: fshl_rot_i512_load:
; AVX512VBMI:       # %bb.0:
; AVX512VBMI-NEXT:    pushq %r15
; AVX512VBMI-NEXT:    pushq %r14
; AVX512VBMI-NEXT:    pushq %r13
; AVX512VBMI-NEXT:    pushq %r12
; AVX512VBMI-NEXT:    pushq %rbx
; AVX512VBMI-NEXT:    movq %rdx, %rcx
; AVX512VBMI-NEXT:    movq 40(%rsi), %r8
; AVX512VBMI-NEXT:    movq 48(%rsi), %rdx
; AVX512VBMI-NEXT:    movq 16(%rsi), %rbx
; AVX512VBMI-NEXT:    movq 56(%rsi), %r10
; AVX512VBMI-NEXT:    movq 24(%rsi), %r14
; AVX512VBMI-NEXT:    movq (%rsi), %r9
; AVX512VBMI-NEXT:    movq 8(%rsi), %r15
; AVX512VBMI-NEXT:    movq 32(%rsi), %r12
; AVX512VBMI-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VBMI-NEXT:    movq %r12, %r11
; AVX512VBMI-NEXT:    cmovneq %r9, %r11
; AVX512VBMI-NEXT:    movq %r14, %r13
; AVX512VBMI-NEXT:    cmovneq %r10, %r13
; AVX512VBMI-NEXT:    movq %rbx, %rax
; AVX512VBMI-NEXT:    cmovneq %rdx, %rax
; AVX512VBMI-NEXT:    movq %r15, %rsi
; AVX512VBMI-NEXT:    cmovneq %r8, %rsi
; AVX512VBMI-NEXT:    cmovneq %r12, %r9
; AVX512VBMI-NEXT:    cmovneq %rbx, %rdx
; AVX512VBMI-NEXT:    cmovneq %r15, %r8
; AVX512VBMI-NEXT:    cmovneq %r14, %r10
; AVX512VBMI-NEXT:    testb $-128, %cl
; AVX512VBMI-NEXT:    movq %rsi, %r14
; AVX512VBMI-NEXT:    cmovneq %r10, %r14
; AVX512VBMI-NEXT:    cmovneq %r8, %r10
; AVX512VBMI-NEXT:    movq %rax, %rbx
; AVX512VBMI-NEXT:    cmovneq %r9, %rbx
; AVX512VBMI-NEXT:    cmovneq %rdx, %r9
; AVX512VBMI-NEXT:    cmoveq %r13, %rsi
; AVX512VBMI-NEXT:    cmoveq %r11, %rax
; AVX512VBMI-NEXT:    cmovneq %r13, %r8
; AVX512VBMI-NEXT:    cmovneq %r11, %rdx
; AVX512VBMI-NEXT:    testb $64, %cl
; AVX512VBMI-NEXT:    movq %rdx, %r11
; AVX512VBMI-NEXT:    cmovneq %r8, %r11
; AVX512VBMI-NEXT:    cmovneq %rax, %r8
; AVX512VBMI-NEXT:    cmovneq %rsi, %rax
; AVX512VBMI-NEXT:    cmovneq %rbx, %rsi
; AVX512VBMI-NEXT:    cmovneq %r14, %rbx
; AVX512VBMI-NEXT:    cmovneq %r9, %r14
; AVX512VBMI-NEXT:    cmoveq %r10, %rdx
; AVX512VBMI-NEXT:    cmovneq %r10, %r9
; AVX512VBMI-NEXT:    movq %r9, %r10
; AVX512VBMI-NEXT:    shldq %cl, %rdx, %r10
; AVX512VBMI-NEXT:    movq %r14, %r15
; AVX512VBMI-NEXT:    shldq %cl, %r9, %r15
; AVX512VBMI-NEXT:    movq %rbx, %r9
; AVX512VBMI-NEXT:    shldq %cl, %r14, %r9
; AVX512VBMI-NEXT:    movq %rsi, %r14
; AVX512VBMI-NEXT:    shldq %cl, %rbx, %r14
; AVX512VBMI-NEXT:    movq %rax, %rbx
; AVX512VBMI-NEXT:    shldq %cl, %rsi, %rbx
; AVX512VBMI-NEXT:    movq %r8, %rsi
; AVX512VBMI-NEXT:    shldq %cl, %rax, %rsi
; AVX512VBMI-NEXT:    movq %rdi, %rax
; AVX512VBMI-NEXT:    movq %r11, %rdi
; AVX512VBMI-NEXT:    shldq %cl, %r8, %rdi
; AVX512VBMI-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VBMI-NEXT:    shldq %cl, %r11, %rdx
; AVX512VBMI-NEXT:    movq %rdx, 56(%rax)
; AVX512VBMI-NEXT:    movq %rdi, 48(%rax)
; AVX512VBMI-NEXT:    movq %rsi, 40(%rax)
; AVX512VBMI-NEXT:    movq %rbx, 32(%rax)
; AVX512VBMI-NEXT:    movq %r14, 24(%rax)
; AVX512VBMI-NEXT:    movq %r9, 16(%rax)
; AVX512VBMI-NEXT:    movq %r15, 8(%rax)
; AVX512VBMI-NEXT:    movq %r10, (%rax)
; AVX512VBMI-NEXT:    popq %rbx
; AVX512VBMI-NEXT:    popq %r12
; AVX512VBMI-NEXT:    popq %r13
; AVX512VBMI-NEXT:    popq %r14
; AVX512VBMI-NEXT:    popq %r15
; AVX512VBMI-NEXT:    retq
  %a0 = load i512, ptr %p0
  %r = call i512 @llvm.fshl.i512(i512 %a0, i512 %a0, i512 %a2)
  ret i512 %r
}

define i512 @fshr_rot_i512_load(ptr %p0, i512 %a2) nounwind {
; SSE-LABEL: fshr_rot_i512_load:
; SSE:       # %bb.0:
; SSE-NEXT:    pushq %r15
; SSE-NEXT:    pushq %r14
; SSE-NEXT:    pushq %r13
; SSE-NEXT:    pushq %r12
; SSE-NEXT:    pushq %rbx
; SSE-NEXT:    movq %rdx, %rcx
; SSE-NEXT:    movq 40(%rsi), %r8
; SSE-NEXT:    movq 48(%rsi), %rax
; SSE-NEXT:    movq 56(%rsi), %r11
; SSE-NEXT:    movq (%rsi), %rdx
; SSE-NEXT:    movq 32(%rsi), %r10
; SSE-NEXT:    testl $256, %ecx # imm = 0x100
; SSE-NEXT:    movq %r10, %r14
; SSE-NEXT:    cmoveq %rdx, %r14
; SSE-NEXT:    movq 24(%rsi), %rbx
; SSE-NEXT:    movq %rbx, %r15
; SSE-NEXT:    cmoveq %r11, %r15
; SSE-NEXT:    movq 16(%rsi), %r12
; SSE-NEXT:    movq %r12, %r9
; SSE-NEXT:    cmoveq %rax, %r9
; SSE-NEXT:    movq 8(%rsi), %r13
; SSE-NEXT:    movq %r13, %rsi
; SSE-NEXT:    cmoveq %r8, %rsi
; SSE-NEXT:    cmoveq %r10, %rdx
; SSE-NEXT:    cmoveq %r12, %rax
; SSE-NEXT:    cmoveq %r13, %r8
; SSE-NEXT:    cmoveq %rbx, %r11
; SSE-NEXT:    testb $-128, %cl
; SSE-NEXT:    movq %rsi, %r10
; SSE-NEXT:    cmoveq %r11, %r10
; SSE-NEXT:    cmoveq %r8, %r11
; SSE-NEXT:    movq %r9, %rbx
; SSE-NEXT:    cmoveq %rdx, %rbx
; SSE-NEXT:    cmoveq %rax, %rdx
; SSE-NEXT:    cmovneq %r15, %rsi
; SSE-NEXT:    cmovneq %r14, %r9
; SSE-NEXT:    cmoveq %r15, %r8
; SSE-NEXT:    cmoveq %r14, %rax
; SSE-NEXT:    testb $64, %cl
; SSE-NEXT:    movq %rax, %r14
; SSE-NEXT:    cmoveq %r8, %r14
; SSE-NEXT:    cmoveq %r9, %r8
; SSE-NEXT:    cmoveq %rsi, %r9
; SSE-NEXT:    cmoveq %rbx, %rsi
; SSE-NEXT:    cmoveq %r10, %rbx
; SSE-NEXT:    cmoveq %rdx, %r10
; SSE-NEXT:    cmoveq %r11, %rdx
; SSE-NEXT:    cmovneq %r11, %rax
; SSE-NEXT:    movq %rax, %r11
; SSE-NEXT:    shrdq %cl, %rdx, %r11
; SSE-NEXT:    shrdq %cl, %r10, %rdx
; SSE-NEXT:    shrdq %cl, %rbx, %r10
; SSE-NEXT:    shrdq %cl, %rsi, %rbx
; SSE-NEXT:    shrdq %cl, %r9, %rsi
; SSE-NEXT:    shrdq %cl, %r8, %r9
; SSE-NEXT:    shrdq %cl, %r14, %r8
; SSE-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE-NEXT:    shrdq %cl, %rax, %r14
; SSE-NEXT:    movq %rdi, %rax
; SSE-NEXT:    movq %r14, 56(%rdi)
; SSE-NEXT:    movq %r8, 48(%rdi)
; SSE-NEXT:    movq %r9, 40(%rdi)
; SSE-NEXT:    movq %rsi, 32(%rdi)
; SSE-NEXT:    movq %rbx, 24(%rdi)
; SSE-NEXT:    movq %r10, 16(%rdi)
; SSE-NEXT:    movq %rdx, 8(%rdi)
; SSE-NEXT:    movq %r11, (%rdi)
; SSE-NEXT:    popq %rbx
; SSE-NEXT:    popq %r12
; SSE-NEXT:    popq %r13
; SSE-NEXT:    popq %r14
; SSE-NEXT:    popq %r15
; SSE-NEXT:    retq
;
; AVX2-LABEL: fshr_rot_i512_load:
; AVX2:       # %bb.0:
; AVX2-NEXT:    pushq %r15
; AVX2-NEXT:    pushq %r14
; AVX2-NEXT:    pushq %r13
; AVX2-NEXT:    pushq %r12
; AVX2-NEXT:    pushq %rbx
; AVX2-NEXT:    movq %rdx, %rcx
; AVX2-NEXT:    movq 40(%rsi), %r8
; AVX2-NEXT:    movq 48(%rsi), %r9
; AVX2-NEXT:    movq 16(%rsi), %rax
; AVX2-NEXT:    movq 56(%rsi), %r14
; AVX2-NEXT:    movq 24(%rsi), %r11
; AVX2-NEXT:    movq (%rsi), %rdx
; AVX2-NEXT:    movq 8(%rsi), %rbx
; AVX2-NEXT:    movq 32(%rsi), %r12
; AVX2-NEXT:    testl $256, %ecx # imm = 0x100
; AVX2-NEXT:    movq %r12, %r15
; AVX2-NEXT:    cmoveq %rdx, %r15
; AVX2-NEXT:    movq %r11, %r13
; AVX2-NEXT:    cmoveq %r14, %r13
; AVX2-NEXT:    movq %rax, %rsi
; AVX2-NEXT:    cmoveq %r9, %rsi
; AVX2-NEXT:    movq %rbx, %r10
; AVX2-NEXT:    cmoveq %r8, %r10
; AVX2-NEXT:    cmoveq %r12, %rdx
; AVX2-NEXT:    cmoveq %rax, %r9
; AVX2-NEXT:    cmoveq %rbx, %r8
; AVX2-NEXT:    cmoveq %r11, %r14
; AVX2-NEXT:    testb $-128, %cl
; AVX2-NEXT:    movq %r10, %r11
; AVX2-NEXT:    cmoveq %r14, %r11
; AVX2-NEXT:    cmoveq %r8, %r14
; AVX2-NEXT:    movq %rsi, %rbx
; AVX2-NEXT:    cmoveq %rdx, %rbx
; AVX2-NEXT:    cmoveq %r9, %rdx
; AVX2-NEXT:    cmovneq %r13, %r10
; AVX2-NEXT:    cmovneq %r15, %rsi
; AVX2-NEXT:    cmoveq %r13, %r8
; AVX2-NEXT:    movq %rdi, %rax
; AVX2-NEXT:    cmoveq %r15, %r9
; AVX2-NEXT:    testb $64, %cl
; AVX2-NEXT:    movq %r9, %rdi
; AVX2-NEXT:    cmoveq %r8, %rdi
; AVX2-NEXT:    cmoveq %rsi, %r8
; AVX2-NEXT:    cmoveq %r10, %rsi
; AVX2-NEXT:    cmoveq %rbx, %r10
; AVX2-NEXT:    cmoveq %r11, %rbx
; AVX2-NEXT:    cmoveq %rdx, %r11
; AVX2-NEXT:    cmoveq %r14, %rdx
; AVX2-NEXT:    cmovneq %r14, %r9
; AVX2-NEXT:    movq %r9, %r14
; AVX2-NEXT:    shrdq %cl, %rdx, %r14
; AVX2-NEXT:    shrdq %cl, %r11, %rdx
; AVX2-NEXT:    shrdq %cl, %rbx, %r11
; AVX2-NEXT:    shrdq %cl, %r10, %rbx
; AVX2-NEXT:    shrdq %cl, %rsi, %r10
; AVX2-NEXT:    shrdq %cl, %r8, %rsi
; AVX2-NEXT:    shrdq %cl, %rdi, %r8
; AVX2-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX2-NEXT:    shrdq %cl, %r9, %rdi
; AVX2-NEXT:    movq %rdi, 56(%rax)
; AVX2-NEXT:    movq %r8, 48(%rax)
; AVX2-NEXT:    movq %rsi, 40(%rax)
; AVX2-NEXT:    movq %r10, 32(%rax)
; AVX2-NEXT:    movq %rbx, 24(%rax)
; AVX2-NEXT:    movq %r11, 16(%rax)
; AVX2-NEXT:    movq %rdx, 8(%rax)
; AVX2-NEXT:    movq %r14, (%rax)
; AVX2-NEXT:    popq %rbx
; AVX2-NEXT:    popq %r12
; AVX2-NEXT:    popq %r13
; AVX2-NEXT:    popq %r14
; AVX2-NEXT:    popq %r15
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: fshr_rot_i512_load:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    pushq %r15
; AVX512F-NEXT:    pushq %r14
; AVX512F-NEXT:    pushq %r13
; AVX512F-NEXT:    pushq %r12
; AVX512F-NEXT:    pushq %rbx
; AVX512F-NEXT:    movq %rdx, %rcx
; AVX512F-NEXT:    movq 40(%rsi), %r8
; AVX512F-NEXT:    movq 48(%rsi), %r9
; AVX512F-NEXT:    movq 16(%rsi), %rax
; AVX512F-NEXT:    movq 56(%rsi), %r14
; AVX512F-NEXT:    movq 24(%rsi), %r11
; AVX512F-NEXT:    movq (%rsi), %rdx
; AVX512F-NEXT:    movq 8(%rsi), %rbx
; AVX512F-NEXT:    movq 32(%rsi), %r12
; AVX512F-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512F-NEXT:    movq %r12, %r15
; AVX512F-NEXT:    cmoveq %rdx, %r15
; AVX512F-NEXT:    movq %r11, %r13
; AVX512F-NEXT:    cmoveq %r14, %r13
; AVX512F-NEXT:    movq %rax, %rsi
; AVX512F-NEXT:    cmoveq %r9, %rsi
; AVX512F-NEXT:    movq %rbx, %r10
; AVX512F-NEXT:    cmoveq %r8, %r10
; AVX512F-NEXT:    cmoveq %r12, %rdx
; AVX512F-NEXT:    cmoveq %rax, %r9
; AVX512F-NEXT:    cmoveq %rbx, %r8
; AVX512F-NEXT:    cmoveq %r11, %r14
; AVX512F-NEXT:    testb $-128, %cl
; AVX512F-NEXT:    movq %r10, %r11
; AVX512F-NEXT:    cmoveq %r14, %r11
; AVX512F-NEXT:    cmoveq %r8, %r14
; AVX512F-NEXT:    movq %rsi, %rbx
; AVX512F-NEXT:    cmoveq %rdx, %rbx
; AVX512F-NEXT:    cmoveq %r9, %rdx
; AVX512F-NEXT:    cmovneq %r13, %r10
; AVX512F-NEXT:    cmovneq %r15, %rsi
; AVX512F-NEXT:    cmoveq %r13, %r8
; AVX512F-NEXT:    movq %rdi, %rax
; AVX512F-NEXT:    cmoveq %r15, %r9
; AVX512F-NEXT:    testb $64, %cl
; AVX512F-NEXT:    movq %r9, %rdi
; AVX512F-NEXT:    cmoveq %r8, %rdi
; AVX512F-NEXT:    cmoveq %rsi, %r8
; AVX512F-NEXT:    cmoveq %r10, %rsi
; AVX512F-NEXT:    cmoveq %rbx, %r10
; AVX512F-NEXT:    cmoveq %r11, %rbx
; AVX512F-NEXT:    cmoveq %rdx, %r11
; AVX512F-NEXT:    cmoveq %r14, %rdx
; AVX512F-NEXT:    cmovneq %r14, %r9
; AVX512F-NEXT:    movq %r9, %r14
; AVX512F-NEXT:    shrdq %cl, %rdx, %r14
; AVX512F-NEXT:    shrdq %cl, %r11, %rdx
; AVX512F-NEXT:    shrdq %cl, %rbx, %r11
; AVX512F-NEXT:    shrdq %cl, %r10, %rbx
; AVX512F-NEXT:    shrdq %cl, %rsi, %r10
; AVX512F-NEXT:    shrdq %cl, %r8, %rsi
; AVX512F-NEXT:    shrdq %cl, %rdi, %r8
; AVX512F-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512F-NEXT:    shrdq %cl, %r9, %rdi
; AVX512F-NEXT:    movq %rdi, 56(%rax)
; AVX512F-NEXT:    movq %r8, 48(%rax)
; AVX512F-NEXT:    movq %rsi, 40(%rax)
; AVX512F-NEXT:    movq %r10, 32(%rax)
; AVX512F-NEXT:    movq %rbx, 24(%rax)
; AVX512F-NEXT:    movq %r11, 16(%rax)
; AVX512F-NEXT:    movq %rdx, 8(%rax)
; AVX512F-NEXT:    movq %r14, (%rax)
; AVX512F-NEXT:    popq %rbx
; AVX512F-NEXT:    popq %r12
; AVX512F-NEXT:    popq %r13
; AVX512F-NEXT:    popq %r14
; AVX512F-NEXT:    popq %r15
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: fshr_rot_i512_load:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    pushq %r15
; AVX512VL-NEXT:    pushq %r14
; AVX512VL-NEXT:    pushq %r13
; AVX512VL-NEXT:    pushq %r12
; AVX512VL-NEXT:    pushq %rbx
; AVX512VL-NEXT:    movq %rdx, %rcx
; AVX512VL-NEXT:    movq 40(%rsi), %r8
; AVX512VL-NEXT:    movq 48(%rsi), %r9
; AVX512VL-NEXT:    movq 16(%rsi), %r11
; AVX512VL-NEXT:    movq 56(%rsi), %rax
; AVX512VL-NEXT:    movq 24(%rsi), %rbx
; AVX512VL-NEXT:    movq (%rsi), %rdx
; AVX512VL-NEXT:    movq 8(%rsi), %r15
; AVX512VL-NEXT:    movq 32(%rsi), %r12
; AVX512VL-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VL-NEXT:    movq %r12, %r14
; AVX512VL-NEXT:    cmoveq %rdx, %r14
; AVX512VL-NEXT:    movq %rbx, %r13
; AVX512VL-NEXT:    cmoveq %rax, %r13
; AVX512VL-NEXT:    movq %r11, %rsi
; AVX512VL-NEXT:    cmoveq %r9, %rsi
; AVX512VL-NEXT:    movq %r15, %r10
; AVX512VL-NEXT:    cmoveq %r8, %r10
; AVX512VL-NEXT:    cmoveq %r12, %rdx
; AVX512VL-NEXT:    cmoveq %r11, %r9
; AVX512VL-NEXT:    cmoveq %r15, %r8
; AVX512VL-NEXT:    cmoveq %rbx, %rax
; AVX512VL-NEXT:    testb $-128, %cl
; AVX512VL-NEXT:    movq %r10, %r11
; AVX512VL-NEXT:    cmoveq %rax, %r11
; AVX512VL-NEXT:    cmoveq %r8, %rax
; AVX512VL-NEXT:    movq %rsi, %rbx
; AVX512VL-NEXT:    cmoveq %rdx, %rbx
; AVX512VL-NEXT:    cmoveq %r9, %rdx
; AVX512VL-NEXT:    cmovneq %r13, %r10
; AVX512VL-NEXT:    cmovneq %r14, %rsi
; AVX512VL-NEXT:    cmoveq %r13, %r8
; AVX512VL-NEXT:    cmoveq %r14, %r9
; AVX512VL-NEXT:    testb $64, %cl
; AVX512VL-NEXT:    movq %r9, %r14
; AVX512VL-NEXT:    cmoveq %r8, %r14
; AVX512VL-NEXT:    cmoveq %rsi, %r8
; AVX512VL-NEXT:    cmoveq %r10, %rsi
; AVX512VL-NEXT:    cmoveq %rbx, %r10
; AVX512VL-NEXT:    cmoveq %r11, %rbx
; AVX512VL-NEXT:    cmoveq %rdx, %r11
; AVX512VL-NEXT:    cmoveq %rax, %rdx
; AVX512VL-NEXT:    cmovneq %rax, %r9
; AVX512VL-NEXT:    movq %r9, %r15
; AVX512VL-NEXT:    shrdq %cl, %rdx, %r15
; AVX512VL-NEXT:    shrdq %cl, %r11, %rdx
; AVX512VL-NEXT:    shrdq %cl, %rbx, %r11
; AVX512VL-NEXT:    shrdq %cl, %r10, %rbx
; AVX512VL-NEXT:    shrdq %cl, %rsi, %r10
; AVX512VL-NEXT:    shrdq %cl, %r8, %rsi
; AVX512VL-NEXT:    movq %rdi, %rax
; AVX512VL-NEXT:    shrdq %cl, %r14, %r8
; AVX512VL-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VL-NEXT:    shrdq %cl, %r9, %r14
; AVX512VL-NEXT:    movq %r14, 56(%rdi)
; AVX512VL-NEXT:    movq %r8, 48(%rdi)
; AVX512VL-NEXT:    movq %rsi, 40(%rdi)
; AVX512VL-NEXT:    movq %r10, 32(%rdi)
; AVX512VL-NEXT:    movq %rbx, 24(%rdi)
; AVX512VL-NEXT:    movq %r11, 16(%rdi)
; AVX512VL-NEXT:    movq %rdx, 8(%rdi)
; AVX512VL-NEXT:    movq %r15, (%rdi)
; AVX512VL-NEXT:    popq %rbx
; AVX512VL-NEXT:    popq %r12
; AVX512VL-NEXT:    popq %r13
; AVX512VL-NEXT:    popq %r14
; AVX512VL-NEXT:    popq %r15
; AVX512VL-NEXT:    retq
;
; AVX512VBMI-LABEL: fshr_rot_i512_load:
; AVX512VBMI:       # %bb.0:
; AVX512VBMI-NEXT:    pushq %r15
; AVX512VBMI-NEXT:    pushq %r14
; AVX512VBMI-NEXT:    pushq %r13
; AVX512VBMI-NEXT:    pushq %r12
; AVX512VBMI-NEXT:    pushq %rbx
; AVX512VBMI-NEXT:    movq %rdx, %rcx
; AVX512VBMI-NEXT:    movq 40(%rsi), %r8
; AVX512VBMI-NEXT:    movq 48(%rsi), %r9
; AVX512VBMI-NEXT:    movq 16(%rsi), %r11
; AVX512VBMI-NEXT:    movq 56(%rsi), %rax
; AVX512VBMI-NEXT:    movq 24(%rsi), %rbx
; AVX512VBMI-NEXT:    movq (%rsi), %rdx
; AVX512VBMI-NEXT:    movq 8(%rsi), %r15
; AVX512VBMI-NEXT:    movq 32(%rsi), %r12
; AVX512VBMI-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VBMI-NEXT:    movq %r12, %r14
; AVX512VBMI-NEXT:    cmoveq %rdx, %r14
; AVX512VBMI-NEXT:    movq %rbx, %r13
; AVX512VBMI-NEXT:    cmoveq %rax, %r13
; AVX512VBMI-NEXT:    movq %r11, %rsi
; AVX512VBMI-NEXT:    cmoveq %r9, %rsi
; AVX512VBMI-NEXT:    movq %r15, %r10
; AVX512VBMI-NEXT:    cmoveq %r8, %r10
; AVX512VBMI-NEXT:    cmoveq %r12, %rdx
; AVX512VBMI-NEXT:    cmoveq %r11, %r9
; AVX512VBMI-NEXT:    cmoveq %r15, %r8
; AVX512VBMI-NEXT:    cmoveq %rbx, %rax
; AVX512VBMI-NEXT:    testb $-128, %cl
; AVX512VBMI-NEXT:    movq %r10, %r11
; AVX512VBMI-NEXT:    cmoveq %rax, %r11
; AVX512VBMI-NEXT:    cmoveq %r8, %rax
; AVX512VBMI-NEXT:    movq %rsi, %rbx
; AVX512VBMI-NEXT:    cmoveq %rdx, %rbx
; AVX512VBMI-NEXT:    cmoveq %r9, %rdx
; AVX512VBMI-NEXT:    cmovneq %r13, %r10
; AVX512VBMI-NEXT:    cmovneq %r14, %rsi
; AVX512VBMI-NEXT:    cmoveq %r13, %r8
; AVX512VBMI-NEXT:    cmoveq %r14, %r9
; AVX512VBMI-NEXT:    testb $64, %cl
; AVX512VBMI-NEXT:    movq %r9, %r14
; AVX512VBMI-NEXT:    cmoveq %r8, %r14
; AVX512VBMI-NEXT:    cmoveq %rsi, %r8
; AVX512VBMI-NEXT:    cmoveq %r10, %rsi
; AVX512VBMI-NEXT:    cmoveq %rbx, %r10
; AVX512VBMI-NEXT:    cmoveq %r11, %rbx
; AVX512VBMI-NEXT:    cmoveq %rdx, %r11
; AVX512VBMI-NEXT:    cmoveq %rax, %rdx
; AVX512VBMI-NEXT:    cmovneq %rax, %r9
; AVX512VBMI-NEXT:    movq %r9, %r15
; AVX512VBMI-NEXT:    shrdq %cl, %rdx, %r15
; AVX512VBMI-NEXT:    shrdq %cl, %r11, %rdx
; AVX512VBMI-NEXT:    shrdq %cl, %rbx, %r11
; AVX512VBMI-NEXT:    shrdq %cl, %r10, %rbx
; AVX512VBMI-NEXT:    shrdq %cl, %rsi, %r10
; AVX512VBMI-NEXT:    shrdq %cl, %r8, %rsi
; AVX512VBMI-NEXT:    movq %rdi, %rax
; AVX512VBMI-NEXT:    shrdq %cl, %r14, %r8
; AVX512VBMI-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VBMI-NEXT:    shrdq %cl, %r9, %r14
; AVX512VBMI-NEXT:    movq %r14, 56(%rdi)
; AVX512VBMI-NEXT:    movq %r8, 48(%rdi)
; AVX512VBMI-NEXT:    movq %rsi, 40(%rdi)
; AVX512VBMI-NEXT:    movq %r10, 32(%rdi)
; AVX512VBMI-NEXT:    movq %rbx, 24(%rdi)
; AVX512VBMI-NEXT:    movq %r11, 16(%rdi)
; AVX512VBMI-NEXT:    movq %rdx, 8(%rdi)
; AVX512VBMI-NEXT:    movq %r15, (%rdi)
; AVX512VBMI-NEXT:    popq %rbx
; AVX512VBMI-NEXT:    popq %r12
; AVX512VBMI-NEXT:    popq %r13
; AVX512VBMI-NEXT:    popq %r14
; AVX512VBMI-NEXT:    popq %r15
; AVX512VBMI-NEXT:    retq
  %a0 = load i512, ptr %p0
  %r = call i512 @llvm.fshr.i512(i512 %a0, i512 %a0, i512 %a2)
  ret i512 %r
}

define i512 @fshl_i512_vector(<8 x i64> %v0, <8 x i64> %v1, i512 %a2) nounwind {
; SSE2-LABEL: fshl_i512_vector:
; SSE2:       # %bb.0:
; SSE2-NEXT:    pushq %rbp
; SSE2-NEXT:    pushq %r15
; SSE2-NEXT:    pushq %r14
; SSE2-NEXT:    pushq %r13
; SSE2-NEXT:    pushq %r12
; SSE2-NEXT:    pushq %rbx
; SSE2-NEXT:    movq %rsi, %rcx
; SSE2-NEXT:    movq %xmm0, %r10
; SSE2-NEXT:    movq %xmm2, %rax
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %r9
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm2[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %rdx
; SSE2-NEXT:    movq %xmm1, %r8
; SSE2-NEXT:    movq %xmm3, %r12
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm1[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %rsi
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm3[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %r15
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm5[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %r11
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm4[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %r14
; SSE2-NEXT:    movq %xmm5, %rbx
; SSE2-NEXT:    movq %xmm7, %r13
; SSE2-NEXT:    testl $256, %ecx # imm = 0x100
; SSE2-NEXT:    cmovneq %r8, %r12
; SSE2-NEXT:    cmovneq %r13, %r8
; SSE2-NEXT:    cmoveq %r13, %rbx
; SSE2-NEXT:    movq %xmm4, %r13
; SSE2-NEXT:    movq %xmm6, %rbp
; SSE2-NEXT:    cmovneq %r10, %rax
; SSE2-NEXT:    cmovneq %rbp, %r10
; SSE2-NEXT:    cmoveq %rbp, %r13
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm6[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %rbp
; SSE2-NEXT:    cmovneq %r9, %rdx
; SSE2-NEXT:    cmovneq %rbp, %r9
; SSE2-NEXT:    cmoveq %rbp, %r14
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm7[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %rbp
; SSE2-NEXT:    cmovneq %rsi, %r15
; SSE2-NEXT:    cmovneq %rbp, %rsi
; SSE2-NEXT:    cmoveq %rbp, %r11
; SSE2-NEXT:    testb $-128, %cl
; SSE2-NEXT:    cmoveq %rbx, %r13
; SSE2-NEXT:    cmoveq %r10, %rbx
; SSE2-NEXT:    cmoveq %r8, %r10
; SSE2-NEXT:    cmoveq %rax, %r8
; SSE2-NEXT:    cmoveq %r12, %rax
; SSE2-NEXT:    cmoveq %r11, %r14
; SSE2-NEXT:    cmoveq %r9, %r11
; SSE2-NEXT:    cmoveq %rsi, %r9
; SSE2-NEXT:    cmoveq %rdx, %rsi
; SSE2-NEXT:    cmoveq %r15, %rdx
; SSE2-NEXT:    testb $64, %cl
; SSE2-NEXT:    cmovneq %rax, %rdx
; SSE2-NEXT:    cmovneq %rsi, %rax
; SSE2-NEXT:    cmovneq %r8, %rsi
; SSE2-NEXT:    cmovneq %r9, %r8
; SSE2-NEXT:    cmovneq %r10, %r9
; SSE2-NEXT:    cmovneq %r11, %r10
; SSE2-NEXT:    cmovneq %rbx, %r11
; SSE2-NEXT:    cmoveq %r14, %r13
; SSE2-NEXT:    cmovneq %r14, %rbx
; SSE2-NEXT:    movq %rbx, %r14
; SSE2-NEXT:    shldq %cl, %r13, %r14
; SSE2-NEXT:    movq %r11, %r15
; SSE2-NEXT:    shldq %cl, %rbx, %r15
; SSE2-NEXT:    movq %r10, %rbx
; SSE2-NEXT:    shldq %cl, %r11, %rbx
; SSE2-NEXT:    movq %r9, %r11
; SSE2-NEXT:    shldq %cl, %r10, %r11
; SSE2-NEXT:    movq %r8, %r10
; SSE2-NEXT:    shldq %cl, %r9, %r10
; SSE2-NEXT:    movq %rsi, %r9
; SSE2-NEXT:    shldq %cl, %r8, %r9
; SSE2-NEXT:    movq %rax, %r8
; SSE2-NEXT:    shldq %cl, %rsi, %r8
; SSE2-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE2-NEXT:    shldq %cl, %rax, %rdx
; SSE2-NEXT:    movq %rdi, %rax
; SSE2-NEXT:    movq %rdx, 56(%rdi)
; SSE2-NEXT:    movq %r8, 48(%rdi)
; SSE2-NEXT:    movq %r9, 40(%rdi)
; SSE2-NEXT:    movq %r10, 32(%rdi)
; SSE2-NEXT:    movq %r11, 24(%rdi)
; SSE2-NEXT:    movq %rbx, 16(%rdi)
; SSE2-NEXT:    movq %r15, 8(%rdi)
; SSE2-NEXT:    movq %r14, (%rdi)
; SSE2-NEXT:    popq %rbx
; SSE2-NEXT:    popq %r12
; SSE2-NEXT:    popq %r13
; SSE2-NEXT:    popq %r14
; SSE2-NEXT:    popq %r15
; SSE2-NEXT:    popq %rbp
; SSE2-NEXT:    retq
;
; SSE42-LABEL: fshl_i512_vector:
; SSE42:       # %bb.0:
; SSE42-NEXT:    pushq %rbp
; SSE42-NEXT:    pushq %r15
; SSE42-NEXT:    pushq %r14
; SSE42-NEXT:    pushq %r13
; SSE42-NEXT:    pushq %r12
; SSE42-NEXT:    pushq %rbx
; SSE42-NEXT:    movq %rsi, %rcx
; SSE42-NEXT:    movq %xmm0, %r10
; SSE42-NEXT:    pextrq $1, %xmm0, %r9
; SSE42-NEXT:    pextrq $1, %xmm2, %rdx
; SSE42-NEXT:    movq %xmm2, %rax
; SSE42-NEXT:    movq %xmm1, %r8
; SSE42-NEXT:    pextrq $1, %xmm1, %rsi
; SSE42-NEXT:    pextrq $1, %xmm3, %r15
; SSE42-NEXT:    pextrq $1, %xmm5, %r11
; SSE42-NEXT:    movq %xmm3, %r13
; SSE42-NEXT:    pextrq $1, %xmm4, %r14
; SSE42-NEXT:    movq %xmm5, %rbx
; SSE42-NEXT:    movq %xmm7, %r12
; SSE42-NEXT:    testl $256, %ecx # imm = 0x100
; SSE42-NEXT:    cmovneq %r8, %r13
; SSE42-NEXT:    cmovneq %r12, %r8
; SSE42-NEXT:    cmoveq %r12, %rbx
; SSE42-NEXT:    movq %xmm4, %r12
; SSE42-NEXT:    movq %xmm6, %rbp
; SSE42-NEXT:    cmovneq %r10, %rax
; SSE42-NEXT:    cmovneq %rbp, %r10
; SSE42-NEXT:    cmoveq %rbp, %r12
; SSE42-NEXT:    pextrq $1, %xmm6, %rbp
; SSE42-NEXT:    cmovneq %r9, %rdx
; SSE42-NEXT:    cmovneq %rbp, %r9
; SSE42-NEXT:    cmoveq %rbp, %r14
; SSE42-NEXT:    pextrq $1, %xmm7, %rbp
; SSE42-NEXT:    cmovneq %rsi, %r15
; SSE42-NEXT:    cmovneq %rbp, %rsi
; SSE42-NEXT:    cmoveq %rbp, %r11
; SSE42-NEXT:    testb $-128, %cl
; SSE42-NEXT:    cmoveq %rbx, %r12
; SSE42-NEXT:    cmoveq %r10, %rbx
; SSE42-NEXT:    cmoveq %r8, %r10
; SSE42-NEXT:    cmoveq %rax, %r8
; SSE42-NEXT:    cmoveq %r13, %rax
; SSE42-NEXT:    cmoveq %r11, %r14
; SSE42-NEXT:    cmoveq %r9, %r11
; SSE42-NEXT:    cmoveq %rsi, %r9
; SSE42-NEXT:    cmoveq %rdx, %rsi
; SSE42-NEXT:    cmoveq %r15, %rdx
; SSE42-NEXT:    testb $64, %cl
; SSE42-NEXT:    cmovneq %rax, %rdx
; SSE42-NEXT:    cmovneq %rsi, %rax
; SSE42-NEXT:    cmovneq %r8, %rsi
; SSE42-NEXT:    cmovneq %r9, %r8
; SSE42-NEXT:    cmovneq %r10, %r9
; SSE42-NEXT:    cmovneq %r11, %r10
; SSE42-NEXT:    cmovneq %rbx, %r11
; SSE42-NEXT:    cmoveq %r14, %r12
; SSE42-NEXT:    cmovneq %r14, %rbx
; SSE42-NEXT:    movq %rbx, %r14
; SSE42-NEXT:    shldq %cl, %r12, %r14
; SSE42-NEXT:    movq %r11, %r15
; SSE42-NEXT:    shldq %cl, %rbx, %r15
; SSE42-NEXT:    movq %r10, %rbx
; SSE42-NEXT:    shldq %cl, %r11, %rbx
; SSE42-NEXT:    movq %r9, %r11
; SSE42-NEXT:    shldq %cl, %r10, %r11
; SSE42-NEXT:    movq %r8, %r10
; SSE42-NEXT:    shldq %cl, %r9, %r10
; SSE42-NEXT:    movq %rsi, %r9
; SSE42-NEXT:    shldq %cl, %r8, %r9
; SSE42-NEXT:    movq %rax, %r8
; SSE42-NEXT:    shldq %cl, %rsi, %r8
; SSE42-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE42-NEXT:    shldq %cl, %rax, %rdx
; SSE42-NEXT:    movq %rdi, %rax
; SSE42-NEXT:    movq %rdx, 56(%rdi)
; SSE42-NEXT:    movq %r8, 48(%rdi)
; SSE42-NEXT:    movq %r9, 40(%rdi)
; SSE42-NEXT:    movq %r10, 32(%rdi)
; SSE42-NEXT:    movq %r11, 24(%rdi)
; SSE42-NEXT:    movq %rbx, 16(%rdi)
; SSE42-NEXT:    movq %r15, 8(%rdi)
; SSE42-NEXT:    movq %r14, (%rdi)
; SSE42-NEXT:    popq %rbx
; SSE42-NEXT:    popq %r12
; SSE42-NEXT:    popq %r13
; SSE42-NEXT:    popq %r14
; SSE42-NEXT:    popq %r15
; SSE42-NEXT:    popq %rbp
; SSE42-NEXT:    retq
;
; AVX2-LABEL: fshl_i512_vector:
; AVX2:       # %bb.0:
; AVX2-NEXT:    pushq %rbp
; AVX2-NEXT:    pushq %r15
; AVX2-NEXT:    pushq %r14
; AVX2-NEXT:    pushq %r13
; AVX2-NEXT:    pushq %r12
; AVX2-NEXT:    pushq %rbx
; AVX2-NEXT:    movq %rsi, %rcx
; AVX2-NEXT:    vmovq %xmm0, %r11
; AVX2-NEXT:    vpextrq $1, %xmm0, %r9
; AVX2-NEXT:    vmovq %xmm1, %rdx
; AVX2-NEXT:    vpextrq $1, %xmm1, %rsi
; AVX2-NEXT:    vextracti128 $1, %ymm0, %xmm0
; AVX2-NEXT:    vmovq %xmm0, %r10
; AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm1
; AVX2-NEXT:    vpextrq $1, %xmm0, %r8
; AVX2-NEXT:    vmovq %xmm1, %r13
; AVX2-NEXT:    vpextrq $1, %xmm1, %rax
; AVX2-NEXT:    vextracti128 $1, %ymm2, %xmm1
; AVX2-NEXT:    vpextrq $1, %xmm1, %rbx
; AVX2-NEXT:    vpextrq $1, %xmm2, %r15
; AVX2-NEXT:    vmovq %xmm2, %r14
; AVX2-NEXT:    vextracti128 $1, %ymm3, %xmm0
; AVX2-NEXT:    vmovq %xmm0, %rbp
; AVX2-NEXT:    testl $256, %ecx # imm = 0x100
; AVX2-NEXT:    cmovneq %r10, %r13
; AVX2-NEXT:    cmovneq %rbp, %r10
; AVX2-NEXT:    vmovq %xmm1, %r12
; AVX2-NEXT:    cmoveq %rbp, %r12
; AVX2-NEXT:    vmovq %xmm3, %rbp
; AVX2-NEXT:    cmovneq %r11, %rdx
; AVX2-NEXT:    cmovneq %rbp, %r11
; AVX2-NEXT:    cmoveq %rbp, %r14
; AVX2-NEXT:    vpextrq $1, %xmm3, %rbp
; AVX2-NEXT:    cmovneq %r9, %rsi
; AVX2-NEXT:    cmovneq %rbp, %r9
; AVX2-NEXT:    cmoveq %rbp, %r15
; AVX2-NEXT:    vpextrq $1, %xmm0, %rbp
; AVX2-NEXT:    cmovneq %r8, %rax
; AVX2-NEXT:    cmovneq %rbp, %r8
; AVX2-NEXT:    cmoveq %rbp, %rbx
; AVX2-NEXT:    testb $-128, %cl
; AVX2-NEXT:    cmoveq %r12, %r14
; AVX2-NEXT:    cmoveq %r11, %r12
; AVX2-NEXT:    cmoveq %r10, %r11
; AVX2-NEXT:    cmoveq %rdx, %r10
; AVX2-NEXT:    cmoveq %r13, %rdx
; AVX2-NEXT:    cmoveq %rbx, %r15
; AVX2-NEXT:    cmoveq %r9, %rbx
; AVX2-NEXT:    cmoveq %r8, %r9
; AVX2-NEXT:    cmoveq %rsi, %r8
; AVX2-NEXT:    cmoveq %rax, %rsi
; AVX2-NEXT:    movq %rdi, %rax
; AVX2-NEXT:    testb $64, %cl
; AVX2-NEXT:    cmovneq %rdx, %rsi
; AVX2-NEXT:    cmovneq %r8, %rdx
; AVX2-NEXT:    cmovneq %r10, %r8
; AVX2-NEXT:    cmovneq %r9, %r10
; AVX2-NEXT:    cmovneq %r11, %r9
; AVX2-NEXT:    cmovneq %rbx, %r11
; AVX2-NEXT:    cmovneq %r12, %rbx
; AVX2-NEXT:    cmoveq %r15, %r14
; AVX2-NEXT:    cmovneq %r15, %r12
; AVX2-NEXT:    movq %r12, %rdi
; AVX2-NEXT:    shldq %cl, %r14, %rdi
; AVX2-NEXT:    movq %rbx, %r14
; AVX2-NEXT:    shldq %cl, %r12, %r14
; AVX2-NEXT:    movq %r11, %r15
; AVX2-NEXT:    shldq %cl, %rbx, %r15
; AVX2-NEXT:    movq %r9, %rbx
; AVX2-NEXT:    shldq %cl, %r11, %rbx
; AVX2-NEXT:    movq %r10, %r11
; AVX2-NEXT:    shldq %cl, %r9, %r11
; AVX2-NEXT:    movq %r8, %r9
; AVX2-NEXT:    shldq %cl, %r10, %r9
; AVX2-NEXT:    movq %rdx, %r10
; AVX2-NEXT:    shldq %cl, %r8, %r10
; AVX2-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX2-NEXT:    shldq %cl, %rdx, %rsi
; AVX2-NEXT:    movq %rsi, 56(%rax)
; AVX2-NEXT:    movq %r10, 48(%rax)
; AVX2-NEXT:    movq %r9, 40(%rax)
; AVX2-NEXT:    movq %r11, 32(%rax)
; AVX2-NEXT:    movq %rbx, 24(%rax)
; AVX2-NEXT:    movq %r15, 16(%rax)
; AVX2-NEXT:    movq %r14, 8(%rax)
; AVX2-NEXT:    movq %rdi, (%rax)
; AVX2-NEXT:    popq %rbx
; AVX2-NEXT:    popq %r12
; AVX2-NEXT:    popq %r13
; AVX2-NEXT:    popq %r14
; AVX2-NEXT:    popq %r15
; AVX2-NEXT:    popq %rbp
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: fshl_i512_vector:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    pushq %rbp
; AVX512F-NEXT:    pushq %r15
; AVX512F-NEXT:    pushq %r14
; AVX512F-NEXT:    pushq %r13
; AVX512F-NEXT:    pushq %r12
; AVX512F-NEXT:    pushq %rbx
; AVX512F-NEXT:    movq %rsi, %rcx
; AVX512F-NEXT:    vmovq %xmm0, %r10
; AVX512F-NEXT:    vextracti32x4 $2, %zmm0, %xmm2
; AVX512F-NEXT:    vmovq %xmm2, %rdx
; AVX512F-NEXT:    vpextrq $1, %xmm0, %r8
; AVX512F-NEXT:    vpextrq $1, %xmm2, %rsi
; AVX512F-NEXT:    vextracti32x4 $3, %zmm0, %xmm2
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm0
; AVX512F-NEXT:    vpextrq $1, %xmm2, %rax
; AVX512F-NEXT:    vpextrq $1, %xmm0, %r9
; AVX512F-NEXT:    vmovq %xmm2, %r13
; AVX512F-NEXT:    vextracti128 $1, %ymm1, %xmm2
; AVX512F-NEXT:    vpextrq $1, %xmm2, %rbx
; AVX512F-NEXT:    vmovq %xmm0, %r11
; AVX512F-NEXT:    vpextrq $1, %xmm1, %r12
; AVX512F-NEXT:    vmovq %xmm2, %r14
; AVX512F-NEXT:    vmovq %xmm1, %r15
; AVX512F-NEXT:    vextracti32x4 $2, %zmm1, %xmm0
; AVX512F-NEXT:    vmovq %xmm0, %rbp
; AVX512F-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512F-NEXT:    cmovneq %r10, %rdx
; AVX512F-NEXT:    cmovneq %rbp, %r10
; AVX512F-NEXT:    vextracti32x4 $3, %zmm1, %xmm1
; AVX512F-NEXT:    cmoveq %rbp, %r15
; AVX512F-NEXT:    vmovq %xmm1, %rbp
; AVX512F-NEXT:    cmovneq %r11, %r13
; AVX512F-NEXT:    cmovneq %rbp, %r11
; AVX512F-NEXT:    cmoveq %rbp, %r14
; AVX512F-NEXT:    vpextrq $1, %xmm0, %rbp
; AVX512F-NEXT:    cmovneq %r8, %rsi
; AVX512F-NEXT:    cmovneq %rbp, %r8
; AVX512F-NEXT:    cmoveq %rbp, %r12
; AVX512F-NEXT:    vpextrq $1, %xmm1, %rbp
; AVX512F-NEXT:    cmovneq %r9, %rax
; AVX512F-NEXT:    cmovneq %rbp, %r9
; AVX512F-NEXT:    cmoveq %rbp, %rbx
; AVX512F-NEXT:    testb $-128, %cl
; AVX512F-NEXT:    cmoveq %r14, %r15
; AVX512F-NEXT:    cmoveq %r10, %r14
; AVX512F-NEXT:    cmoveq %r11, %r10
; AVX512F-NEXT:    cmoveq %rdx, %r11
; AVX512F-NEXT:    cmoveq %r13, %rdx
; AVX512F-NEXT:    cmoveq %rbx, %r12
; AVX512F-NEXT:    cmoveq %r8, %rbx
; AVX512F-NEXT:    cmoveq %r9, %r8
; AVX512F-NEXT:    cmoveq %rsi, %r9
; AVX512F-NEXT:    cmoveq %rax, %rsi
; AVX512F-NEXT:    movq %rdi, %rax
; AVX512F-NEXT:    testb $64, %cl
; AVX512F-NEXT:    cmovneq %rdx, %rsi
; AVX512F-NEXT:    cmovneq %r9, %rdx
; AVX512F-NEXT:    cmovneq %r11, %r9
; AVX512F-NEXT:    cmovneq %r8, %r11
; AVX512F-NEXT:    cmovneq %r10, %r8
; AVX512F-NEXT:    cmovneq %rbx, %r10
; AVX512F-NEXT:    cmovneq %r14, %rbx
; AVX512F-NEXT:    cmoveq %r12, %r15
; AVX512F-NEXT:    cmovneq %r12, %r14
; AVX512F-NEXT:    movq %r14, %rdi
; AVX512F-NEXT:    shldq %cl, %r15, %rdi
; AVX512F-NEXT:    movq %rbx, %r15
; AVX512F-NEXT:    shldq %cl, %r14, %r15
; AVX512F-NEXT:    movq %r10, %r14
; AVX512F-NEXT:    shldq %cl, %rbx, %r14
; AVX512F-NEXT:    movq %r8, %rbx
; AVX512F-NEXT:    shldq %cl, %r10, %rbx
; AVX512F-NEXT:    movq %r11, %r10
; AVX512F-NEXT:    shldq %cl, %r8, %r10
; AVX512F-NEXT:    movq %r9, %r8
; AVX512F-NEXT:    shldq %cl, %r11, %r8
; AVX512F-NEXT:    movq %rdx, %r11
; AVX512F-NEXT:    shldq %cl, %r9, %r11
; AVX512F-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512F-NEXT:    shldq %cl, %rdx, %rsi
; AVX512F-NEXT:    movq %rsi, 56(%rax)
; AVX512F-NEXT:    movq %r11, 48(%rax)
; AVX512F-NEXT:    movq %r8, 40(%rax)
; AVX512F-NEXT:    movq %r10, 32(%rax)
; AVX512F-NEXT:    movq %rbx, 24(%rax)
; AVX512F-NEXT:    movq %r14, 16(%rax)
; AVX512F-NEXT:    movq %r15, 8(%rax)
; AVX512F-NEXT:    movq %rdi, (%rax)
; AVX512F-NEXT:    popq %rbx
; AVX512F-NEXT:    popq %r12
; AVX512F-NEXT:    popq %r13
; AVX512F-NEXT:    popq %r14
; AVX512F-NEXT:    popq %r15
; AVX512F-NEXT:    popq %rbp
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: fshl_i512_vector:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    pushq %rbp
; AVX512VL-NEXT:    pushq %r15
; AVX512VL-NEXT:    pushq %r14
; AVX512VL-NEXT:    pushq %r13
; AVX512VL-NEXT:    pushq %r12
; AVX512VL-NEXT:    pushq %rbx
; AVX512VL-NEXT:    movq %rsi, %rcx
; AVX512VL-NEXT:    vmovq %xmm0, %r9
; AVX512VL-NEXT:    vextracti32x4 $2, %zmm0, %xmm2
; AVX512VL-NEXT:    vmovq %xmm2, %rdx
; AVX512VL-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512VL-NEXT:    vpextrq $1, %xmm2, %rsi
; AVX512VL-NEXT:    vextracti32x4 $3, %zmm0, %xmm2
; AVX512VL-NEXT:    vmovq %xmm2, %r13
; AVX512VL-NEXT:    vextracti128 $1, %ymm0, %xmm0
; AVX512VL-NEXT:    vpextrq $1, %xmm2, %r12
; AVX512VL-NEXT:    vpextrq $1, %xmm0, %r8
; AVX512VL-NEXT:    vmovq %xmm0, %r10
; AVX512VL-NEXT:    vextracti128 $1, %ymm1, %xmm0
; AVX512VL-NEXT:    vpextrq $1, %xmm0, %r11
; AVX512VL-NEXT:    vpextrq $1, %xmm1, %r15
; AVX512VL-NEXT:    vmovq %xmm0, %rbx
; AVX512VL-NEXT:    vmovq %xmm1, %r14
; AVX512VL-NEXT:    vextracti32x4 $2, %zmm1, %xmm0
; AVX512VL-NEXT:    vmovq %xmm0, %rbp
; AVX512VL-NEXT:    vextracti32x4 $3, %zmm1, %xmm1
; AVX512VL-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VL-NEXT:    cmovneq %r9, %rdx
; AVX512VL-NEXT:    cmovneq %rbp, %r9
; AVX512VL-NEXT:    cmoveq %rbp, %r14
; AVX512VL-NEXT:    vmovq %xmm1, %rbp
; AVX512VL-NEXT:    cmovneq %r10, %r13
; AVX512VL-NEXT:    cmovneq %rbp, %r10
; AVX512VL-NEXT:    cmoveq %rbp, %rbx
; AVX512VL-NEXT:    vpextrq $1, %xmm0, %rbp
; AVX512VL-NEXT:    cmovneq %rax, %rsi
; AVX512VL-NEXT:    cmovneq %rbp, %rax
; AVX512VL-NEXT:    cmoveq %rbp, %r15
; AVX512VL-NEXT:    vpextrq $1, %xmm1, %rbp
; AVX512VL-NEXT:    cmovneq %r8, %r12
; AVX512VL-NEXT:    cmovneq %rbp, %r8
; AVX512VL-NEXT:    cmoveq %rbp, %r11
; AVX512VL-NEXT:    testb $-128, %cl
; AVX512VL-NEXT:    cmoveq %rbx, %r14
; AVX512VL-NEXT:    cmoveq %r9, %rbx
; AVX512VL-NEXT:    cmoveq %r10, %r9
; AVX512VL-NEXT:    cmoveq %rdx, %r10
; AVX512VL-NEXT:    cmoveq %r13, %rdx
; AVX512VL-NEXT:    cmoveq %r11, %r15
; AVX512VL-NEXT:    cmoveq %rax, %r11
; AVX512VL-NEXT:    cmoveq %r8, %rax
; AVX512VL-NEXT:    cmoveq %rsi, %r8
; AVX512VL-NEXT:    cmoveq %r12, %rsi
; AVX512VL-NEXT:    testb $64, %cl
; AVX512VL-NEXT:    cmovneq %rdx, %rsi
; AVX512VL-NEXT:    cmovneq %r8, %rdx
; AVX512VL-NEXT:    cmovneq %r10, %r8
; AVX512VL-NEXT:    cmovneq %rax, %r10
; AVX512VL-NEXT:    cmovneq %r9, %rax
; AVX512VL-NEXT:    cmovneq %r11, %r9
; AVX512VL-NEXT:    cmovneq %rbx, %r11
; AVX512VL-NEXT:    cmoveq %r15, %r14
; AVX512VL-NEXT:    cmovneq %r15, %rbx
; AVX512VL-NEXT:    movq %rbx, %r15
; AVX512VL-NEXT:    shldq %cl, %r14, %r15
; AVX512VL-NEXT:    movq %r11, %r14
; AVX512VL-NEXT:    shldq %cl, %rbx, %r14
; AVX512VL-NEXT:    movq %r9, %rbx
; AVX512VL-NEXT:    shldq %cl, %r11, %rbx
; AVX512VL-NEXT:    movq %rax, %r11
; AVX512VL-NEXT:    shldq %cl, %r9, %r11
; AVX512VL-NEXT:    movq %r10, %r9
; AVX512VL-NEXT:    shldq %cl, %rax, %r9
; AVX512VL-NEXT:    movq %r8, %r12
; AVX512VL-NEXT:    shldq %cl, %r10, %r12
; AVX512VL-NEXT:    movq %rdi, %rax
; AVX512VL-NEXT:    movq %rdx, %rdi
; AVX512VL-NEXT:    shldq %cl, %r8, %rdi
; AVX512VL-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VL-NEXT:    shldq %cl, %rdx, %rsi
; AVX512VL-NEXT:    movq %rsi, 56(%rax)
; AVX512VL-NEXT:    movq %rdi, 48(%rax)
; AVX512VL-NEXT:    movq %r12, 40(%rax)
; AVX512VL-NEXT:    movq %r9, 32(%rax)
; AVX512VL-NEXT:    movq %r11, 24(%rax)
; AVX512VL-NEXT:    movq %rbx, 16(%rax)
; AVX512VL-NEXT:    movq %r14, 8(%rax)
; AVX512VL-NEXT:    movq %r15, (%rax)
; AVX512VL-NEXT:    popq %rbx
; AVX512VL-NEXT:    popq %r12
; AVX512VL-NEXT:    popq %r13
; AVX512VL-NEXT:    popq %r14
; AVX512VL-NEXT:    popq %r15
; AVX512VL-NEXT:    popq %rbp
; AVX512VL-NEXT:    vzeroupper
; AVX512VL-NEXT:    retq
;
; AVX512VBMI-LABEL: fshl_i512_vector:
; AVX512VBMI:       # %bb.0:
; AVX512VBMI-NEXT:    pushq %rbp
; AVX512VBMI-NEXT:    pushq %r15
; AVX512VBMI-NEXT:    pushq %r14
; AVX512VBMI-NEXT:    pushq %r13
; AVX512VBMI-NEXT:    pushq %r12
; AVX512VBMI-NEXT:    pushq %rbx
; AVX512VBMI-NEXT:    movq %rsi, %rcx
; AVX512VBMI-NEXT:    vmovq %xmm0, %r9
; AVX512VBMI-NEXT:    vextracti32x4 $2, %zmm0, %xmm2
; AVX512VBMI-NEXT:    vmovq %xmm2, %rdx
; AVX512VBMI-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512VBMI-NEXT:    vpextrq $1, %xmm2, %rsi
; AVX512VBMI-NEXT:    vextracti32x4 $3, %zmm0, %xmm2
; AVX512VBMI-NEXT:    vmovq %xmm2, %r13
; AVX512VBMI-NEXT:    vextracti128 $1, %ymm0, %xmm0
; AVX512VBMI-NEXT:    vpextrq $1, %xmm2, %r12
; AVX512VBMI-NEXT:    vpextrq $1, %xmm0, %r8
; AVX512VBMI-NEXT:    vmovq %xmm0, %r10
; AVX512VBMI-NEXT:    vextracti128 $1, %ymm1, %xmm0
; AVX512VBMI-NEXT:    vpextrq $1, %xmm0, %r11
; AVX512VBMI-NEXT:    vpextrq $1, %xmm1, %r15
; AVX512VBMI-NEXT:    vmovq %xmm0, %rbx
; AVX512VBMI-NEXT:    vmovq %xmm1, %r14
; AVX512VBMI-NEXT:    vextracti32x4 $2, %zmm1, %xmm0
; AVX512VBMI-NEXT:    vmovq %xmm0, %rbp
; AVX512VBMI-NEXT:    vextracti32x4 $3, %zmm1, %xmm1
; AVX512VBMI-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VBMI-NEXT:    cmovneq %r9, %rdx
; AVX512VBMI-NEXT:    cmovneq %rbp, %r9
; AVX512VBMI-NEXT:    cmoveq %rbp, %r14
; AVX512VBMI-NEXT:    vmovq %xmm1, %rbp
; AVX512VBMI-NEXT:    cmovneq %r10, %r13
; AVX512VBMI-NEXT:    cmovneq %rbp, %r10
; AVX512VBMI-NEXT:    cmoveq %rbp, %rbx
; AVX512VBMI-NEXT:    vpextrq $1, %xmm0, %rbp
; AVX512VBMI-NEXT:    cmovneq %rax, %rsi
; AVX512VBMI-NEXT:    cmovneq %rbp, %rax
; AVX512VBMI-NEXT:    cmoveq %rbp, %r15
; AVX512VBMI-NEXT:    vpextrq $1, %xmm1, %rbp
; AVX512VBMI-NEXT:    cmovneq %r8, %r12
; AVX512VBMI-NEXT:    cmovneq %rbp, %r8
; AVX512VBMI-NEXT:    cmoveq %rbp, %r11
; AVX512VBMI-NEXT:    testb $-128, %cl
; AVX512VBMI-NEXT:    cmoveq %rbx, %r14
; AVX512VBMI-NEXT:    cmoveq %r9, %rbx
; AVX512VBMI-NEXT:    cmoveq %r10, %r9
; AVX512VBMI-NEXT:    cmoveq %rdx, %r10
; AVX512VBMI-NEXT:    cmoveq %r13, %rdx
; AVX512VBMI-NEXT:    cmoveq %r11, %r15
; AVX512VBMI-NEXT:    cmoveq %rax, %r11
; AVX512VBMI-NEXT:    cmoveq %r8, %rax
; AVX512VBMI-NEXT:    cmoveq %rsi, %r8
; AVX512VBMI-NEXT:    cmoveq %r12, %rsi
; AVX512VBMI-NEXT:    testb $64, %cl
; AVX512VBMI-NEXT:    cmovneq %rdx, %rsi
; AVX512VBMI-NEXT:    cmovneq %r8, %rdx
; AVX512VBMI-NEXT:    cmovneq %r10, %r8
; AVX512VBMI-NEXT:    cmovneq %rax, %r10
; AVX512VBMI-NEXT:    cmovneq %r9, %rax
; AVX512VBMI-NEXT:    cmovneq %r11, %r9
; AVX512VBMI-NEXT:    cmovneq %rbx, %r11
; AVX512VBMI-NEXT:    cmoveq %r15, %r14
; AVX512VBMI-NEXT:    cmovneq %r15, %rbx
; AVX512VBMI-NEXT:    movq %rbx, %r15
; AVX512VBMI-NEXT:    shldq %cl, %r14, %r15
; AVX512VBMI-NEXT:    movq %r11, %r14
; AVX512VBMI-NEXT:    shldq %cl, %rbx, %r14
; AVX512VBMI-NEXT:    movq %r9, %rbx
; AVX512VBMI-NEXT:    shldq %cl, %r11, %rbx
; AVX512VBMI-NEXT:    movq %rax, %r11
; AVX512VBMI-NEXT:    shldq %cl, %r9, %r11
; AVX512VBMI-NEXT:    movq %r10, %r9
; AVX512VBMI-NEXT:    shldq %cl, %rax, %r9
; AVX512VBMI-NEXT:    movq %r8, %r12
; AVX512VBMI-NEXT:    shldq %cl, %r10, %r12
; AVX512VBMI-NEXT:    movq %rdi, %rax
; AVX512VBMI-NEXT:    movq %rdx, %rdi
; AVX512VBMI-NEXT:    shldq %cl, %r8, %rdi
; AVX512VBMI-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VBMI-NEXT:    shldq %cl, %rdx, %rsi
; AVX512VBMI-NEXT:    movq %rsi, 56(%rax)
; AVX512VBMI-NEXT:    movq %rdi, 48(%rax)
; AVX512VBMI-NEXT:    movq %r12, 40(%rax)
; AVX512VBMI-NEXT:    movq %r9, 32(%rax)
; AVX512VBMI-NEXT:    movq %r11, 24(%rax)
; AVX512VBMI-NEXT:    movq %rbx, 16(%rax)
; AVX512VBMI-NEXT:    movq %r14, 8(%rax)
; AVX512VBMI-NEXT:    movq %r15, (%rax)
; AVX512VBMI-NEXT:    popq %rbx
; AVX512VBMI-NEXT:    popq %r12
; AVX512VBMI-NEXT:    popq %r13
; AVX512VBMI-NEXT:    popq %r14
; AVX512VBMI-NEXT:    popq %r15
; AVX512VBMI-NEXT:    popq %rbp
; AVX512VBMI-NEXT:    vzeroupper
; AVX512VBMI-NEXT:    retq
  %a0 = bitcast <8 x i64> %v0 to i512
  %a1 = bitcast <8 x i64> %v1 to i512
  %r = call i512 @llvm.fshl.i512(i512 %a0, i512 %a1, i512 %a2)
  ret i512 %r
}

define i512 @fshr_i512_vector(<8 x i64> %v0, <8 x i64> %v1, i512 %a2) nounwind {
; SSE2-LABEL: fshr_i512_vector:
; SSE2:       # %bb.0:
; SSE2-NEXT:    pushq %rbp
; SSE2-NEXT:    pushq %r15
; SSE2-NEXT:    pushq %r14
; SSE2-NEXT:    pushq %r13
; SSE2-NEXT:    pushq %r12
; SSE2-NEXT:    pushq %rbx
; SSE2-NEXT:    movq %rsi, %rcx
; SSE2-NEXT:    movq %xmm0, %rdx
; SSE2-NEXT:    movq %xmm2, %r8
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %rsi
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm2[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %rax
; SSE2-NEXT:    movq %xmm1, %r9
; SSE2-NEXT:    movq %xmm3, %r13
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm1[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %r11
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm3[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %r12
; SSE2-NEXT:    movq %xmm4, %r14
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm4[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %r10
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm5[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %rbx
; SSE2-NEXT:    movq %xmm6, %r15
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm7[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %rbp
; SSE2-NEXT:    testl $256, %ecx # imm = 0x100
; SSE2-NEXT:    cmoveq %r11, %r12
; SSE2-NEXT:    cmoveq %rbp, %r11
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm6[2,3,2,3]
; SSE2-NEXT:    cmovneq %rbp, %rbx
; SSE2-NEXT:    movq %xmm0, %rbp
; SSE2-NEXT:    cmoveq %rsi, %rax
; SSE2-NEXT:    cmoveq %rdx, %r8
; SSE2-NEXT:    cmoveq %rbp, %rsi
; SSE2-NEXT:    cmoveq %r15, %rdx
; SSE2-NEXT:    cmovneq %rbp, %r10
; SSE2-NEXT:    cmovneq %r15, %r14
; SSE2-NEXT:    movq %xmm5, %r15
; SSE2-NEXT:    movq %xmm7, %rbp
; SSE2-NEXT:    cmoveq %r9, %r13
; SSE2-NEXT:    cmoveq %rbp, %r9
; SSE2-NEXT:    cmovneq %rbp, %r15
; SSE2-NEXT:    testb $-128, %cl
; SSE2-NEXT:    cmovneq %r15, %r14
; SSE2-NEXT:    cmovneq %rdx, %r15
; SSE2-NEXT:    cmovneq %r9, %rdx
; SSE2-NEXT:    cmovneq %r8, %r9
; SSE2-NEXT:    cmovneq %r13, %r8
; SSE2-NEXT:    cmovneq %rbx, %r10
; SSE2-NEXT:    cmovneq %rsi, %rbx
; SSE2-NEXT:    cmovneq %r11, %rsi
; SSE2-NEXT:    cmovneq %rax, %r11
; SSE2-NEXT:    cmovneq %r12, %rax
; SSE2-NEXT:    testb $64, %cl
; SSE2-NEXT:    cmoveq %r8, %rax
; SSE2-NEXT:    cmoveq %r11, %r8
; SSE2-NEXT:    cmoveq %r9, %r11
; SSE2-NEXT:    cmoveq %rsi, %r9
; SSE2-NEXT:    cmoveq %rdx, %rsi
; SSE2-NEXT:    cmoveq %rbx, %rdx
; SSE2-NEXT:    cmoveq %r15, %rbx
; SSE2-NEXT:    cmoveq %r10, %r15
; SSE2-NEXT:    cmoveq %r14, %r10
; SSE2-NEXT:    shrdq %cl, %r15, %r10
; SSE2-NEXT:    shrdq %cl, %rbx, %r15
; SSE2-NEXT:    shrdq %cl, %rdx, %rbx
; SSE2-NEXT:    shrdq %cl, %rsi, %rdx
; SSE2-NEXT:    shrdq %cl, %r9, %rsi
; SSE2-NEXT:    shrdq %cl, %r11, %r9
; SSE2-NEXT:    shrdq %cl, %r8, %r11
; SSE2-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE2-NEXT:    shrdq %cl, %rax, %r8
; SSE2-NEXT:    movq %rdi, %rax
; SSE2-NEXT:    movq %r8, 56(%rdi)
; SSE2-NEXT:    movq %r11, 48(%rdi)
; SSE2-NEXT:    movq %r9, 40(%rdi)
; SSE2-NEXT:    movq %rsi, 32(%rdi)
; SSE2-NEXT:    movq %rdx, 24(%rdi)
; SSE2-NEXT:    movq %rbx, 16(%rdi)
; SSE2-NEXT:    movq %r15, 8(%rdi)
; SSE2-NEXT:    movq %r10, (%rdi)
; SSE2-NEXT:    popq %rbx
; SSE2-NEXT:    popq %r12
; SSE2-NEXT:    popq %r13
; SSE2-NEXT:    popq %r14
; SSE2-NEXT:    popq %r15
; SSE2-NEXT:    popq %rbp
; SSE2-NEXT:    retq
;
; SSE42-LABEL: fshr_i512_vector:
; SSE42:       # %bb.0:
; SSE42-NEXT:    pushq %rbp
; SSE42-NEXT:    pushq %r15
; SSE42-NEXT:    pushq %r14
; SSE42-NEXT:    pushq %r13
; SSE42-NEXT:    pushq %r12
; SSE42-NEXT:    pushq %rbx
; SSE42-NEXT:    movq %rsi, %rcx
; SSE42-NEXT:    movq %xmm0, %rdx
; SSE42-NEXT:    pextrq $1, %xmm0, %rsi
; SSE42-NEXT:    pextrq $1, %xmm2, %rax
; SSE42-NEXT:    movq %xmm2, %r9
; SSE42-NEXT:    movq %xmm1, %r8
; SSE42-NEXT:    pextrq $1, %xmm1, %r11
; SSE42-NEXT:    movq %xmm3, %r13
; SSE42-NEXT:    pextrq $1, %xmm3, %r12
; SSE42-NEXT:    movq %xmm4, %r14
; SSE42-NEXT:    pextrq $1, %xmm4, %r10
; SSE42-NEXT:    pextrq $1, %xmm5, %rbx
; SSE42-NEXT:    movq %xmm6, %r15
; SSE42-NEXT:    pextrq $1, %xmm7, %rbp
; SSE42-NEXT:    testl $256, %ecx # imm = 0x100
; SSE42-NEXT:    cmoveq %r11, %r12
; SSE42-NEXT:    cmoveq %rbp, %r11
; SSE42-NEXT:    cmovneq %rbp, %rbx
; SSE42-NEXT:    pextrq $1, %xmm6, %rbp
; SSE42-NEXT:    cmoveq %rsi, %rax
; SSE42-NEXT:    cmoveq %rdx, %r9
; SSE42-NEXT:    cmoveq %rbp, %rsi
; SSE42-NEXT:    cmoveq %r15, %rdx
; SSE42-NEXT:    cmovneq %rbp, %r10
; SSE42-NEXT:    cmovneq %r15, %r14
; SSE42-NEXT:    movq %xmm5, %r15
; SSE42-NEXT:    movq %xmm7, %rbp
; SSE42-NEXT:    cmoveq %r8, %r13
; SSE42-NEXT:    cmoveq %rbp, %r8
; SSE42-NEXT:    cmovneq %rbp, %r15
; SSE42-NEXT:    testb $-128, %cl
; SSE42-NEXT:    cmovneq %r15, %r14
; SSE42-NEXT:    cmovneq %rdx, %r15
; SSE42-NEXT:    cmovneq %r8, %rdx
; SSE42-NEXT:    cmovneq %r9, %r8
; SSE42-NEXT:    cmovneq %r13, %r9
; SSE42-NEXT:    cmovneq %rbx, %r10
; SSE42-NEXT:    cmovneq %rsi, %rbx
; SSE42-NEXT:    cmovneq %r11, %rsi
; SSE42-NEXT:    cmovneq %rax, %r11
; SSE42-NEXT:    cmovneq %r12, %rax
; SSE42-NEXT:    testb $64, %cl
; SSE42-NEXT:    cmoveq %r9, %rax
; SSE42-NEXT:    cmoveq %r11, %r9
; SSE42-NEXT:    cmoveq %r8, %r11
; SSE42-NEXT:    cmoveq %rsi, %r8
; SSE42-NEXT:    cmoveq %rdx, %rsi
; SSE42-NEXT:    cmoveq %rbx, %rdx
; SSE42-NEXT:    cmoveq %r15, %rbx
; SSE42-NEXT:    cmoveq %r10, %r15
; SSE42-NEXT:    cmoveq %r14, %r10
; SSE42-NEXT:    shrdq %cl, %r15, %r10
; SSE42-NEXT:    shrdq %cl, %rbx, %r15
; SSE42-NEXT:    shrdq %cl, %rdx, %rbx
; SSE42-NEXT:    shrdq %cl, %rsi, %rdx
; SSE42-NEXT:    shrdq %cl, %r8, %rsi
; SSE42-NEXT:    shrdq %cl, %r11, %r8
; SSE42-NEXT:    shrdq %cl, %r9, %r11
; SSE42-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE42-NEXT:    shrdq %cl, %rax, %r9
; SSE42-NEXT:    movq %rdi, %rax
; SSE42-NEXT:    movq %r9, 56(%rdi)
; SSE42-NEXT:    movq %r11, 48(%rdi)
; SSE42-NEXT:    movq %r8, 40(%rdi)
; SSE42-NEXT:    movq %rsi, 32(%rdi)
; SSE42-NEXT:    movq %rdx, 24(%rdi)
; SSE42-NEXT:    movq %rbx, 16(%rdi)
; SSE42-NEXT:    movq %r15, 8(%rdi)
; SSE42-NEXT:    movq %r10, (%rdi)
; SSE42-NEXT:    popq %rbx
; SSE42-NEXT:    popq %r12
; SSE42-NEXT:    popq %r13
; SSE42-NEXT:    popq %r14
; SSE42-NEXT:    popq %r15
; SSE42-NEXT:    popq %rbp
; SSE42-NEXT:    retq
;
; AVX2-LABEL: fshr_i512_vector:
; AVX2:       # %bb.0:
; AVX2-NEXT:    pushq %rbp
; AVX2-NEXT:    pushq %r15
; AVX2-NEXT:    pushq %r14
; AVX2-NEXT:    pushq %r13
; AVX2-NEXT:    pushq %r12
; AVX2-NEXT:    pushq %rbx
; AVX2-NEXT:    movq %rsi, %rcx
; AVX2-NEXT:    vmovq %xmm0, %rdx
; AVX2-NEXT:    vmovq %xmm1, %r8
; AVX2-NEXT:    vpextrq $1, %xmm0, %rsi
; AVX2-NEXT:    vpextrq $1, %xmm1, %r10
; AVX2-NEXT:    vextracti128 $1, %ymm0, %xmm0
; AVX2-NEXT:    vmovq %xmm0, %r9
; AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm1
; AVX2-NEXT:    vmovq %xmm1, %r13
; AVX2-NEXT:    vpextrq $1, %xmm0, %rbx
; AVX2-NEXT:    vpextrq $1, %xmm1, %rax
; AVX2-NEXT:    vextracti128 $1, %ymm2, %xmm0
; AVX2-NEXT:    vmovq %xmm2, %r15
; AVX2-NEXT:    vpextrq $1, %xmm2, %r11
; AVX2-NEXT:    vpextrq $1, %xmm0, %r14
; AVX2-NEXT:    vpextrq $1, %xmm3, %r12
; AVX2-NEXT:    vextracti128 $1, %ymm3, %xmm1
; AVX2-NEXT:    vpextrq $1, %xmm1, %rbp
; AVX2-NEXT:    testl $256, %ecx # imm = 0x100
; AVX2-NEXT:    cmoveq %rbx, %rax
; AVX2-NEXT:    cmoveq %rsi, %r10
; AVX2-NEXT:    cmoveq %rbp, %rbx
; AVX2-NEXT:    cmoveq %r12, %rsi
; AVX2-NEXT:    cmovneq %rbp, %r14
; AVX2-NEXT:    cmovneq %r12, %r11
; AVX2-NEXT:    vmovq %xmm3, %r12
; AVX2-NEXT:    vmovq %xmm1, %rbp
; AVX2-NEXT:    cmoveq %r9, %r13
; AVX2-NEXT:    cmoveq %rdx, %r8
; AVX2-NEXT:    cmoveq %rbp, %r9
; AVX2-NEXT:    cmoveq %r12, %rdx
; AVX2-NEXT:    cmovneq %r12, %r15
; AVX2-NEXT:    vmovq %xmm0, %r12
; AVX2-NEXT:    cmovneq %rbp, %r12
; AVX2-NEXT:    testb $-128, %cl
; AVX2-NEXT:    cmovneq %r12, %r15
; AVX2-NEXT:    cmovneq %rdx, %r12
; AVX2-NEXT:    cmovneq %r9, %rdx
; AVX2-NEXT:    cmovneq %r8, %r9
; AVX2-NEXT:    cmovneq %r13, %r8
; AVX2-NEXT:    cmovneq %r14, %r11
; AVX2-NEXT:    cmovneq %rsi, %r14
; AVX2-NEXT:    cmovneq %rbx, %rsi
; AVX2-NEXT:    cmovneq %r10, %rbx
; AVX2-NEXT:    cmovneq %rax, %r10
; AVX2-NEXT:    movq %rdi, %rax
; AVX2-NEXT:    testb $64, %cl
; AVX2-NEXT:    cmoveq %r8, %r10
; AVX2-NEXT:    cmoveq %rbx, %r8
; AVX2-NEXT:    cmoveq %r9, %rbx
; AVX2-NEXT:    cmoveq %rsi, %r9
; AVX2-NEXT:    cmoveq %rdx, %rsi
; AVX2-NEXT:    cmoveq %r14, %rdx
; AVX2-NEXT:    cmoveq %r12, %r14
; AVX2-NEXT:    cmoveq %r11, %r12
; AVX2-NEXT:    cmoveq %r15, %r11
; AVX2-NEXT:    shrdq %cl, %r12, %r11
; AVX2-NEXT:    shrdq %cl, %r14, %r12
; AVX2-NEXT:    shrdq %cl, %rdx, %r14
; AVX2-NEXT:    shrdq %cl, %rsi, %rdx
; AVX2-NEXT:    shrdq %cl, %r9, %rsi
; AVX2-NEXT:    shrdq %cl, %rbx, %r9
; AVX2-NEXT:    shrdq %cl, %r8, %rbx
; AVX2-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX2-NEXT:    shrdq %cl, %r10, %r8
; AVX2-NEXT:    movq %r8, 56(%rdi)
; AVX2-NEXT:    movq %rbx, 48(%rdi)
; AVX2-NEXT:    movq %r9, 40(%rdi)
; AVX2-NEXT:    movq %rsi, 32(%rdi)
; AVX2-NEXT:    movq %rdx, 24(%rdi)
; AVX2-NEXT:    movq %r14, 16(%rdi)
; AVX2-NEXT:    movq %r12, 8(%rdi)
; AVX2-NEXT:    movq %r11, (%rdi)
; AVX2-NEXT:    popq %rbx
; AVX2-NEXT:    popq %r12
; AVX2-NEXT:    popq %r13
; AVX2-NEXT:    popq %r14
; AVX2-NEXT:    popq %r15
; AVX2-NEXT:    popq %rbp
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: fshr_i512_vector:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    pushq %rbp
; AVX512F-NEXT:    pushq %r15
; AVX512F-NEXT:    pushq %r14
; AVX512F-NEXT:    pushq %r13
; AVX512F-NEXT:    pushq %r12
; AVX512F-NEXT:    pushq %rbx
; AVX512F-NEXT:    movq %rsi, %rcx
; AVX512F-NEXT:    vmovq %xmm0, %rdx
; AVX512F-NEXT:    vextracti32x4 $2, %zmm0, %xmm2
; AVX512F-NEXT:    vpextrq $1, %xmm0, %rsi
; AVX512F-NEXT:    vmovq %xmm2, %r8
; AVX512F-NEXT:    vpextrq $1, %xmm2, %r9
; AVX512F-NEXT:    vextracti32x4 $3, %zmm0, %xmm2
; AVX512F-NEXT:    vmovq %xmm2, %r13
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm0
; AVX512F-NEXT:    vpextrq $1, %xmm2, %rax
; AVX512F-NEXT:    vmovq %xmm0, %r10
; AVX512F-NEXT:    vpextrq $1, %xmm0, %r11
; AVX512F-NEXT:    vextracti128 $1, %ymm1, %xmm0
; AVX512F-NEXT:    vpextrq $1, %xmm0, %r14
; AVX512F-NEXT:    vpextrq $1, %xmm1, %rbx
; AVX512F-NEXT:    vmovq %xmm1, %r15
; AVX512F-NEXT:    vextracti32x4 $2, %zmm1, %xmm2
; AVX512F-NEXT:    vpextrq $1, %xmm2, %r12
; AVX512F-NEXT:    vextracti32x4 $3, %zmm1, %xmm1
; AVX512F-NEXT:    vpextrq $1, %xmm1, %rbp
; AVX512F-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512F-NEXT:    cmoveq %r11, %rax
; AVX512F-NEXT:    cmoveq %rsi, %r9
; AVX512F-NEXT:    cmoveq %rbp, %r11
; AVX512F-NEXT:    cmoveq %r12, %rsi
; AVX512F-NEXT:    cmovneq %r12, %rbx
; AVX512F-NEXT:    cmovneq %rbp, %r14
; AVX512F-NEXT:    vmovq %xmm2, %r12
; AVX512F-NEXT:    vmovq %xmm1, %rbp
; AVX512F-NEXT:    cmoveq %r10, %r13
; AVX512F-NEXT:    cmoveq %rdx, %r8
; AVX512F-NEXT:    cmoveq %rbp, %r10
; AVX512F-NEXT:    cmoveq %r12, %rdx
; AVX512F-NEXT:    cmovneq %r12, %r15
; AVX512F-NEXT:    vmovq %xmm0, %r12
; AVX512F-NEXT:    cmovneq %rbp, %r12
; AVX512F-NEXT:    testb $-128, %cl
; AVX512F-NEXT:    cmovneq %r12, %r15
; AVX512F-NEXT:    cmovneq %rdx, %r12
; AVX512F-NEXT:    cmovneq %r10, %rdx
; AVX512F-NEXT:    cmovneq %r8, %r10
; AVX512F-NEXT:    cmovneq %r13, %r8
; AVX512F-NEXT:    cmovneq %r14, %rbx
; AVX512F-NEXT:    cmovneq %rsi, %r14
; AVX512F-NEXT:    cmovneq %r11, %rsi
; AVX512F-NEXT:    cmovneq %r9, %r11
; AVX512F-NEXT:    cmovneq %rax, %r9
; AVX512F-NEXT:    movq %rdi, %rax
; AVX512F-NEXT:    testb $64, %cl
; AVX512F-NEXT:    cmoveq %r8, %r9
; AVX512F-NEXT:    cmoveq %r11, %r8
; AVX512F-NEXT:    cmoveq %r10, %r11
; AVX512F-NEXT:    cmoveq %rsi, %r10
; AVX512F-NEXT:    cmoveq %rdx, %rsi
; AVX512F-NEXT:    cmoveq %r14, %rdx
; AVX512F-NEXT:    cmoveq %r12, %r14
; AVX512F-NEXT:    cmoveq %rbx, %r12
; AVX512F-NEXT:    cmoveq %r15, %rbx
; AVX512F-NEXT:    shrdq %cl, %r12, %rbx
; AVX512F-NEXT:    shrdq %cl, %r14, %r12
; AVX512F-NEXT:    shrdq %cl, %rdx, %r14
; AVX512F-NEXT:    shrdq %cl, %rsi, %rdx
; AVX512F-NEXT:    shrdq %cl, %r10, %rsi
; AVX512F-NEXT:    shrdq %cl, %r11, %r10
; AVX512F-NEXT:    shrdq %cl, %r8, %r11
; AVX512F-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512F-NEXT:    shrdq %cl, %r9, %r8
; AVX512F-NEXT:    movq %r8, 56(%rdi)
; AVX512F-NEXT:    movq %r11, 48(%rdi)
; AVX512F-NEXT:    movq %r10, 40(%rdi)
; AVX512F-NEXT:    movq %rsi, 32(%rdi)
; AVX512F-NEXT:    movq %rdx, 24(%rdi)
; AVX512F-NEXT:    movq %r14, 16(%rdi)
; AVX512F-NEXT:    movq %r12, 8(%rdi)
; AVX512F-NEXT:    movq %rbx, (%rdi)
; AVX512F-NEXT:    popq %rbx
; AVX512F-NEXT:    popq %r12
; AVX512F-NEXT:    popq %r13
; AVX512F-NEXT:    popq %r14
; AVX512F-NEXT:    popq %r15
; AVX512F-NEXT:    popq %rbp
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: fshr_i512_vector:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    pushq %rbp
; AVX512VL-NEXT:    pushq %r15
; AVX512VL-NEXT:    pushq %r14
; AVX512VL-NEXT:    pushq %r13
; AVX512VL-NEXT:    pushq %r12
; AVX512VL-NEXT:    pushq %rbx
; AVX512VL-NEXT:    movq %rsi, %rcx
; AVX512VL-NEXT:    vmovq %xmm0, %rdx
; AVX512VL-NEXT:    vextracti32x4 $2, %zmm0, %xmm2
; AVX512VL-NEXT:    vmovq %xmm2, %r8
; AVX512VL-NEXT:    vpextrq $1, %xmm0, %rsi
; AVX512VL-NEXT:    vpextrq $1, %xmm2, %r9
; AVX512VL-NEXT:    vextracti32x4 $3, %zmm0, %xmm2
; AVX512VL-NEXT:    vmovq %xmm2, %r13
; AVX512VL-NEXT:    vextracti128 $1, %ymm0, %xmm0
; AVX512VL-NEXT:    vpextrq $1, %xmm2, %r12
; AVX512VL-NEXT:    vpextrq $1, %xmm0, %rbx
; AVX512VL-NEXT:    vmovq %xmm0, %r11
; AVX512VL-NEXT:    vextracti128 $1, %ymm1, %xmm0
; AVX512VL-NEXT:    vmovq %xmm0, %r10
; AVX512VL-NEXT:    vmovq %xmm1, %rax
; AVX512VL-NEXT:    vpextrq $1, %xmm0, %r15
; AVX512VL-NEXT:    vpextrq $1, %xmm1, %r14
; AVX512VL-NEXT:    vextracti32x4 $2, %zmm1, %xmm0
; AVX512VL-NEXT:    vpextrq $1, %xmm0, %rbp
; AVX512VL-NEXT:    vextracti32x4 $3, %zmm1, %xmm1
; AVX512VL-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VL-NEXT:    cmoveq %rsi, %r9
; AVX512VL-NEXT:    cmoveq %rbp, %rsi
; AVX512VL-NEXT:    cmovneq %rbp, %r14
; AVX512VL-NEXT:    vpextrq $1, %xmm1, %rbp
; AVX512VL-NEXT:    cmoveq %rbx, %r12
; AVX512VL-NEXT:    cmoveq %rbp, %rbx
; AVX512VL-NEXT:    cmovneq %rbp, %r15
; AVX512VL-NEXT:    vmovq %xmm0, %rbp
; AVX512VL-NEXT:    cmoveq %rdx, %r8
; AVX512VL-NEXT:    cmoveq %rbp, %rdx
; AVX512VL-NEXT:    cmovneq %rbp, %rax
; AVX512VL-NEXT:    vmovq %xmm1, %rbp
; AVX512VL-NEXT:    cmoveq %r11, %r13
; AVX512VL-NEXT:    cmoveq %rbp, %r11
; AVX512VL-NEXT:    cmovneq %rbp, %r10
; AVX512VL-NEXT:    testb $-128, %cl
; AVX512VL-NEXT:    cmovneq %r10, %rax
; AVX512VL-NEXT:    cmovneq %rdx, %r10
; AVX512VL-NEXT:    cmovneq %r11, %rdx
; AVX512VL-NEXT:    cmovneq %r8, %r11
; AVX512VL-NEXT:    cmovneq %r13, %r8
; AVX512VL-NEXT:    cmovneq %r15, %r14
; AVX512VL-NEXT:    cmovneq %rsi, %r15
; AVX512VL-NEXT:    cmovneq %rbx, %rsi
; AVX512VL-NEXT:    cmovneq %r9, %rbx
; AVX512VL-NEXT:    cmovneq %r12, %r9
; AVX512VL-NEXT:    testb $64, %cl
; AVX512VL-NEXT:    cmoveq %r8, %r9
; AVX512VL-NEXT:    cmoveq %rbx, %r8
; AVX512VL-NEXT:    cmoveq %r11, %rbx
; AVX512VL-NEXT:    cmoveq %rsi, %r11
; AVX512VL-NEXT:    cmoveq %rdx, %rsi
; AVX512VL-NEXT:    cmoveq %r15, %rdx
; AVX512VL-NEXT:    cmoveq %r10, %r15
; AVX512VL-NEXT:    cmoveq %r14, %r10
; AVX512VL-NEXT:    cmoveq %rax, %r14
; AVX512VL-NEXT:    shrdq %cl, %r10, %r14
; AVX512VL-NEXT:    shrdq %cl, %r15, %r10
; AVX512VL-NEXT:    shrdq %cl, %rdx, %r15
; AVX512VL-NEXT:    shrdq %cl, %rsi, %rdx
; AVX512VL-NEXT:    shrdq %cl, %r11, %rsi
; AVX512VL-NEXT:    shrdq %cl, %rbx, %r11
; AVX512VL-NEXT:    movq %rdi, %rax
; AVX512VL-NEXT:    shrdq %cl, %r8, %rbx
; AVX512VL-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VL-NEXT:    shrdq %cl, %r9, %r8
; AVX512VL-NEXT:    movq %r8, 56(%rdi)
; AVX512VL-NEXT:    movq %rbx, 48(%rdi)
; AVX512VL-NEXT:    movq %r11, 40(%rdi)
; AVX512VL-NEXT:    movq %rsi, 32(%rdi)
; AVX512VL-NEXT:    movq %rdx, 24(%rdi)
; AVX512VL-NEXT:    movq %r15, 16(%rdi)
; AVX512VL-NEXT:    movq %r10, 8(%rdi)
; AVX512VL-NEXT:    movq %r14, (%rdi)
; AVX512VL-NEXT:    popq %rbx
; AVX512VL-NEXT:    popq %r12
; AVX512VL-NEXT:    popq %r13
; AVX512VL-NEXT:    popq %r14
; AVX512VL-NEXT:    popq %r15
; AVX512VL-NEXT:    popq %rbp
; AVX512VL-NEXT:    vzeroupper
; AVX512VL-NEXT:    retq
;
; AVX512VBMI-LABEL: fshr_i512_vector:
; AVX512VBMI:       # %bb.0:
; AVX512VBMI-NEXT:    pushq %rbp
; AVX512VBMI-NEXT:    pushq %r15
; AVX512VBMI-NEXT:    pushq %r14
; AVX512VBMI-NEXT:    pushq %r13
; AVX512VBMI-NEXT:    pushq %r12
; AVX512VBMI-NEXT:    pushq %rbx
; AVX512VBMI-NEXT:    movq %rsi, %rcx
; AVX512VBMI-NEXT:    vmovq %xmm0, %rdx
; AVX512VBMI-NEXT:    vextracti32x4 $2, %zmm0, %xmm2
; AVX512VBMI-NEXT:    vmovq %xmm2, %r8
; AVX512VBMI-NEXT:    vpextrq $1, %xmm0, %rsi
; AVX512VBMI-NEXT:    vpextrq $1, %xmm2, %r9
; AVX512VBMI-NEXT:    vextracti32x4 $3, %zmm0, %xmm2
; AVX512VBMI-NEXT:    vmovq %xmm2, %r13
; AVX512VBMI-NEXT:    vextracti128 $1, %ymm0, %xmm0
; AVX512VBMI-NEXT:    vpextrq $1, %xmm2, %r12
; AVX512VBMI-NEXT:    vpextrq $1, %xmm0, %rbx
; AVX512VBMI-NEXT:    vmovq %xmm0, %r11
; AVX512VBMI-NEXT:    vextracti128 $1, %ymm1, %xmm0
; AVX512VBMI-NEXT:    vmovq %xmm0, %r10
; AVX512VBMI-NEXT:    vmovq %xmm1, %rax
; AVX512VBMI-NEXT:    vpextrq $1, %xmm0, %r15
; AVX512VBMI-NEXT:    vpextrq $1, %xmm1, %r14
; AVX512VBMI-NEXT:    vextracti32x4 $2, %zmm1, %xmm0
; AVX512VBMI-NEXT:    vpextrq $1, %xmm0, %rbp
; AVX512VBMI-NEXT:    vextracti32x4 $3, %zmm1, %xmm1
; AVX512VBMI-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VBMI-NEXT:    cmoveq %rsi, %r9
; AVX512VBMI-NEXT:    cmoveq %rbp, %rsi
; AVX512VBMI-NEXT:    cmovneq %rbp, %r14
; AVX512VBMI-NEXT:    vpextrq $1, %xmm1, %rbp
; AVX512VBMI-NEXT:    cmoveq %rbx, %r12
; AVX512VBMI-NEXT:    cmoveq %rbp, %rbx
; AVX512VBMI-NEXT:    cmovneq %rbp, %r15
; AVX512VBMI-NEXT:    vmovq %xmm0, %rbp
; AVX512VBMI-NEXT:    cmoveq %rdx, %r8
; AVX512VBMI-NEXT:    cmoveq %rbp, %rdx
; AVX512VBMI-NEXT:    cmovneq %rbp, %rax
; AVX512VBMI-NEXT:    vmovq %xmm1, %rbp
; AVX512VBMI-NEXT:    cmoveq %r11, %r13
; AVX512VBMI-NEXT:    cmoveq %rbp, %r11
; AVX512VBMI-NEXT:    cmovneq %rbp, %r10
; AVX512VBMI-NEXT:    testb $-128, %cl
; AVX512VBMI-NEXT:    cmovneq %r10, %rax
; AVX512VBMI-NEXT:    cmovneq %rdx, %r10
; AVX512VBMI-NEXT:    cmovneq %r11, %rdx
; AVX512VBMI-NEXT:    cmovneq %r8, %r11
; AVX512VBMI-NEXT:    cmovneq %r13, %r8
; AVX512VBMI-NEXT:    cmovneq %r15, %r14
; AVX512VBMI-NEXT:    cmovneq %rsi, %r15
; AVX512VBMI-NEXT:    cmovneq %rbx, %rsi
; AVX512VBMI-NEXT:    cmovneq %r9, %rbx
; AVX512VBMI-NEXT:    cmovneq %r12, %r9
; AVX512VBMI-NEXT:    testb $64, %cl
; AVX512VBMI-NEXT:    cmoveq %r8, %r9
; AVX512VBMI-NEXT:    cmoveq %rbx, %r8
; AVX512VBMI-NEXT:    cmoveq %r11, %rbx
; AVX512VBMI-NEXT:    cmoveq %rsi, %r11
; AVX512VBMI-NEXT:    cmoveq %rdx, %rsi
; AVX512VBMI-NEXT:    cmoveq %r15, %rdx
; AVX512VBMI-NEXT:    cmoveq %r10, %r15
; AVX512VBMI-NEXT:    cmoveq %r14, %r10
; AVX512VBMI-NEXT:    cmoveq %rax, %r14
; AVX512VBMI-NEXT:    shrdq %cl, %r10, %r14
; AVX512VBMI-NEXT:    shrdq %cl, %r15, %r10
; AVX512VBMI-NEXT:    shrdq %cl, %rdx, %r15
; AVX512VBMI-NEXT:    shrdq %cl, %rsi, %rdx
; AVX512VBMI-NEXT:    shrdq %cl, %r11, %rsi
; AVX512VBMI-NEXT:    shrdq %cl, %rbx, %r11
; AVX512VBMI-NEXT:    movq %rdi, %rax
; AVX512VBMI-NEXT:    shrdq %cl, %r8, %rbx
; AVX512VBMI-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VBMI-NEXT:    shrdq %cl, %r9, %r8
; AVX512VBMI-NEXT:    movq %r8, 56(%rdi)
; AVX512VBMI-NEXT:    movq %rbx, 48(%rdi)
; AVX512VBMI-NEXT:    movq %r11, 40(%rdi)
; AVX512VBMI-NEXT:    movq %rsi, 32(%rdi)
; AVX512VBMI-NEXT:    movq %rdx, 24(%rdi)
; AVX512VBMI-NEXT:    movq %r15, 16(%rdi)
; AVX512VBMI-NEXT:    movq %r10, 8(%rdi)
; AVX512VBMI-NEXT:    movq %r14, (%rdi)
; AVX512VBMI-NEXT:    popq %rbx
; AVX512VBMI-NEXT:    popq %r12
; AVX512VBMI-NEXT:    popq %r13
; AVX512VBMI-NEXT:    popq %r14
; AVX512VBMI-NEXT:    popq %r15
; AVX512VBMI-NEXT:    popq %rbp
; AVX512VBMI-NEXT:    vzeroupper
; AVX512VBMI-NEXT:    retq
  %a0 = bitcast <8 x i64> %v0 to i512
  %a1 = bitcast <8 x i64> %v1 to i512
  %r = call i512 @llvm.fshr.i512(i512 %a0, i512 %a1, i512 %a2)
  ret i512 %r
}

define i512 @fshl_rot_i512_vector(<8 x i64> %v0, i512 %a2) nounwind {
; SSE2-LABEL: fshl_rot_i512_vector:
; SSE2:       # %bb.0:
; SSE2-NEXT:    pushq %r15
; SSE2-NEXT:    pushq %r14
; SSE2-NEXT:    pushq %r13
; SSE2-NEXT:    pushq %r12
; SSE2-NEXT:    pushq %rbx
; SSE2-NEXT:    movq %rsi, %rcx
; SSE2-NEXT:    pshufd {{.*#+}} xmm4 = xmm2[2,3,2,3]
; SSE2-NEXT:    movq %xmm4, %rax
; SSE2-NEXT:    pshufd {{.*#+}} xmm4 = xmm0[2,3,2,3]
; SSE2-NEXT:    movq %xmm3, %rdx
; SSE2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[2,3,2,3]
; SSE2-NEXT:    movq %xmm3, %r10
; SSE2-NEXT:    pshufd {{.*#+}} xmm3 = xmm1[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %r8
; SSE2-NEXT:    movq %xmm2, %rbx
; SSE2-NEXT:    testl $256, %ecx # imm = 0x100
; SSE2-NEXT:    movq %rbx, %r11
; SSE2-NEXT:    cmovneq %r8, %r11
; SSE2-NEXT:    movq %xmm3, %r14
; SSE2-NEXT:    movq %r14, %r15
; SSE2-NEXT:    cmovneq %r10, %r15
; SSE2-NEXT:    movq %xmm1, %r12
; SSE2-NEXT:    movq %r12, %rsi
; SSE2-NEXT:    cmovneq %rdx, %rsi
; SSE2-NEXT:    movq %xmm4, %r13
; SSE2-NEXT:    movq %r13, %r9
; SSE2-NEXT:    cmovneq %rax, %r9
; SSE2-NEXT:    cmovneq %rbx, %r8
; SSE2-NEXT:    cmovneq %r12, %rdx
; SSE2-NEXT:    cmovneq %r13, %rax
; SSE2-NEXT:    cmovneq %r14, %r10
; SSE2-NEXT:    testb $-128, %cl
; SSE2-NEXT:    movq %r9, %r14
; SSE2-NEXT:    cmovneq %r10, %r14
; SSE2-NEXT:    cmovneq %rax, %r10
; SSE2-NEXT:    movq %rsi, %rbx
; SSE2-NEXT:    cmovneq %r8, %rbx
; SSE2-NEXT:    cmovneq %rdx, %r8
; SSE2-NEXT:    cmoveq %r15, %r9
; SSE2-NEXT:    cmoveq %r11, %rsi
; SSE2-NEXT:    cmovneq %r15, %rax
; SSE2-NEXT:    cmovneq %r11, %rdx
; SSE2-NEXT:    testb $64, %cl
; SSE2-NEXT:    movq %rdx, %r11
; SSE2-NEXT:    cmovneq %rax, %r11
; SSE2-NEXT:    cmovneq %rsi, %rax
; SSE2-NEXT:    cmovneq %r9, %rsi
; SSE2-NEXT:    cmovneq %rbx, %r9
; SSE2-NEXT:    cmovneq %r14, %rbx
; SSE2-NEXT:    cmovneq %r8, %r14
; SSE2-NEXT:    cmoveq %r10, %rdx
; SSE2-NEXT:    cmovneq %r10, %r8
; SSE2-NEXT:    movq %r8, %r10
; SSE2-NEXT:    shldq %cl, %rdx, %r10
; SSE2-NEXT:    movq %r14, %r15
; SSE2-NEXT:    shldq %cl, %r8, %r15
; SSE2-NEXT:    movq %rbx, %r8
; SSE2-NEXT:    shldq %cl, %r14, %r8
; SSE2-NEXT:    movq %r9, %r14
; SSE2-NEXT:    shldq %cl, %rbx, %r14
; SSE2-NEXT:    movq %rsi, %rbx
; SSE2-NEXT:    shldq %cl, %r9, %rbx
; SSE2-NEXT:    movq %rax, %r9
; SSE2-NEXT:    shldq %cl, %rsi, %r9
; SSE2-NEXT:    movq %r11, %rsi
; SSE2-NEXT:    shldq %cl, %rax, %rsi
; SSE2-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE2-NEXT:    shldq %cl, %r11, %rdx
; SSE2-NEXT:    movq %rdi, %rax
; SSE2-NEXT:    movq %rdx, 56(%rdi)
; SSE2-NEXT:    movq %rsi, 48(%rdi)
; SSE2-NEXT:    movq %r9, 40(%rdi)
; SSE2-NEXT:    movq %rbx, 32(%rdi)
; SSE2-NEXT:    movq %r14, 24(%rdi)
; SSE2-NEXT:    movq %r8, 16(%rdi)
; SSE2-NEXT:    movq %r15, 8(%rdi)
; SSE2-NEXT:    movq %r10, (%rdi)
; SSE2-NEXT:    popq %rbx
; SSE2-NEXT:    popq %r12
; SSE2-NEXT:    popq %r13
; SSE2-NEXT:    popq %r14
; SSE2-NEXT:    popq %r15
; SSE2-NEXT:    retq
;
; SSE42-LABEL: fshl_rot_i512_vector:
; SSE42:       # %bb.0:
; SSE42-NEXT:    pushq %r15
; SSE42-NEXT:    pushq %r14
; SSE42-NEXT:    pushq %r13
; SSE42-NEXT:    pushq %r12
; SSE42-NEXT:    pushq %rbx
; SSE42-NEXT:    movq %rsi, %rcx
; SSE42-NEXT:    pextrq $1, %xmm2, %rax
; SSE42-NEXT:    pextrq $1, %xmm0, %rbx
; SSE42-NEXT:    pextrq $1, %xmm3, %r10
; SSE42-NEXT:    pextrq $1, %xmm1, %r14
; SSE42-NEXT:    movq %xmm0, %r8
; SSE42-NEXT:    movq %xmm2, %r15
; SSE42-NEXT:    testl $256, %ecx # imm = 0x100
; SSE42-NEXT:    movq %r15, %r11
; SSE42-NEXT:    cmovneq %r8, %r11
; SSE42-NEXT:    movq %xmm3, %rdx
; SSE42-NEXT:    movq %r14, %r12
; SSE42-NEXT:    cmovneq %r10, %r12
; SSE42-NEXT:    movq %xmm1, %r13
; SSE42-NEXT:    movq %r13, %rsi
; SSE42-NEXT:    cmovneq %rdx, %rsi
; SSE42-NEXT:    movq %rbx, %r9
; SSE42-NEXT:    cmovneq %rax, %r9
; SSE42-NEXT:    cmovneq %r15, %r8
; SSE42-NEXT:    cmovneq %r13, %rdx
; SSE42-NEXT:    cmovneq %rbx, %rax
; SSE42-NEXT:    cmovneq %r14, %r10
; SSE42-NEXT:    testb $-128, %cl
; SSE42-NEXT:    movq %r9, %r14
; SSE42-NEXT:    cmovneq %r10, %r14
; SSE42-NEXT:    cmovneq %rax, %r10
; SSE42-NEXT:    movq %rsi, %rbx
; SSE42-NEXT:    cmovneq %r8, %rbx
; SSE42-NEXT:    cmovneq %rdx, %r8
; SSE42-NEXT:    cmoveq %r12, %r9
; SSE42-NEXT:    cmoveq %r11, %rsi
; SSE42-NEXT:    cmovneq %r12, %rax
; SSE42-NEXT:    cmovneq %r11, %rdx
; SSE42-NEXT:    testb $64, %cl
; SSE42-NEXT:    movq %rdx, %r11
; SSE42-NEXT:    cmovneq %rax, %r11
; SSE42-NEXT:    cmovneq %rsi, %rax
; SSE42-NEXT:    cmovneq %r9, %rsi
; SSE42-NEXT:    cmovneq %rbx, %r9
; SSE42-NEXT:    cmovneq %r14, %rbx
; SSE42-NEXT:    cmovneq %r8, %r14
; SSE42-NEXT:    cmoveq %r10, %rdx
; SSE42-NEXT:    cmovneq %r10, %r8
; SSE42-NEXT:    movq %r8, %r10
; SSE42-NEXT:    shldq %cl, %rdx, %r10
; SSE42-NEXT:    movq %r14, %r15
; SSE42-NEXT:    shldq %cl, %r8, %r15
; SSE42-NEXT:    movq %rbx, %r8
; SSE42-NEXT:    shldq %cl, %r14, %r8
; SSE42-NEXT:    movq %r9, %r14
; SSE42-NEXT:    shldq %cl, %rbx, %r14
; SSE42-NEXT:    movq %rsi, %rbx
; SSE42-NEXT:    shldq %cl, %r9, %rbx
; SSE42-NEXT:    movq %rax, %r9
; SSE42-NEXT:    shldq %cl, %rsi, %r9
; SSE42-NEXT:    movq %r11, %rsi
; SSE42-NEXT:    shldq %cl, %rax, %rsi
; SSE42-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE42-NEXT:    shldq %cl, %r11, %rdx
; SSE42-NEXT:    movq %rdi, %rax
; SSE42-NEXT:    movq %rdx, 56(%rdi)
; SSE42-NEXT:    movq %rsi, 48(%rdi)
; SSE42-NEXT:    movq %r9, 40(%rdi)
; SSE42-NEXT:    movq %rbx, 32(%rdi)
; SSE42-NEXT:    movq %r14, 24(%rdi)
; SSE42-NEXT:    movq %r8, 16(%rdi)
; SSE42-NEXT:    movq %r15, 8(%rdi)
; SSE42-NEXT:    movq %r10, (%rdi)
; SSE42-NEXT:    popq %rbx
; SSE42-NEXT:    popq %r12
; SSE42-NEXT:    popq %r13
; SSE42-NEXT:    popq %r14
; SSE42-NEXT:    popq %r15
; SSE42-NEXT:    retq
;
; AVX2-LABEL: fshl_rot_i512_vector:
; AVX2:       # %bb.0:
; AVX2-NEXT:    pushq %r15
; AVX2-NEXT:    pushq %r14
; AVX2-NEXT:    pushq %r13
; AVX2-NEXT:    pushq %r12
; AVX2-NEXT:    pushq %rbx
; AVX2-NEXT:    vpextrq $1, %xmm1, %rdx
; AVX2-NEXT:    vpextrq $1, %xmm0, %rax
; AVX2-NEXT:    movq %rsi, %rcx
; AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm2
; AVX2-NEXT:    vmovq %xmm2, %rsi
; AVX2-NEXT:    vextracti128 $1, %ymm0, %xmm3
; AVX2-NEXT:    vpextrq $1, %xmm2, %r11
; AVX2-NEXT:    vpextrq $1, %xmm3, %rbx
; AVX2-NEXT:    vmovq %xmm3, %r14
; AVX2-NEXT:    vmovq %xmm0, %r10
; AVX2-NEXT:    vmovq %xmm1, %r12
; AVX2-NEXT:    testl $256, %ecx # imm = 0x100
; AVX2-NEXT:    movq %r12, %r15
; AVX2-NEXT:    cmovneq %r10, %r15
; AVX2-NEXT:    movq %rbx, %r13
; AVX2-NEXT:    cmovneq %r11, %r13
; AVX2-NEXT:    movq %r14, %r8
; AVX2-NEXT:    cmovneq %rsi, %r8
; AVX2-NEXT:    movq %rax, %r9
; AVX2-NEXT:    cmovneq %rdx, %r9
; AVX2-NEXT:    cmovneq %r12, %r10
; AVX2-NEXT:    cmovneq %r14, %rsi
; AVX2-NEXT:    cmovneq %rax, %rdx
; AVX2-NEXT:    cmovneq %rbx, %r11
; AVX2-NEXT:    testb $-128, %cl
; AVX2-NEXT:    movq %r9, %r14
; AVX2-NEXT:    cmovneq %r11, %r14
; AVX2-NEXT:    cmovneq %rdx, %r11
; AVX2-NEXT:    movq %r8, %rbx
; AVX2-NEXT:    cmovneq %r10, %rbx
; AVX2-NEXT:    cmovneq %rsi, %r10
; AVX2-NEXT:    cmoveq %r13, %r9
; AVX2-NEXT:    cmoveq %r15, %r8
; AVX2-NEXT:    cmovneq %r13, %rdx
; AVX2-NEXT:    movq %rdi, %rax
; AVX2-NEXT:    cmovneq %r15, %rsi
; AVX2-NEXT:    testb $64, %cl
; AVX2-NEXT:    movq %rsi, %rdi
; AVX2-NEXT:    cmovneq %rdx, %rdi
; AVX2-NEXT:    cmovneq %r8, %rdx
; AVX2-NEXT:    cmovneq %r9, %r8
; AVX2-NEXT:    cmovneq %rbx, %r9
; AVX2-NEXT:    cmovneq %r14, %rbx
; AVX2-NEXT:    cmovneq %r10, %r14
; AVX2-NEXT:    cmoveq %r11, %rsi
; AVX2-NEXT:    cmovneq %r11, %r10
; AVX2-NEXT:    movq %r10, %r11
; AVX2-NEXT:    shldq %cl, %rsi, %r11
; AVX2-NEXT:    movq %r14, %r15
; AVX2-NEXT:    shldq %cl, %r10, %r15
; AVX2-NEXT:    movq %rbx, %r10
; AVX2-NEXT:    shldq %cl, %r14, %r10
; AVX2-NEXT:    movq %r9, %r14
; AVX2-NEXT:    shldq %cl, %rbx, %r14
; AVX2-NEXT:    movq %r8, %rbx
; AVX2-NEXT:    shldq %cl, %r9, %rbx
; AVX2-NEXT:    movq %rdx, %r9
; AVX2-NEXT:    shldq %cl, %r8, %r9
; AVX2-NEXT:    movq %rdi, %r8
; AVX2-NEXT:    shldq %cl, %rdx, %r8
; AVX2-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX2-NEXT:    shldq %cl, %rdi, %rsi
; AVX2-NEXT:    movq %rsi, 56(%rax)
; AVX2-NEXT:    movq %r8, 48(%rax)
; AVX2-NEXT:    movq %r9, 40(%rax)
; AVX2-NEXT:    movq %rbx, 32(%rax)
; AVX2-NEXT:    movq %r14, 24(%rax)
; AVX2-NEXT:    movq %r10, 16(%rax)
; AVX2-NEXT:    movq %r15, 8(%rax)
; AVX2-NEXT:    movq %r11, (%rax)
; AVX2-NEXT:    popq %rbx
; AVX2-NEXT:    popq %r12
; AVX2-NEXT:    popq %r13
; AVX2-NEXT:    popq %r14
; AVX2-NEXT:    popq %r15
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: fshl_rot_i512_vector:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    pushq %r15
; AVX512F-NEXT:    pushq %r14
; AVX512F-NEXT:    pushq %r13
; AVX512F-NEXT:    pushq %r12
; AVX512F-NEXT:    pushq %rbx
; AVX512F-NEXT:    vextracti32x4 $2, %zmm0, %xmm1
; AVX512F-NEXT:    vpextrq $1, %xmm1, %rdx
; AVX512F-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512F-NEXT:    movq %rsi, %rcx
; AVX512F-NEXT:    vextracti32x4 $3, %zmm0, %xmm2
; AVX512F-NEXT:    vmovq %xmm2, %rsi
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm3
; AVX512F-NEXT:    vpextrq $1, %xmm2, %r11
; AVX512F-NEXT:    vpextrq $1, %xmm3, %rbx
; AVX512F-NEXT:    vmovq %xmm3, %r14
; AVX512F-NEXT:    vmovq %xmm0, %r10
; AVX512F-NEXT:    vmovq %xmm1, %r12
; AVX512F-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512F-NEXT:    movq %r12, %r15
; AVX512F-NEXT:    cmovneq %r10, %r15
; AVX512F-NEXT:    movq %rbx, %r13
; AVX512F-NEXT:    cmovneq %r11, %r13
; AVX512F-NEXT:    movq %r14, %r8
; AVX512F-NEXT:    cmovneq %rsi, %r8
; AVX512F-NEXT:    movq %rax, %r9
; AVX512F-NEXT:    cmovneq %rdx, %r9
; AVX512F-NEXT:    cmovneq %r12, %r10
; AVX512F-NEXT:    cmovneq %r14, %rsi
; AVX512F-NEXT:    cmovneq %rax, %rdx
; AVX512F-NEXT:    cmovneq %rbx, %r11
; AVX512F-NEXT:    testb $-128, %cl
; AVX512F-NEXT:    movq %r9, %r14
; AVX512F-NEXT:    cmovneq %r11, %r14
; AVX512F-NEXT:    cmovneq %rdx, %r11
; AVX512F-NEXT:    movq %r8, %rbx
; AVX512F-NEXT:    cmovneq %r10, %rbx
; AVX512F-NEXT:    cmovneq %rsi, %r10
; AVX512F-NEXT:    cmoveq %r13, %r9
; AVX512F-NEXT:    cmoveq %r15, %r8
; AVX512F-NEXT:    cmovneq %r13, %rdx
; AVX512F-NEXT:    movq %rdi, %rax
; AVX512F-NEXT:    cmovneq %r15, %rsi
; AVX512F-NEXT:    testb $64, %cl
; AVX512F-NEXT:    movq %rsi, %rdi
; AVX512F-NEXT:    cmovneq %rdx, %rdi
; AVX512F-NEXT:    cmovneq %r8, %rdx
; AVX512F-NEXT:    cmovneq %r9, %r8
; AVX512F-NEXT:    cmovneq %rbx, %r9
; AVX512F-NEXT:    cmovneq %r14, %rbx
; AVX512F-NEXT:    cmovneq %r10, %r14
; AVX512F-NEXT:    cmoveq %r11, %rsi
; AVX512F-NEXT:    cmovneq %r11, %r10
; AVX512F-NEXT:    movq %r10, %r11
; AVX512F-NEXT:    shldq %cl, %rsi, %r11
; AVX512F-NEXT:    movq %r14, %r15
; AVX512F-NEXT:    shldq %cl, %r10, %r15
; AVX512F-NEXT:    movq %rbx, %r10
; AVX512F-NEXT:    shldq %cl, %r14, %r10
; AVX512F-NEXT:    movq %r9, %r14
; AVX512F-NEXT:    shldq %cl, %rbx, %r14
; AVX512F-NEXT:    movq %r8, %rbx
; AVX512F-NEXT:    shldq %cl, %r9, %rbx
; AVX512F-NEXT:    movq %rdx, %r9
; AVX512F-NEXT:    shldq %cl, %r8, %r9
; AVX512F-NEXT:    movq %rdi, %r8
; AVX512F-NEXT:    shldq %cl, %rdx, %r8
; AVX512F-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512F-NEXT:    shldq %cl, %rdi, %rsi
; AVX512F-NEXT:    movq %rsi, 56(%rax)
; AVX512F-NEXT:    movq %r8, 48(%rax)
; AVX512F-NEXT:    movq %r9, 40(%rax)
; AVX512F-NEXT:    movq %rbx, 32(%rax)
; AVX512F-NEXT:    movq %r14, 24(%rax)
; AVX512F-NEXT:    movq %r10, 16(%rax)
; AVX512F-NEXT:    movq %r15, 8(%rax)
; AVX512F-NEXT:    movq %r11, (%rax)
; AVX512F-NEXT:    popq %rbx
; AVX512F-NEXT:    popq %r12
; AVX512F-NEXT:    popq %r13
; AVX512F-NEXT:    popq %r14
; AVX512F-NEXT:    popq %r15
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: fshl_rot_i512_vector:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    pushq %r15
; AVX512VL-NEXT:    pushq %r14
; AVX512VL-NEXT:    pushq %r13
; AVX512VL-NEXT:    pushq %r12
; AVX512VL-NEXT:    pushq %rbx
; AVX512VL-NEXT:    vextracti32x4 $2, %zmm0, %xmm1
; AVX512VL-NEXT:    vpextrq $1, %xmm1, %rdx
; AVX512VL-NEXT:    vpextrq $1, %xmm0, %rbx
; AVX512VL-NEXT:    movq %rsi, %rcx
; AVX512VL-NEXT:    vextracti32x4 $3, %zmm0, %xmm2
; AVX512VL-NEXT:    vmovq %xmm2, %rsi
; AVX512VL-NEXT:    vextracti128 $1, %ymm0, %xmm3
; AVX512VL-NEXT:    vpextrq $1, %xmm2, %r10
; AVX512VL-NEXT:    vmovq %xmm3, %r14
; AVX512VL-NEXT:    vpextrq $1, %xmm3, %r15
; AVX512VL-NEXT:    vmovq %xmm0, %r9
; AVX512VL-NEXT:    vmovq %xmm1, %r12
; AVX512VL-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VL-NEXT:    movq %r12, %r11
; AVX512VL-NEXT:    cmovneq %r9, %r11
; AVX512VL-NEXT:    movq %r15, %r13
; AVX512VL-NEXT:    cmovneq %r10, %r13
; AVX512VL-NEXT:    movq %r14, %rax
; AVX512VL-NEXT:    cmovneq %rsi, %rax
; AVX512VL-NEXT:    movq %rbx, %r8
; AVX512VL-NEXT:    cmovneq %rdx, %r8
; AVX512VL-NEXT:    cmovneq %r12, %r9
; AVX512VL-NEXT:    cmovneq %r14, %rsi
; AVX512VL-NEXT:    cmovneq %rbx, %rdx
; AVX512VL-NEXT:    cmovneq %r15, %r10
; AVX512VL-NEXT:    testb $-128, %cl
; AVX512VL-NEXT:    movq %r8, %r14
; AVX512VL-NEXT:    cmovneq %r10, %r14
; AVX512VL-NEXT:    cmovneq %rdx, %r10
; AVX512VL-NEXT:    movq %rax, %rbx
; AVX512VL-NEXT:    cmovneq %r9, %rbx
; AVX512VL-NEXT:    cmovneq %rsi, %r9
; AVX512VL-NEXT:    cmoveq %r13, %r8
; AVX512VL-NEXT:    cmoveq %r11, %rax
; AVX512VL-NEXT:    cmovneq %r13, %rdx
; AVX512VL-NEXT:    cmovneq %r11, %rsi
; AVX512VL-NEXT:    testb $64, %cl
; AVX512VL-NEXT:    movq %rsi, %r11
; AVX512VL-NEXT:    cmovneq %rdx, %r11
; AVX512VL-NEXT:    cmovneq %rax, %rdx
; AVX512VL-NEXT:    cmovneq %r8, %rax
; AVX512VL-NEXT:    cmovneq %rbx, %r8
; AVX512VL-NEXT:    cmovneq %r14, %rbx
; AVX512VL-NEXT:    cmovneq %r9, %r14
; AVX512VL-NEXT:    cmoveq %r10, %rsi
; AVX512VL-NEXT:    cmovneq %r10, %r9
; AVX512VL-NEXT:    movq %r9, %r10
; AVX512VL-NEXT:    shldq %cl, %rsi, %r10
; AVX512VL-NEXT:    movq %r14, %r15
; AVX512VL-NEXT:    shldq %cl, %r9, %r15
; AVX512VL-NEXT:    movq %rbx, %r9
; AVX512VL-NEXT:    shldq %cl, %r14, %r9
; AVX512VL-NEXT:    movq %r8, %r14
; AVX512VL-NEXT:    shldq %cl, %rbx, %r14
; AVX512VL-NEXT:    movq %rax, %rbx
; AVX512VL-NEXT:    shldq %cl, %r8, %rbx
; AVX512VL-NEXT:    movq %rdx, %r8
; AVX512VL-NEXT:    shldq %cl, %rax, %r8
; AVX512VL-NEXT:    movq %rdi, %rax
; AVX512VL-NEXT:    movq %r11, %rdi
; AVX512VL-NEXT:    shldq %cl, %rdx, %rdi
; AVX512VL-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VL-NEXT:    shldq %cl, %r11, %rsi
; AVX512VL-NEXT:    movq %rsi, 56(%rax)
; AVX512VL-NEXT:    movq %rdi, 48(%rax)
; AVX512VL-NEXT:    movq %r8, 40(%rax)
; AVX512VL-NEXT:    movq %rbx, 32(%rax)
; AVX512VL-NEXT:    movq %r14, 24(%rax)
; AVX512VL-NEXT:    movq %r9, 16(%rax)
; AVX512VL-NEXT:    movq %r15, 8(%rax)
; AVX512VL-NEXT:    movq %r10, (%rax)
; AVX512VL-NEXT:    popq %rbx
; AVX512VL-NEXT:    popq %r12
; AVX512VL-NEXT:    popq %r13
; AVX512VL-NEXT:    popq %r14
; AVX512VL-NEXT:    popq %r15
; AVX512VL-NEXT:    vzeroupper
; AVX512VL-NEXT:    retq
;
; AVX512VBMI-LABEL: fshl_rot_i512_vector:
; AVX512VBMI:       # %bb.0:
; AVX512VBMI-NEXT:    pushq %r15
; AVX512VBMI-NEXT:    pushq %r14
; AVX512VBMI-NEXT:    pushq %r13
; AVX512VBMI-NEXT:    pushq %r12
; AVX512VBMI-NEXT:    pushq %rbx
; AVX512VBMI-NEXT:    vextracti32x4 $2, %zmm0, %xmm1
; AVX512VBMI-NEXT:    vpextrq $1, %xmm1, %rdx
; AVX512VBMI-NEXT:    vpextrq $1, %xmm0, %rbx
; AVX512VBMI-NEXT:    movq %rsi, %rcx
; AVX512VBMI-NEXT:    vextracti32x4 $3, %zmm0, %xmm2
; AVX512VBMI-NEXT:    vmovq %xmm2, %rsi
; AVX512VBMI-NEXT:    vextracti128 $1, %ymm0, %xmm3
; AVX512VBMI-NEXT:    vpextrq $1, %xmm2, %r10
; AVX512VBMI-NEXT:    vmovq %xmm3, %r14
; AVX512VBMI-NEXT:    vpextrq $1, %xmm3, %r15
; AVX512VBMI-NEXT:    vmovq %xmm0, %r9
; AVX512VBMI-NEXT:    vmovq %xmm1, %r12
; AVX512VBMI-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VBMI-NEXT:    movq %r12, %r11
; AVX512VBMI-NEXT:    cmovneq %r9, %r11
; AVX512VBMI-NEXT:    movq %r15, %r13
; AVX512VBMI-NEXT:    cmovneq %r10, %r13
; AVX512VBMI-NEXT:    movq %r14, %rax
; AVX512VBMI-NEXT:    cmovneq %rsi, %rax
; AVX512VBMI-NEXT:    movq %rbx, %r8
; AVX512VBMI-NEXT:    cmovneq %rdx, %r8
; AVX512VBMI-NEXT:    cmovneq %r12, %r9
; AVX512VBMI-NEXT:    cmovneq %r14, %rsi
; AVX512VBMI-NEXT:    cmovneq %rbx, %rdx
; AVX512VBMI-NEXT:    cmovneq %r15, %r10
; AVX512VBMI-NEXT:    testb $-128, %cl
; AVX512VBMI-NEXT:    movq %r8, %r14
; AVX512VBMI-NEXT:    cmovneq %r10, %r14
; AVX512VBMI-NEXT:    cmovneq %rdx, %r10
; AVX512VBMI-NEXT:    movq %rax, %rbx
; AVX512VBMI-NEXT:    cmovneq %r9, %rbx
; AVX512VBMI-NEXT:    cmovneq %rsi, %r9
; AVX512VBMI-NEXT:    cmoveq %r13, %r8
; AVX512VBMI-NEXT:    cmoveq %r11, %rax
; AVX512VBMI-NEXT:    cmovneq %r13, %rdx
; AVX512VBMI-NEXT:    cmovneq %r11, %rsi
; AVX512VBMI-NEXT:    testb $64, %cl
; AVX512VBMI-NEXT:    movq %rsi, %r11
; AVX512VBMI-NEXT:    cmovneq %rdx, %r11
; AVX512VBMI-NEXT:    cmovneq %rax, %rdx
; AVX512VBMI-NEXT:    cmovneq %r8, %rax
; AVX512VBMI-NEXT:    cmovneq %rbx, %r8
; AVX512VBMI-NEXT:    cmovneq %r14, %rbx
; AVX512VBMI-NEXT:    cmovneq %r9, %r14
; AVX512VBMI-NEXT:    cmoveq %r10, %rsi
; AVX512VBMI-NEXT:    cmovneq %r10, %r9
; AVX512VBMI-NEXT:    movq %r9, %r10
; AVX512VBMI-NEXT:    shldq %cl, %rsi, %r10
; AVX512VBMI-NEXT:    movq %r14, %r15
; AVX512VBMI-NEXT:    shldq %cl, %r9, %r15
; AVX512VBMI-NEXT:    movq %rbx, %r9
; AVX512VBMI-NEXT:    shldq %cl, %r14, %r9
; AVX512VBMI-NEXT:    movq %r8, %r14
; AVX512VBMI-NEXT:    shldq %cl, %rbx, %r14
; AVX512VBMI-NEXT:    movq %rax, %rbx
; AVX512VBMI-NEXT:    shldq %cl, %r8, %rbx
; AVX512VBMI-NEXT:    movq %rdx, %r8
; AVX512VBMI-NEXT:    shldq %cl, %rax, %r8
; AVX512VBMI-NEXT:    movq %rdi, %rax
; AVX512VBMI-NEXT:    movq %r11, %rdi
; AVX512VBMI-NEXT:    shldq %cl, %rdx, %rdi
; AVX512VBMI-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VBMI-NEXT:    shldq %cl, %r11, %rsi
; AVX512VBMI-NEXT:    movq %rsi, 56(%rax)
; AVX512VBMI-NEXT:    movq %rdi, 48(%rax)
; AVX512VBMI-NEXT:    movq %r8, 40(%rax)
; AVX512VBMI-NEXT:    movq %rbx, 32(%rax)
; AVX512VBMI-NEXT:    movq %r14, 24(%rax)
; AVX512VBMI-NEXT:    movq %r9, 16(%rax)
; AVX512VBMI-NEXT:    movq %r15, 8(%rax)
; AVX512VBMI-NEXT:    movq %r10, (%rax)
; AVX512VBMI-NEXT:    popq %rbx
; AVX512VBMI-NEXT:    popq %r12
; AVX512VBMI-NEXT:    popq %r13
; AVX512VBMI-NEXT:    popq %r14
; AVX512VBMI-NEXT:    popq %r15
; AVX512VBMI-NEXT:    vzeroupper
; AVX512VBMI-NEXT:    retq
  %a0 = bitcast <8 x i64> %v0 to i512
  %r = call i512 @llvm.fshl.i512(i512 %a0, i512 %a0, i512 %a2)
  ret i512 %r
}

define i512 @fshr_rot_i512_vector(<8 x i64> %v0, i512 %a2) nounwind {
; SSE2-LABEL: fshr_rot_i512_vector:
; SSE2:       # %bb.0:
; SSE2-NEXT:    pushq %r15
; SSE2-NEXT:    pushq %r14
; SSE2-NEXT:    pushq %r13
; SSE2-NEXT:    pushq %r12
; SSE2-NEXT:    pushq %rbx
; SSE2-NEXT:    movq %rsi, %rcx
; SSE2-NEXT:    pshufd {{.*#+}} xmm4 = xmm2[2,3,2,3]
; SSE2-NEXT:    movq %xmm4, %rdx
; SSE2-NEXT:    pshufd {{.*#+}} xmm4 = xmm0[2,3,2,3]
; SSE2-NEXT:    movq %xmm3, %rax
; SSE2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[2,3,2,3]
; SSE2-NEXT:    movq %xmm3, %r11
; SSE2-NEXT:    pshufd {{.*#+}} xmm3 = xmm1[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, %rsi
; SSE2-NEXT:    movq %xmm2, %r10
; SSE2-NEXT:    testl $256, %ecx # imm = 0x100
; SSE2-NEXT:    movq %r10, %r14
; SSE2-NEXT:    cmoveq %rsi, %r14
; SSE2-NEXT:    movq %xmm3, %rbx
; SSE2-NEXT:    movq %rbx, %r15
; SSE2-NEXT:    cmoveq %r11, %r15
; SSE2-NEXT:    movq %xmm1, %r12
; SSE2-NEXT:    movq %r12, %r8
; SSE2-NEXT:    cmoveq %rax, %r8
; SSE2-NEXT:    movq %xmm4, %r13
; SSE2-NEXT:    movq %r13, %r9
; SSE2-NEXT:    cmoveq %rdx, %r9
; SSE2-NEXT:    cmoveq %r10, %rsi
; SSE2-NEXT:    cmoveq %r12, %rax
; SSE2-NEXT:    cmoveq %r13, %rdx
; SSE2-NEXT:    cmoveq %rbx, %r11
; SSE2-NEXT:    testb $-128, %cl
; SSE2-NEXT:    movq %r9, %r10
; SSE2-NEXT:    cmoveq %r11, %r10
; SSE2-NEXT:    cmoveq %rdx, %r11
; SSE2-NEXT:    movq %r8, %rbx
; SSE2-NEXT:    cmoveq %rsi, %rbx
; SSE2-NEXT:    cmoveq %rax, %rsi
; SSE2-NEXT:    cmovneq %r15, %r9
; SSE2-NEXT:    cmovneq %r14, %r8
; SSE2-NEXT:    cmoveq %r15, %rdx
; SSE2-NEXT:    cmoveq %r14, %rax
; SSE2-NEXT:    testb $64, %cl
; SSE2-NEXT:    movq %rax, %r14
; SSE2-NEXT:    cmoveq %rdx, %r14
; SSE2-NEXT:    cmoveq %r8, %rdx
; SSE2-NEXT:    cmoveq %r9, %r8
; SSE2-NEXT:    cmoveq %rbx, %r9
; SSE2-NEXT:    cmoveq %r10, %rbx
; SSE2-NEXT:    cmoveq %rsi, %r10
; SSE2-NEXT:    cmoveq %r11, %rsi
; SSE2-NEXT:    cmovneq %r11, %rax
; SSE2-NEXT:    movq %rax, %r11
; SSE2-NEXT:    shrdq %cl, %rsi, %r11
; SSE2-NEXT:    shrdq %cl, %r10, %rsi
; SSE2-NEXT:    shrdq %cl, %rbx, %r10
; SSE2-NEXT:    shrdq %cl, %r9, %rbx
; SSE2-NEXT:    shrdq %cl, %r8, %r9
; SSE2-NEXT:    shrdq %cl, %rdx, %r8
; SSE2-NEXT:    shrdq %cl, %r14, %rdx
; SSE2-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE2-NEXT:    shrdq %cl, %rax, %r14
; SSE2-NEXT:    movq %rdi, %rax
; SSE2-NEXT:    movq %r14, 56(%rdi)
; SSE2-NEXT:    movq %rdx, 48(%rdi)
; SSE2-NEXT:    movq %r8, 40(%rdi)
; SSE2-NEXT:    movq %r9, 32(%rdi)
; SSE2-NEXT:    movq %rbx, 24(%rdi)
; SSE2-NEXT:    movq %r10, 16(%rdi)
; SSE2-NEXT:    movq %rsi, 8(%rdi)
; SSE2-NEXT:    movq %r11, (%rdi)
; SSE2-NEXT:    popq %rbx
; SSE2-NEXT:    popq %r12
; SSE2-NEXT:    popq %r13
; SSE2-NEXT:    popq %r14
; SSE2-NEXT:    popq %r15
; SSE2-NEXT:    retq
;
; SSE42-LABEL: fshr_rot_i512_vector:
; SSE42:       # %bb.0:
; SSE42-NEXT:    pushq %r15
; SSE42-NEXT:    pushq %r14
; SSE42-NEXT:    pushq %r13
; SSE42-NEXT:    pushq %r12
; SSE42-NEXT:    pushq %rbx
; SSE42-NEXT:    movq %rsi, %rcx
; SSE42-NEXT:    pextrq $1, %xmm2, %rdx
; SSE42-NEXT:    pextrq $1, %xmm0, %r10
; SSE42-NEXT:    pextrq $1, %xmm3, %rbx
; SSE42-NEXT:    pextrq $1, %xmm1, %r11
; SSE42-NEXT:    movq %xmm0, %rsi
; SSE42-NEXT:    movq %xmm2, %r15
; SSE42-NEXT:    testl $256, %ecx # imm = 0x100
; SSE42-NEXT:    movq %r15, %r14
; SSE42-NEXT:    cmoveq %rsi, %r14
; SSE42-NEXT:    movq %xmm3, %rax
; SSE42-NEXT:    movq %r11, %r12
; SSE42-NEXT:    cmoveq %rbx, %r12
; SSE42-NEXT:    movq %xmm1, %r13
; SSE42-NEXT:    movq %r13, %r8
; SSE42-NEXT:    cmoveq %rax, %r8
; SSE42-NEXT:    movq %r10, %r9
; SSE42-NEXT:    cmoveq %rdx, %r9
; SSE42-NEXT:    cmoveq %r15, %rsi
; SSE42-NEXT:    cmoveq %r13, %rax
; SSE42-NEXT:    cmoveq %r10, %rdx
; SSE42-NEXT:    cmoveq %r11, %rbx
; SSE42-NEXT:    testb $-128, %cl
; SSE42-NEXT:    movq %r9, %r10
; SSE42-NEXT:    cmoveq %rbx, %r10
; SSE42-NEXT:    cmoveq %rdx, %rbx
; SSE42-NEXT:    movq %r8, %r11
; SSE42-NEXT:    cmoveq %rsi, %r11
; SSE42-NEXT:    cmoveq %rax, %rsi
; SSE42-NEXT:    cmovneq %r12, %r9
; SSE42-NEXT:    cmovneq %r14, %r8
; SSE42-NEXT:    cmoveq %r12, %rdx
; SSE42-NEXT:    cmoveq %r14, %rax
; SSE42-NEXT:    testb $64, %cl
; SSE42-NEXT:    movq %rax, %r14
; SSE42-NEXT:    cmoveq %rdx, %r14
; SSE42-NEXT:    cmoveq %r8, %rdx
; SSE42-NEXT:    cmoveq %r9, %r8
; SSE42-NEXT:    cmoveq %r11, %r9
; SSE42-NEXT:    cmoveq %r10, %r11
; SSE42-NEXT:    cmoveq %rsi, %r10
; SSE42-NEXT:    cmoveq %rbx, %rsi
; SSE42-NEXT:    cmovneq %rbx, %rax
; SSE42-NEXT:    movq %rax, %rbx
; SSE42-NEXT:    shrdq %cl, %rsi, %rbx
; SSE42-NEXT:    shrdq %cl, %r10, %rsi
; SSE42-NEXT:    shrdq %cl, %r11, %r10
; SSE42-NEXT:    shrdq %cl, %r9, %r11
; SSE42-NEXT:    shrdq %cl, %r8, %r9
; SSE42-NEXT:    shrdq %cl, %rdx, %r8
; SSE42-NEXT:    shrdq %cl, %r14, %rdx
; SSE42-NEXT:    # kill: def $cl killed $cl killed $rcx
; SSE42-NEXT:    shrdq %cl, %rax, %r14
; SSE42-NEXT:    movq %rdi, %rax
; SSE42-NEXT:    movq %r14, 56(%rdi)
; SSE42-NEXT:    movq %rdx, 48(%rdi)
; SSE42-NEXT:    movq %r8, 40(%rdi)
; SSE42-NEXT:    movq %r9, 32(%rdi)
; SSE42-NEXT:    movq %r11, 24(%rdi)
; SSE42-NEXT:    movq %r10, 16(%rdi)
; SSE42-NEXT:    movq %rsi, 8(%rdi)
; SSE42-NEXT:    movq %rbx, (%rdi)
; SSE42-NEXT:    popq %rbx
; SSE42-NEXT:    popq %r12
; SSE42-NEXT:    popq %r13
; SSE42-NEXT:    popq %r14
; SSE42-NEXT:    popq %r15
; SSE42-NEXT:    retq
;
; AVX2-LABEL: fshr_rot_i512_vector:
; AVX2:       # %bb.0:
; AVX2-NEXT:    pushq %r15
; AVX2-NEXT:    pushq %r14
; AVX2-NEXT:    pushq %r13
; AVX2-NEXT:    pushq %r12
; AVX2-NEXT:    pushq %rbx
; AVX2-NEXT:    vpextrq $1, %xmm1, %rdx
; AVX2-NEXT:    vpextrq $1, %xmm0, %rax
; AVX2-NEXT:    movq %rsi, %rcx
; AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm2
; AVX2-NEXT:    vmovq %xmm2, %r8
; AVX2-NEXT:    vextracti128 $1, %ymm0, %xmm3
; AVX2-NEXT:    vpextrq $1, %xmm2, %r14
; AVX2-NEXT:    vpextrq $1, %xmm3, %r11
; AVX2-NEXT:    vmovq %xmm3, %rbx
; AVX2-NEXT:    vmovq %xmm0, %rsi
; AVX2-NEXT:    vmovq %xmm1, %r12
; AVX2-NEXT:    testl $256, %ecx # imm = 0x100
; AVX2-NEXT:    movq %r12, %r15
; AVX2-NEXT:    cmoveq %rsi, %r15
; AVX2-NEXT:    movq %r11, %r13
; AVX2-NEXT:    cmoveq %r14, %r13
; AVX2-NEXT:    movq %rbx, %r9
; AVX2-NEXT:    cmoveq %r8, %r9
; AVX2-NEXT:    movq %rax, %r10
; AVX2-NEXT:    cmoveq %rdx, %r10
; AVX2-NEXT:    cmoveq %r12, %rsi
; AVX2-NEXT:    cmoveq %rbx, %r8
; AVX2-NEXT:    cmoveq %rax, %rdx
; AVX2-NEXT:    cmoveq %r11, %r14
; AVX2-NEXT:    testb $-128, %cl
; AVX2-NEXT:    movq %r10, %r11
; AVX2-NEXT:    cmoveq %r14, %r11
; AVX2-NEXT:    cmoveq %rdx, %r14
; AVX2-NEXT:    movq %r9, %rbx
; AVX2-NEXT:    cmoveq %rsi, %rbx
; AVX2-NEXT:    cmoveq %r8, %rsi
; AVX2-NEXT:    cmovneq %r13, %r10
; AVX2-NEXT:    cmovneq %r15, %r9
; AVX2-NEXT:    cmoveq %r13, %rdx
; AVX2-NEXT:    movq %rdi, %rax
; AVX2-NEXT:    cmoveq %r15, %r8
; AVX2-NEXT:    testb $64, %cl
; AVX2-NEXT:    movq %r8, %rdi
; AVX2-NEXT:    cmoveq %rdx, %rdi
; AVX2-NEXT:    cmoveq %r9, %rdx
; AVX2-NEXT:    cmoveq %r10, %r9
; AVX2-NEXT:    cmoveq %rbx, %r10
; AVX2-NEXT:    cmoveq %r11, %rbx
; AVX2-NEXT:    cmoveq %rsi, %r11
; AVX2-NEXT:    cmoveq %r14, %rsi
; AVX2-NEXT:    cmovneq %r14, %r8
; AVX2-NEXT:    movq %r8, %r14
; AVX2-NEXT:    shrdq %cl, %rsi, %r14
; AVX2-NEXT:    shrdq %cl, %r11, %rsi
; AVX2-NEXT:    shrdq %cl, %rbx, %r11
; AVX2-NEXT:    shrdq %cl, %r10, %rbx
; AVX2-NEXT:    shrdq %cl, %r9, %r10
; AVX2-NEXT:    shrdq %cl, %rdx, %r9
; AVX2-NEXT:    shrdq %cl, %rdi, %rdx
; AVX2-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX2-NEXT:    shrdq %cl, %r8, %rdi
; AVX2-NEXT:    movq %rdi, 56(%rax)
; AVX2-NEXT:    movq %rdx, 48(%rax)
; AVX2-NEXT:    movq %r9, 40(%rax)
; AVX2-NEXT:    movq %r10, 32(%rax)
; AVX2-NEXT:    movq %rbx, 24(%rax)
; AVX2-NEXT:    movq %r11, 16(%rax)
; AVX2-NEXT:    movq %rsi, 8(%rax)
; AVX2-NEXT:    movq %r14, (%rax)
; AVX2-NEXT:    popq %rbx
; AVX2-NEXT:    popq %r12
; AVX2-NEXT:    popq %r13
; AVX2-NEXT:    popq %r14
; AVX2-NEXT:    popq %r15
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: fshr_rot_i512_vector:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    pushq %r15
; AVX512F-NEXT:    pushq %r14
; AVX512F-NEXT:    pushq %r13
; AVX512F-NEXT:    pushq %r12
; AVX512F-NEXT:    pushq %rbx
; AVX512F-NEXT:    vextracti32x4 $2, %zmm0, %xmm1
; AVX512F-NEXT:    vpextrq $1, %xmm1, %rdx
; AVX512F-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512F-NEXT:    movq %rsi, %rcx
; AVX512F-NEXT:    vextracti32x4 $3, %zmm0, %xmm2
; AVX512F-NEXT:    vmovq %xmm2, %r8
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm3
; AVX512F-NEXT:    vpextrq $1, %xmm2, %r14
; AVX512F-NEXT:    vpextrq $1, %xmm3, %r11
; AVX512F-NEXT:    vmovq %xmm3, %rbx
; AVX512F-NEXT:    vmovq %xmm0, %rsi
; AVX512F-NEXT:    vmovq %xmm1, %r12
; AVX512F-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512F-NEXT:    movq %r12, %r15
; AVX512F-NEXT:    cmoveq %rsi, %r15
; AVX512F-NEXT:    movq %r11, %r13
; AVX512F-NEXT:    cmoveq %r14, %r13
; AVX512F-NEXT:    movq %rbx, %r9
; AVX512F-NEXT:    cmoveq %r8, %r9
; AVX512F-NEXT:    movq %rax, %r10
; AVX512F-NEXT:    cmoveq %rdx, %r10
; AVX512F-NEXT:    cmoveq %r12, %rsi
; AVX512F-NEXT:    cmoveq %rbx, %r8
; AVX512F-NEXT:    cmoveq %rax, %rdx
; AVX512F-NEXT:    cmoveq %r11, %r14
; AVX512F-NEXT:    testb $-128, %cl
; AVX512F-NEXT:    movq %r10, %r11
; AVX512F-NEXT:    cmoveq %r14, %r11
; AVX512F-NEXT:    cmoveq %rdx, %r14
; AVX512F-NEXT:    movq %r9, %rbx
; AVX512F-NEXT:    cmoveq %rsi, %rbx
; AVX512F-NEXT:    cmoveq %r8, %rsi
; AVX512F-NEXT:    cmovneq %r13, %r10
; AVX512F-NEXT:    cmovneq %r15, %r9
; AVX512F-NEXT:    cmoveq %r13, %rdx
; AVX512F-NEXT:    movq %rdi, %rax
; AVX512F-NEXT:    cmoveq %r15, %r8
; AVX512F-NEXT:    testb $64, %cl
; AVX512F-NEXT:    movq %r8, %rdi
; AVX512F-NEXT:    cmoveq %rdx, %rdi
; AVX512F-NEXT:    cmoveq %r9, %rdx
; AVX512F-NEXT:    cmoveq %r10, %r9
; AVX512F-NEXT:    cmoveq %rbx, %r10
; AVX512F-NEXT:    cmoveq %r11, %rbx
; AVX512F-NEXT:    cmoveq %rsi, %r11
; AVX512F-NEXT:    cmoveq %r14, %rsi
; AVX512F-NEXT:    cmovneq %r14, %r8
; AVX512F-NEXT:    movq %r8, %r14
; AVX512F-NEXT:    shrdq %cl, %rsi, %r14
; AVX512F-NEXT:    shrdq %cl, %r11, %rsi
; AVX512F-NEXT:    shrdq %cl, %rbx, %r11
; AVX512F-NEXT:    shrdq %cl, %r10, %rbx
; AVX512F-NEXT:    shrdq %cl, %r9, %r10
; AVX512F-NEXT:    shrdq %cl, %rdx, %r9
; AVX512F-NEXT:    shrdq %cl, %rdi, %rdx
; AVX512F-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512F-NEXT:    shrdq %cl, %r8, %rdi
; AVX512F-NEXT:    movq %rdi, 56(%rax)
; AVX512F-NEXT:    movq %rdx, 48(%rax)
; AVX512F-NEXT:    movq %r9, 40(%rax)
; AVX512F-NEXT:    movq %r10, 32(%rax)
; AVX512F-NEXT:    movq %rbx, 24(%rax)
; AVX512F-NEXT:    movq %r11, 16(%rax)
; AVX512F-NEXT:    movq %rsi, 8(%rax)
; AVX512F-NEXT:    movq %r14, (%rax)
; AVX512F-NEXT:    popq %rbx
; AVX512F-NEXT:    popq %r12
; AVX512F-NEXT:    popq %r13
; AVX512F-NEXT:    popq %r14
; AVX512F-NEXT:    popq %r15
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: fshr_rot_i512_vector:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    pushq %r15
; AVX512VL-NEXT:    pushq %r14
; AVX512VL-NEXT:    pushq %r13
; AVX512VL-NEXT:    pushq %r12
; AVX512VL-NEXT:    pushq %rbx
; AVX512VL-NEXT:    vextracti32x4 $2, %zmm0, %xmm1
; AVX512VL-NEXT:    vpextrq $1, %xmm1, %rdx
; AVX512VL-NEXT:    vpextrq $1, %xmm0, %r11
; AVX512VL-NEXT:    movq %rsi, %rcx
; AVX512VL-NEXT:    vextracti32x4 $3, %zmm0, %xmm2
; AVX512VL-NEXT:    vmovq %xmm2, %r8
; AVX512VL-NEXT:    vextracti128 $1, %ymm0, %xmm3
; AVX512VL-NEXT:    vpextrq $1, %xmm2, %rax
; AVX512VL-NEXT:    vmovq %xmm3, %rbx
; AVX512VL-NEXT:    vpextrq $1, %xmm3, %r15
; AVX512VL-NEXT:    vmovq %xmm0, %rsi
; AVX512VL-NEXT:    vmovq %xmm1, %r12
; AVX512VL-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VL-NEXT:    movq %r12, %r14
; AVX512VL-NEXT:    cmoveq %rsi, %r14
; AVX512VL-NEXT:    movq %r15, %r13
; AVX512VL-NEXT:    cmoveq %rax, %r13
; AVX512VL-NEXT:    movq %rbx, %r9
; AVX512VL-NEXT:    cmoveq %r8, %r9
; AVX512VL-NEXT:    movq %r11, %r10
; AVX512VL-NEXT:    cmoveq %rdx, %r10
; AVX512VL-NEXT:    cmoveq %r12, %rsi
; AVX512VL-NEXT:    cmoveq %rbx, %r8
; AVX512VL-NEXT:    cmoveq %r11, %rdx
; AVX512VL-NEXT:    cmoveq %r15, %rax
; AVX512VL-NEXT:    testb $-128, %cl
; AVX512VL-NEXT:    movq %r10, %r11
; AVX512VL-NEXT:    cmoveq %rax, %r11
; AVX512VL-NEXT:    cmoveq %rdx, %rax
; AVX512VL-NEXT:    movq %r9, %rbx
; AVX512VL-NEXT:    cmoveq %rsi, %rbx
; AVX512VL-NEXT:    cmoveq %r8, %rsi
; AVX512VL-NEXT:    cmovneq %r13, %r10
; AVX512VL-NEXT:    cmovneq %r14, %r9
; AVX512VL-NEXT:    cmoveq %r13, %rdx
; AVX512VL-NEXT:    cmoveq %r14, %r8
; AVX512VL-NEXT:    testb $64, %cl
; AVX512VL-NEXT:    movq %r8, %r14
; AVX512VL-NEXT:    cmoveq %rdx, %r14
; AVX512VL-NEXT:    cmoveq %r9, %rdx
; AVX512VL-NEXT:    cmoveq %r10, %r9
; AVX512VL-NEXT:    cmoveq %rbx, %r10
; AVX512VL-NEXT:    cmoveq %r11, %rbx
; AVX512VL-NEXT:    cmoveq %rsi, %r11
; AVX512VL-NEXT:    cmoveq %rax, %rsi
; AVX512VL-NEXT:    cmovneq %rax, %r8
; AVX512VL-NEXT:    movq %r8, %r15
; AVX512VL-NEXT:    shrdq %cl, %rsi, %r15
; AVX512VL-NEXT:    shrdq %cl, %r11, %rsi
; AVX512VL-NEXT:    shrdq %cl, %rbx, %r11
; AVX512VL-NEXT:    shrdq %cl, %r10, %rbx
; AVX512VL-NEXT:    shrdq %cl, %r9, %r10
; AVX512VL-NEXT:    shrdq %cl, %rdx, %r9
; AVX512VL-NEXT:    movq %rdi, %rax
; AVX512VL-NEXT:    shrdq %cl, %r14, %rdx
; AVX512VL-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VL-NEXT:    shrdq %cl, %r8, %r14
; AVX512VL-NEXT:    movq %r14, 56(%rdi)
; AVX512VL-NEXT:    movq %rdx, 48(%rdi)
; AVX512VL-NEXT:    movq %r9, 40(%rdi)
; AVX512VL-NEXT:    movq %r10, 32(%rdi)
; AVX512VL-NEXT:    movq %rbx, 24(%rdi)
; AVX512VL-NEXT:    movq %r11, 16(%rdi)
; AVX512VL-NEXT:    movq %rsi, 8(%rdi)
; AVX512VL-NEXT:    movq %r15, (%rdi)
; AVX512VL-NEXT:    popq %rbx
; AVX512VL-NEXT:    popq %r12
; AVX512VL-NEXT:    popq %r13
; AVX512VL-NEXT:    popq %r14
; AVX512VL-NEXT:    popq %r15
; AVX512VL-NEXT:    vzeroupper
; AVX512VL-NEXT:    retq
;
; AVX512VBMI-LABEL: fshr_rot_i512_vector:
; AVX512VBMI:       # %bb.0:
; AVX512VBMI-NEXT:    pushq %r15
; AVX512VBMI-NEXT:    pushq %r14
; AVX512VBMI-NEXT:    pushq %r13
; AVX512VBMI-NEXT:    pushq %r12
; AVX512VBMI-NEXT:    pushq %rbx
; AVX512VBMI-NEXT:    vextracti32x4 $2, %zmm0, %xmm1
; AVX512VBMI-NEXT:    vpextrq $1, %xmm1, %rdx
; AVX512VBMI-NEXT:    vpextrq $1, %xmm0, %r11
; AVX512VBMI-NEXT:    movq %rsi, %rcx
; AVX512VBMI-NEXT:    vextracti32x4 $3, %zmm0, %xmm2
; AVX512VBMI-NEXT:    vmovq %xmm2, %r8
; AVX512VBMI-NEXT:    vextracti128 $1, %ymm0, %xmm3
; AVX512VBMI-NEXT:    vpextrq $1, %xmm2, %rax
; AVX512VBMI-NEXT:    vmovq %xmm3, %rbx
; AVX512VBMI-NEXT:    vpextrq $1, %xmm3, %r15
; AVX512VBMI-NEXT:    vmovq %xmm0, %rsi
; AVX512VBMI-NEXT:    vmovq %xmm1, %r12
; AVX512VBMI-NEXT:    testl $256, %ecx # imm = 0x100
; AVX512VBMI-NEXT:    movq %r12, %r14
; AVX512VBMI-NEXT:    cmoveq %rsi, %r14
; AVX512VBMI-NEXT:    movq %r15, %r13
; AVX512VBMI-NEXT:    cmoveq %rax, %r13
; AVX512VBMI-NEXT:    movq %rbx, %r9
; AVX512VBMI-NEXT:    cmoveq %r8, %r9
; AVX512VBMI-NEXT:    movq %r11, %r10
; AVX512VBMI-NEXT:    cmoveq %rdx, %r10
; AVX512VBMI-NEXT:    cmoveq %r12, %rsi
; AVX512VBMI-NEXT:    cmoveq %rbx, %r8
; AVX512VBMI-NEXT:    cmoveq %r11, %rdx
; AVX512VBMI-NEXT:    cmoveq %r15, %rax
; AVX512VBMI-NEXT:    testb $-128, %cl
; AVX512VBMI-NEXT:    movq %r10, %r11
; AVX512VBMI-NEXT:    cmoveq %rax, %r11
; AVX512VBMI-NEXT:    cmoveq %rdx, %rax
; AVX512VBMI-NEXT:    movq %r9, %rbx
; AVX512VBMI-NEXT:    cmoveq %rsi, %rbx
; AVX512VBMI-NEXT:    cmoveq %r8, %rsi
; AVX512VBMI-NEXT:    cmovneq %r13, %r10
; AVX512VBMI-NEXT:    cmovneq %r14, %r9
; AVX512VBMI-NEXT:    cmoveq %r13, %rdx
; AVX512VBMI-NEXT:    cmoveq %r14, %r8
; AVX512VBMI-NEXT:    testb $64, %cl
; AVX512VBMI-NEXT:    movq %r8, %r14
; AVX512VBMI-NEXT:    cmoveq %rdx, %r14
; AVX512VBMI-NEXT:    cmoveq %r9, %rdx
; AVX512VBMI-NEXT:    cmoveq %r10, %r9
; AVX512VBMI-NEXT:    cmoveq %rbx, %r10
; AVX512VBMI-NEXT:    cmoveq %r11, %rbx
; AVX512VBMI-NEXT:    cmoveq %rsi, %r11
; AVX512VBMI-NEXT:    cmoveq %rax, %rsi
; AVX512VBMI-NEXT:    cmovneq %rax, %r8
; AVX512VBMI-NEXT:    movq %r8, %r15
; AVX512VBMI-NEXT:    shrdq %cl, %rsi, %r15
; AVX512VBMI-NEXT:    shrdq %cl, %r11, %rsi
; AVX512VBMI-NEXT:    shrdq %cl, %rbx, %r11
; AVX512VBMI-NEXT:    shrdq %cl, %r10, %rbx
; AVX512VBMI-NEXT:    shrdq %cl, %r9, %r10
; AVX512VBMI-NEXT:    shrdq %cl, %rdx, %r9
; AVX512VBMI-NEXT:    movq %rdi, %rax
; AVX512VBMI-NEXT:    shrdq %cl, %r14, %rdx
; AVX512VBMI-NEXT:    # kill: def $cl killed $cl killed $rcx
; AVX512VBMI-NEXT:    shrdq %cl, %r8, %r14
; AVX512VBMI-NEXT:    movq %r14, 56(%rdi)
; AVX512VBMI-NEXT:    movq %rdx, 48(%rdi)
; AVX512VBMI-NEXT:    movq %r9, 40(%rdi)
; AVX512VBMI-NEXT:    movq %r10, 32(%rdi)
; AVX512VBMI-NEXT:    movq %rbx, 24(%rdi)
; AVX512VBMI-NEXT:    movq %r11, 16(%rdi)
; AVX512VBMI-NEXT:    movq %rsi, 8(%rdi)
; AVX512VBMI-NEXT:    movq %r15, (%rdi)
; AVX512VBMI-NEXT:    popq %rbx
; AVX512VBMI-NEXT:    popq %r12
; AVX512VBMI-NEXT:    popq %r13
; AVX512VBMI-NEXT:    popq %r14
; AVX512VBMI-NEXT:    popq %r15
; AVX512VBMI-NEXT:    vzeroupper
; AVX512VBMI-NEXT:    retq
  %a0 = bitcast <8 x i64> %v0 to i512
  %r = call i512 @llvm.fshr.i512(i512 %a0, i512 %a0, i512 %a2)
  ret i512 %r
}

;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; AVX512: {{.*}}
